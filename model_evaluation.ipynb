{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and determine working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import topic model \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Get stop words \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import NLP vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "# Import models \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get current directory\n",
    "dir = os.path.dirname(os.path.abspath('__file__'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2876509 entries, 0 to 2876508\n",
      "Data columns (total 2 columns):\n",
      "stars_review        int64\n",
      "processed_review    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load df from a csv - all text to lower case, tokenize into list of strings, remove punctuation and lemmatize\n",
    "preprocessed_path = os.path.join(dir, '02_processed_data','review_text_stars.csv')\n",
    "preprocessed_df = pd.read_csv(preprocessed_path, index_col = False)\n",
    "preprocessed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training and test sets using a fixed seed for reproducibility \n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_df.processed_review, preprocessed_df.stars_review, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013556\n",
      "862953\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering on full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Add neutral words related to restaurants to list of stop words\n",
    "stopWords.update(['restaurant', 'place', 'bar', 'service', 'food', 'lunch', 'breakfast', 'dinner', 'price', 'order', 'ordered'])\n",
    "\n",
    "# Remove stopwords that might reflect sentiment\n",
    "stopWords = [word for word in stopWords if word not in ['above', 'not', 'below', 't', 'off', 'no', 'again', 'against', 'under', 'hadn', 'up', 'shan', 'more', 'hasn', 'won','couldn', 'wasn', 'mustn', 'out', 'don','down', 'haven', 'price', 'mightn', 'isn', 'wouldn', 'needn', 'shouldn', 'weren', 'aren', 'didn', 'ain', 'doesn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text using unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8362\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams,bigrams and trigrams and customized stopwords \n",
    "count_vectorizer_full = CountVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             ngram_range = (1,3),\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "count_full = count_vectorizer_full.fit_transform(X_train)\n",
    "\n",
    "# Print the length of features of the count_vectorizer\n",
    "print( len(count_vectorizer_full.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizer using unigrams,bigrams and trigrams and customized stopwords \n",
    "tfidf_vectorizer_full = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             ngram_range = (1,3),\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "tfidf_full = tfidf_vectorizer_full.fit_transform(X_train)\n",
    "\n",
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer_full.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mini dataset, a variety of models will be trained on a variety of feature sets to identify promising candidates. The promising combinations will then be tuned in the following section and trained on the full training data set. \n",
    "\n",
    "It should be noted that to assess model performance, the classification accuracy will be the primary metric. \n",
    "A Confusion matrix will be created using the best performing parameters from the cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model tuning\n",
    "def cross_validation_tuning(classifier, param_grid, X_trn, y_trn):\n",
    "    classifier_cv = GridSearchCV(classifier, param_grid, cv=3)\n",
    "    classifier_cv.fit(X_trn, y_trn)\n",
    "    # Print the optimal parameters and best score\n",
    "    print(\"Tuned Classifier Parameters: {}\".format(classifier_cv.best_params_))\n",
    "    print(\"Tuned Classifier Accuracy: {:.3f}\".format(classifier_cv.best_score_))\n",
    "    # Predict the labels\n",
    "    pred = classifier_cv.predict(X_trn)\n",
    "    # Compute accuracy\n",
    "    score = metrics.accuracy_score(y_trn, pred)\n",
    "    # Calculate and print the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_trn, pred, labels=[1,2,3,4,5])\n",
    "    print('For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.')\n",
    "    print(cm)\n",
    "    return classifier_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Logistic regression model\n",
    "def logreg_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'C': [0.0001, 0.01, 1, 100, 10000], 'penalty': ['l1', 'l2']} \n",
    "    logreg_classifier = LogisticRegression()\n",
    "    tuned_logreg_classifier = cross_validation_tuning(logreg_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_logreg_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "def construct_dnn(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation ='relu', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(512, activation ='relu'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "#     dnn_model.add(Dense(512, activation ='relu'))\n",
    "#     dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model\n",
    "\n",
    "# Build model\n",
    "def dnn_model(X_trn, y_trn):\n",
    "    n_cols = X_trn.shape[1]\n",
    "    input_shape =(n_cols, )\n",
    "    model = construct_dnn(input_shape)\n",
    "    \n",
    "    # Define early_stopping_monitor\n",
    "    early_stopping_monitor = EarlyStopping(patience=2)\n",
    "    # Define fit\n",
    "    history = model.fit(X_trn, pd.get_dummies(y_trn), epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline \n",
    "# The baseline assumes review is a 5 star rating (the most common class of data). \n",
    "length = len(y_train)\n",
    "correct_pred = len(y_train[y_train == 5])\n",
    "baseline_accuracy = correct_pred / length \n",
    "print(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_sets = {'count': count_full, \n",
    "                'tfidf': tfidf_full}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define test for feature sets\n",
    "def test_features(model, sets):\n",
    "    results = defaultdict(float)\n",
    "    for key, x_values_mini in sets.items():\n",
    "        print(key)\n",
    "        model_instance = model(x_values_mini, y_mini)\n",
    "        results[key] = model_instance.best_score_\n",
    "        print('')\n",
    "    print('--------------------------')\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define deep neural net tests for feature sets\n",
    "def dnn_test_results(sets):\n",
    "    results = defaultdict(float)\n",
    "    for key, x_values_mini in sets.items():\n",
    "        print(key)\n",
    "        model_instance, history = dnn_model(x_values_mini, y_mini)\n",
    "        results[key] = max(history.history['val_acc'])\n",
    "        print('')\n",
    "    print('--------------------------')\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run tuning for logistic regression \n",
    "logreg_mini_results = test_features(logreg_model, feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run deep neural nets\n",
    "dnn_mini_results = dnn_test_results(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
