{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and determine working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chriskhoo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Get stop words \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import NLP vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import models \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get current directory\n",
    "dir = os.path.dirname(os.path.abspath('__file__'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2876509 entries, 0 to 2876508\n",
      "Data columns (total 2 columns):\n",
      "stars_review        int64\n",
      "processed_review    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load df from a csv - all text to lower case, tokenize into list of strings, remove punctuation and lemmatize\n",
    "preprocessed_path = os.path.join(dir, '02_processed_data','review_text_stars.csv')\n",
    "preprocessed_df = pd.read_csv(preprocessed_path, index_col = False)\n",
    "preprocessed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training and test sets using a fixed seed for reproducibility \n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_df.processed_review, preprocessed_df.stars_review, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013556\n",
      "862953\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering on full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Add neutral words related to restaurants to list of stop words\n",
    "stopWords.update(['restaurant', 'place', 'bar', 'service', 'food', 'lunch', 'breakfast', 'dinner', 'price', 'order', 'ordered'])\n",
    "\n",
    "# Remove stopwords that might reflect sentiment\n",
    "stopWords = [word for word in stopWords if word not in ['above', 'not', 'below', 't', 'off', 'no', 'again', 'against', 'under', 'hadn', 'up', 'shan', 'more', 'hasn', 'won','couldn', 'wasn', 'mustn', 'out', 'don','down', 'haven', 'price', 'mightn', 'isn', 'wouldn', 'needn', 'shouldn', 'weren', 'aren', 'didn', 'ain', 'doesn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text using unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8362\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams,bigrams and trigrams and customized stopwords \n",
    "count_vectorizer_full = CountVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             ngram_range = (1,3),\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "count_train_full = count_vectorizer_full.fit_transform(X_train)\n",
    "\n",
    "# Print the length of features of the count_vectorizer\n",
    "print( len(count_vectorizer_full.get_feature_names()) )\n",
    "\n",
    "# Save sparse matrix \n",
    "filename_out__count_train = os.path.join(dir, '02_processed_data','count_train.pkl')\n",
    "joblib.dump(count_train_full, filename_out__count_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/chriskhoo/Documents/SpringBoard/Springboard_Capstone1/02_processed_data/count_vectorizer_full.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save count_vectorizer\n",
    "filename_out__count_vectorizer_full = os.path.join(dir, '02_processed_data','count_vectorizer_full.pkl')\n",
    "joblib.dump(count_vectorizer_full, filename_out__count_vectorizer_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/chriskhoo/Documents/SpringBoard/Springboard_Capstone1/02_processed_data/count_test.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test data (independent variables)\n",
    "count_test_full = count_vectorizer_full.transform(X_test)\n",
    "\n",
    "# Save sparse matrix \n",
    "filename_out__count_test = os.path.join(dir, '02_processed_data','count_test.pkl')\n",
    "joblib.dump(count_test_full, filename_out__count_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/chriskhoo/Documents/SpringBoard/Springboard_Capstone1/02_processed_data/tfidf_train.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams,bigrams and trigrams and customized stopwords \n",
    "tfidf_vectorizer_full = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             ngram_range = (1,3),\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "tfidf_train_full = tfidf_vectorizer_full.fit_transform(X_train)\n",
    "\n",
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer_full.get_feature_names()) )\n",
    "\n",
    "# Save sparse matrix \n",
    "filename_out__tfidf_train = os.path.join(dir, '02_processed_data','tfidf_train.pkl')\n",
    "joblib.dump(tfidf_train_full, filename_out__tfidf_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/chriskhoo/Documents/SpringBoard/Springboard_Capstone1/02_processed_data/tfidf_test.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save tfidf_vectorizer\n",
    "filename_out__tfidf_vectorizer_full = os.path.join(dir, '02_processed_data','tfidf_vectorizer_full.pkl')\n",
    "joblib.dump(tfidf_vectorizer_full, filename_out__tfidf_vectorizer_full) \n",
    "\n",
    "# Transform the test data (independent variables)\n",
    "tfidf_test_full = tfidf_vectorizer_full.transform(X_test)\n",
    "\n",
    "# Save sparse matrix \n",
    "filename_out__tfidf_test = os.path.join(dir, '02_processed_data','tfidf_test.pkl')\n",
    "joblib.dump(tfidf_test_full, filename_out__tfidf_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mini dataset, a variety of models will be trained on a variety of feature sets to identify promising candidates. The promising combinations will then be tuned in the following section and trained on the full training data set. \n",
    "\n",
    "It should be noted that to assess model performance, the classification accuracy will be the primary metric. \n",
    "A Confusion matrix will be created using the best performing parameters from the cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model tuning\n",
    "def cross_validation_tuning(classifier, param_grid, X_trn, y_trn):\n",
    "    classifier_cv = GridSearchCV(classifier, param_grid, cv=3)\n",
    "    classifier_cv.fit(X_trn, y_trn)\n",
    "    # Print the optimal parameters and best score\n",
    "    print(\"Tuned Classifier Parameters: {}\".format(classifier_cv.best_params_))\n",
    "    print(\"Tuned Classifier Accuracy: {:.3f}\".format(classifier_cv.best_score_))\n",
    "    # Predict the labels\n",
    "    pred = classifier_cv.predict(X_trn)\n",
    "    # Compute accuracy\n",
    "    score = metrics.accuracy_score(y_trn, pred)\n",
    "    # Calculate and print the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_trn, pred, labels=[1,2,3,4,5])\n",
    "    print('For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.')\n",
    "    print(cm)\n",
    "    return classifier_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Logistic regression model\n",
    "def logreg_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'C': [0.0001, 0.01, 1, 100, 10000], 'penalty': ['l1', 'l2']} \n",
    "    logreg_classifier = LogisticRegression()\n",
    "    tuned_logreg_classifier = cross_validation_tuning(logreg_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_logreg_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "def construct_architecture_1(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation ='relu', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(512, activation ='relu'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model\n",
    "\n",
    "def construct_architecture_2(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(256, activation ='relu', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(256, activation ='relu'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model\n",
    "\n",
    "def construct_architecture_3(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation ='relu', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(512, activation ='relu'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(512, activation ='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model\n",
    "\n",
    "# Build model\n",
    "def dnn_model_variable(X_trn, y_trn, architecture):\n",
    "    n_cols = X_trn.shape[1]\n",
    "    input_shape =(n_cols, )\n",
    "    model = architecture(input_shape)\n",
    "    \n",
    "    # Define early_stopping_monitor\n",
    "    early_stopping_monitor = EarlyStopping(monitor='val_acc', patience=2)\n",
    "    # Define fit\n",
    "    history = model.fit(X_trn, pd.get_dummies(y_trn), epochs=30, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37066364183563805\n"
     ]
    }
   ],
   "source": [
    "# Calculate baseline \n",
    "# The baseline assumes review is a 5 star rating (the most common class of data). \n",
    "length = len(y_train)\n",
    "correct_pred = len(y_train[y_train == 5])\n",
    "baseline_accuracy = correct_pred / length \n",
    "print(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_set = {'count': count_train_full} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " tfidf_set = {'tfidf': tfidf_train_full }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define test for feature sets\n",
    "def test_features(model, sets):\n",
    "    results = defaultdict(float)\n",
    "    for key, x_values in sets.items():\n",
    "        print(key)\n",
    "        model_instance = model(x_values, y_train)\n",
    "        results[key] = model_instance.best_score_\n",
    "        print('')\n",
    "    print('--------------------------')\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define deep neural net tests for feature sets\n",
    "def dnn_test_results_variable(sets, architecture):\n",
    "    results = defaultdict(float)\n",
    "    for key, x_values in sets.items():\n",
    "        print(key)\n",
    "        model_instance, history = dnn_model_variable(x_values, y_train, architecture)\n",
    "        results[key] = max(history.history['val_acc'])\n",
    "        print('')\n",
    "    print('--------------------------')\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 4424s 3ms/step - loss: 0.8329 - acc: 0.6345 - val_loss: 0.8021 - val_acc: 0.6486\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 4171s 3ms/step - loss: 0.7971 - acc: 0.6519 - val_loss: 0.7919 - val_acc: 0.6498\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 4075s 3ms/step - loss: 0.7748 - acc: 0.6635 - val_loss: 0.7913 - val_acc: 0.6520\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 4694s 3ms/step - loss: 0.7516 - acc: 0.6765 - val_loss: 0.8037 - val_acc: 0.6475\n",
      "Epoch 5/30\n",
      "1610844/1610844 [==============================] - 4779s 3ms/step - loss: 0.7278 - acc: 0.6896 - val_loss: 0.8261 - val_acc: 0.6469\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.65203917439699233})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on bag of words features\n",
    "dnn_count_results = dnn_test_results_variable(count_set, construct_architecture_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 2605s 2ms/step - loss: 0.8305 - acc: 0.6356 - val_loss: 0.7939 - val_acc: 0.6492\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 2735s 2ms/step - loss: 0.7952 - acc: 0.6523 - val_loss: 0.7931 - val_acc: 0.6517\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 2400s 1ms/step - loss: 0.7769 - acc: 0.6613 - val_loss: 0.7932 - val_acc: 0.6501\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 2246s 1ms/step - loss: 0.7599 - acc: 0.6707 - val_loss: 0.7961 - val_acc: 0.6508\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.65173622837223233})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on bag of words features\n",
    "dnn_count_results_2 = dnn_test_results_variable(count_set, construct_architecture_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 4869s 3ms/step - loss: 0.8396 - acc: 0.6330 - val_loss: 0.7973 - val_acc: 0.6494\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 5010s 3ms/step - loss: 0.8038 - acc: 0.6513 - val_loss: 0.7947 - val_acc: 0.6491\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 4629s 3ms/step - loss: 0.7836 - acc: 0.6625 - val_loss: 0.8239 - val_acc: 0.6497\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 4579s 3ms/step - loss: 0.7616 - acc: 0.6755 - val_loss: 0.7993 - val_acc: 0.6487\n",
      "Epoch 5/30\n",
      "1610844/1610844 [==============================] - 4796s 3ms/step - loss: 0.7397 - acc: 0.6894 - val_loss: 0.8052 - val_acc: 0.6503\n",
      "Epoch 6/30\n",
      "1610844/1610844 [==============================] - 5002s 3ms/step - loss: 0.7175 - acc: 0.7033 - val_loss: 0.8178 - val_acc: 0.6475\n",
      "Epoch 7/30\n",
      "1610844/1610844 [==============================] - 4350s 3ms/step - loss: 0.6965 - acc: 0.7159 - val_loss: 0.8401 - val_acc: 0.6439\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.65032579113491318})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on bag of words features\n",
    "dnn_count_results_3 = dnn_test_results_variable(count_set, construct_architecture_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.639\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[179779  22872   6896   5494   8328]\n",
      " [ 54001  72584  42502  17940   9926]\n",
      " [ 13734  26406 117801  99450  28616]\n",
      " [  3080   2991  30901 293099 230804]\n",
      " [  1854    647   3288  97368 643195]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.6393718376841766})\n"
     ]
    }
   ],
   "source": [
    "# run tuning for logistic regression on bag of words features\n",
    "logreg_count_results = test_features(logreg_model, count_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 4814s 3ms/step - loss: 0.8116 - acc: 0.6424 - val_loss: 0.7837 - val_acc: 0.6530\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 4702s 3ms/step - loss: 0.7682 - acc: 0.6633 - val_loss: 0.7856 - val_acc: 0.6514\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 4705s 3ms/step - loss: 0.7314 - acc: 0.6829 - val_loss: 0.7896 - val_acc: 0.6504\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.65302995689227039})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_1 = dnn_test_results_variable(tfidf_set, construct_architecture_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 2477s 2ms/step - loss: 0.8120 - acc: 0.6423 - val_loss: 0.7845 - val_acc: 0.6526\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 2409s 1ms/step - loss: 0.7732 - acc: 0.6610 - val_loss: 0.7781 - val_acc: 0.6554\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 2290s 1ms/step - loss: 0.7462 - acc: 0.6752 - val_loss: 0.7819 - val_acc: 0.6535\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 2401s 1ms/step - loss: 0.7198 - acc: 0.6893 - val_loss: 0.7911 - val_acc: 0.6505\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.65541627763761701})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_2 = dnn_test_results_variable(tfidf_set, construct_architecture_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 6052s 4ms/step - loss: 0.8203 - acc: 0.6398 - val_loss: 0.7924 - val_acc: 0.6520\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 6118s 4ms/step - loss: 0.7795 - acc: 0.6605 - val_loss: 0.7821 - val_acc: 0.6543\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 4606s 3ms/step - loss: 0.7480 - acc: 0.6780 - val_loss: 0.7905 - val_acc: 0.6520\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 4202s 3ms/step - loss: 0.7125 - acc: 0.6991 - val_loss: 0.7995 - val_acc: 0.6506\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.65428643795128627})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_3 = dnn_test_results_variable(tfidf_set, construct_architecture_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.646\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[179655  26933   7467   4541   4773]\n",
      " [ 52441  76430  46032  15717   6333]\n",
      " [ 14106  27596 124452  99173  20680]\n",
      " [  3705   3392  34226 314541 205011]\n",
      " [  2544    851   4115 115232 623610]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.6464766810558038})\n"
     ]
    }
   ],
   "source": [
    "# run tuning for logistic regression on tfidf features\n",
    "logreg_tfidf_results = test_features(logreg_model, tfidf_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further tuning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on initial training on the full dataset. It can be observed that the best results have been from DNN's trained on the TFIDF n-grams feature set. The best performing DNN architecture has been the simplest (2 layers, each with 256 nodes) out of the 3. \n",
    "\n",
    "For this reason, further tuning will be performed on DNN architectures using the TFIDF n-gram feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_architecture_4(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(256, activation = 'tanh', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(256, activation = 'tanh'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_architecture_5(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(256, activation = 'sigmoid', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(256, activation = 'sigmoid'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_architecture_6(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(256, activation ='relu', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(256, activation ='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_architecture_7(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(128, activation ='relu', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(128, activation ='relu'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_architecture_8(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(128, activation ='sigmoid', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(128, activation ='sigmoid'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_architecture_9(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(128, activation ='sigmoid', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.4))\n",
    "    dnn_model.add(Dense(128, activation ='sigmoid'))\n",
    "    dnn_model.add(Dropout(0.4))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_architecture_10(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(256, activation ='sigmoid', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.4))\n",
    "    dnn_model.add(Dense(256, activation ='sigmoid'))\n",
    "    dnn_model.add(Dropout(0.4))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_architecture_11(input_shape):\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(256, activation ='sigmoid', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.4))\n",
    "    dnn_model.add(Dense(256, activation ='sigmoid'))\n",
    "    dnn_model.add(Dropout(0.4))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 3375s 2ms/step - loss: 0.8235 - acc: 0.6367 - val_loss: 0.7981 - val_acc: 0.6480\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 3420s 2ms/step - loss: 0.8060 - acc: 0.6444 - val_loss: 0.7945 - val_acc: 0.6488\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 2931s 2ms/step - loss: 0.7987 - acc: 0.6472 - val_loss: 0.7881 - val_acc: 0.6496\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 2003s 1ms/step - loss: 0.7933 - acc: 0.6494 - val_loss: 0.8133 - val_acc: 0.6338\n",
      "Epoch 5/30\n",
      "1610844/1610844 [==============================] - 2021s 1ms/step - loss: 0.7887 - acc: 0.6515 - val_loss: 0.7954 - val_acc: 0.6459\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.64959325771211585})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_4 = dnn_test_results_variable(tfidf_set, construct_architecture_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 2095s 1ms/step - loss: 0.8377 - acc: 0.6289 - val_loss: 0.7890 - val_acc: 0.6504\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 2050s 1ms/step - loss: 0.7910 - acc: 0.6509 - val_loss: 0.7766 - val_acc: 0.6556\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 2095s 1ms/step - loss: 0.7785 - acc: 0.6565 - val_loss: 0.7737 - val_acc: 0.6567\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 2052s 1ms/step - loss: 0.7694 - acc: 0.6604 - val_loss: 0.7719 - val_acc: 0.6579\n",
      "Epoch 5/30\n",
      "1610844/1610844 [==============================] - 2058s 1ms/step - loss: 0.7616 - acc: 0.6640 - val_loss: 0.7692 - val_acc: 0.6600\n",
      "Epoch 6/30\n",
      "1610844/1610844 [==============================] - 2054s 1ms/step - loss: 0.7532 - acc: 0.6677 - val_loss: 0.7718 - val_acc: 0.6595\n",
      "Epoch 7/30\n",
      "1610844/1610844 [==============================] - 2053s 1ms/step - loss: 0.7449 - acc: 0.6712 - val_loss: 0.7682 - val_acc: 0.6600\n",
      "Epoch 8/30\n",
      "1610844/1610844 [==============================] - 2019s 1ms/step - loss: 0.7362 - acc: 0.6754 - val_loss: 0.7701 - val_acc: 0.6599\n",
      "Epoch 9/30\n",
      "1610844/1610844 [==============================] - 2029s 1ms/step - loss: 0.7272 - acc: 0.6796 - val_loss: 0.7710 - val_acc: 0.6600\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.66003992928007715})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_5 = dnn_test_results_variable(tfidf_set, construct_architecture_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 2438s 2ms/step - loss: 0.8068 - acc: 0.6445 - val_loss: 0.7830 - val_acc: 0.6537\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 3680s 2ms/step - loss: 0.7626 - acc: 0.6655 - val_loss: 0.7825 - val_acc: 0.6521\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 2920s 2ms/step - loss: 0.7268 - acc: 0.6838 - val_loss: 0.7856 - val_acc: 0.6530\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.65372275968871452})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_6 = dnn_test_results_variable(tfidf_set, construct_architecture_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 1377s 855us/step - loss: 0.8146 - acc: 0.6414 - val_loss: 0.7844 - val_acc: 0.6536\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 1326s 823us/step - loss: 0.7796 - acc: 0.6579 - val_loss: 0.7796 - val_acc: 0.6556\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 1309s 813us/step - loss: 0.7614 - acc: 0.6677 - val_loss: 0.7789 - val_acc: 0.6551\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 1474s 915us/step - loss: 0.7459 - acc: 0.6760 - val_loss: 0.7824 - val_acc: 0.6547\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.65564472873974056})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_7 = dnn_test_results_variable(tfidf_set, construct_architecture_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 1855s 1ms/step - loss: 0.8483 - acc: 0.6239 - val_loss: 0.7894 - val_acc: 0.6490\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 1421s 882us/step - loss: 0.7970 - acc: 0.6481 - val_loss: 0.7797 - val_acc: 0.6541\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 1513s 939us/step - loss: 0.7846 - acc: 0.6541 - val_loss: 0.7748 - val_acc: 0.6562\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 1572s 976us/step - loss: 0.7764 - acc: 0.6580 - val_loss: 0.7729 - val_acc: 0.6570\n",
      "Epoch 5/30\n",
      "1610844/1610844 [==============================] - 1429s 887us/step - loss: 0.7700 - acc: 0.6609 - val_loss: 0.7707 - val_acc: 0.6580\n",
      "Epoch 6/30\n",
      "1610844/1610844 [==============================] - 1389s 862us/step - loss: 0.7640 - acc: 0.6638 - val_loss: 0.7687 - val_acc: 0.6593\n",
      "Epoch 7/30\n",
      "1610844/1610844 [==============================] - 1485s 922us/step - loss: 0.7580 - acc: 0.6658 - val_loss: 0.7690 - val_acc: 0.6591\n",
      "Epoch 8/30\n",
      "1610844/1610844 [==============================] - 1481s 919us/step - loss: 0.7524 - acc: 0.6688 - val_loss: 0.7681 - val_acc: 0.6600\n",
      "Epoch 9/30\n",
      "1610844/1610844 [==============================] - 1482s 920us/step - loss: 0.7469 - acc: 0.6714 - val_loss: 0.7681 - val_acc: 0.6600\n",
      "Epoch 10/30\n",
      "1610844/1610844 [==============================] - 1494s 927us/step - loss: 0.7412 - acc: 0.6742 - val_loss: 0.7702 - val_acc: 0.6590\n",
      "Epoch 11/30\n",
      "1610844/1610844 [==============================] - 1511s 938us/step - loss: 0.7355 - acc: 0.6773 - val_loss: 0.7700 - val_acc: 0.6592\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.65998778283298842})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_8 = dnn_test_results_variable(tfidf_set, construct_architecture_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 1559s 968us/step - loss: 0.8610 - acc: 0.6187 - val_loss: 0.7929 - val_acc: 0.6481\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 1388s 862us/step - loss: 0.8049 - acc: 0.6448 - val_loss: 0.7819 - val_acc: 0.6532\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 1048s 651us/step - loss: 0.7924 - acc: 0.6508 - val_loss: 0.7789 - val_acc: 0.6548\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 1041s 646us/step - loss: 0.7847 - acc: 0.6546 - val_loss: 0.7762 - val_acc: 0.6564\n",
      "Epoch 5/30\n",
      "1610844/1610844 [==============================] - 1040s 646us/step - loss: 0.7786 - acc: 0.6577 - val_loss: 0.7727 - val_acc: 0.6573\n",
      "Epoch 6/30\n",
      "1610844/1610844 [==============================] - 1043s 648us/step - loss: 0.7731 - acc: 0.6606 - val_loss: 0.7718 - val_acc: 0.6585\n",
      "Epoch 7/30\n",
      "1610844/1610844 [==============================] - 1064s 661us/step - loss: 0.7688 - acc: 0.6620 - val_loss: 0.7701 - val_acc: 0.6586\n",
      "Epoch 8/30\n",
      "1610844/1610844 [==============================] - 1040s 645us/step - loss: 0.7644 - acc: 0.6644 - val_loss: 0.7701 - val_acc: 0.6591\n",
      "Epoch 9/30\n",
      "1610844/1610844 [==============================] - 1055s 655us/step - loss: 0.7604 - acc: 0.6661 - val_loss: 0.7691 - val_acc: 0.6589\n",
      "Epoch 10/30\n",
      "1610844/1610844 [==============================] - 1048s 650us/step - loss: 0.7566 - acc: 0.6680 - val_loss: 0.7702 - val_acc: 0.6586\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.65914350702179614})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_9 = dnn_test_results_variable(tfidf_set, construct_architecture_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 1897s 1ms/step - loss: 0.8497 - acc: 0.6239 - val_loss: 0.7875 - val_acc: 0.6503\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 1907s 1ms/step - loss: 0.7971 - acc: 0.6482 - val_loss: 0.7800 - val_acc: 0.6541\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 1934s 1ms/step - loss: 0.7844 - acc: 0.6540 - val_loss: 0.7771 - val_acc: 0.6562\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 2746s 2ms/step - loss: 0.7756 - acc: 0.6583 - val_loss: 0.7724 - val_acc: 0.6581\n",
      "Epoch 5/30\n",
      "1610844/1610844 [==============================] - 2820s 2ms/step - loss: 0.7687 - acc: 0.6612 - val_loss: 0.7706 - val_acc: 0.6585\n",
      "Epoch 6/30\n",
      "1610844/1610844 [==============================] - 2553s 2ms/step - loss: 0.7619 - acc: 0.6641 - val_loss: 0.7714 - val_acc: 0.6579\n",
      "Epoch 7/30\n",
      "1610844/1610844 [==============================] - 3014s 2ms/step - loss: 0.7551 - acc: 0.6675 - val_loss: 0.7685 - val_acc: 0.6596\n",
      "Epoch 8/30\n",
      "1610844/1610844 [==============================] - 3380s 2ms/step - loss: 0.7486 - acc: 0.6703 - val_loss: 0.7685 - val_acc: 0.6599\n",
      "Epoch 9/30\n",
      "1610844/1610844 [==============================] - 3343s 2ms/step - loss: 0.7419 - acc: 0.6738 - val_loss: 0.7684 - val_acc: 0.6602\n",
      "Epoch 10/30\n",
      "1610844/1610844 [==============================] - 2124s 1ms/step - loss: 0.7350 - acc: 0.6772 - val_loss: 0.7687 - val_acc: 0.6603\n",
      "Epoch 11/30\n",
      "1610844/1610844 [==============================] - 1960s 1ms/step - loss: 0.7279 - acc: 0.6806 - val_loss: 0.7695 - val_acc: 0.6599\n",
      "Epoch 12/30\n",
      "1610844/1610844 [==============================] - 1976s 1ms/step - loss: 0.7206 - acc: 0.6845 - val_loss: 0.7733 - val_acc: 0.6591\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.6602658972162776})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_10 = dnn_test_results_variable(tfidf_set, construct_architecture_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Train on 1610844 samples, validate on 402712 samples\n",
      "Epoch 1/30\n",
      "1610844/1610844 [==============================] - 1338s 831us/step - loss: 1.4740 - acc: 0.3699 - val_loss: 1.4702 - val_acc: 0.3692\n",
      "Epoch 2/30\n",
      "1610844/1610844 [==============================] - 1696s 1ms/step - loss: 1.4173 - acc: 0.3840 - val_loss: 1.1967 - val_acc: 0.4727\n",
      "Epoch 3/30\n",
      "1610844/1610844 [==============================] - 2064s 1ms/step - loss: 1.1746 - acc: 0.4668 - val_loss: 1.0266 - val_acc: 0.5273\n",
      "Epoch 4/30\n",
      "1610844/1610844 [==============================] - 2160s 1ms/step - loss: 1.0827 - acc: 0.5004 - val_loss: 0.9920 - val_acc: 0.5411\n",
      "Epoch 5/30\n",
      "1610844/1610844 [==============================] - 2150s 1ms/step - loss: 1.0495 - acc: 0.5140 - val_loss: 0.9764 - val_acc: 0.5479\n",
      "Epoch 6/30\n",
      "1610844/1610844 [==============================] - 2107s 1ms/step - loss: 1.0305 - acc: 0.5228 - val_loss: 0.9663 - val_acc: 0.5524\n",
      "Epoch 7/30\n",
      "1610844/1610844 [==============================] - 1990s 1ms/step - loss: 1.0177 - acc: 0.5286 - val_loss: 0.9577 - val_acc: 0.5582\n",
      "Epoch 8/30\n",
      "1610844/1610844 [==============================] - 2251s 1ms/step - loss: 1.0067 - acc: 0.5343 - val_loss: 0.9471 - val_acc: 0.5641\n",
      "Epoch 9/30\n",
      "1610844/1610844 [==============================] - 2471s 2ms/step - loss: 0.9945 - acc: 0.5415 - val_loss: 0.9299 - val_acc: 0.5766\n",
      "Epoch 10/30\n",
      "1610844/1610844 [==============================] - 2219s 1ms/step - loss: 0.9780 - acc: 0.5521 - val_loss: 0.9062 - val_acc: 0.5941\n",
      "Epoch 11/30\n",
      "1610844/1610844 [==============================] - 2283s 1ms/step - loss: 0.9597 - acc: 0.5636 - val_loss: 0.8896 - val_acc: 0.6050\n",
      "Epoch 12/30\n",
      "1610844/1610844 [==============================] - 2083s 1ms/step - loss: 0.9452 - acc: 0.5728 - val_loss: 0.8768 - val_acc: 0.6126\n",
      "Epoch 13/30\n",
      "1610844/1610844 [==============================] - 2009s 1ms/step - loss: 0.9335 - acc: 0.5795 - val_loss: 0.8677 - val_acc: 0.6160\n",
      "Epoch 14/30\n",
      "1610844/1610844 [==============================] - 2022s 1ms/step - loss: 0.9238 - acc: 0.5849 - val_loss: 0.8604 - val_acc: 0.6195\n",
      "Epoch 15/30\n",
      "1610844/1610844 [==============================] - 1682s 1ms/step - loss: 0.9167 - acc: 0.5893 - val_loss: 0.8571 - val_acc: 0.6207\n",
      "Epoch 16/30\n",
      "1610844/1610844 [==============================] - 2029s 1ms/step - loss: 0.9106 - acc: 0.5922 - val_loss: 0.8513 - val_acc: 0.6249\n",
      "Epoch 17/30\n",
      "1610844/1610844 [==============================] - 2071s 1ms/step - loss: 0.9054 - acc: 0.5950 - val_loss: 0.8476 - val_acc: 0.6259\n",
      "Epoch 18/30\n",
      "1610844/1610844 [==============================] - 2124s 1ms/step - loss: 0.9010 - acc: 0.5975 - val_loss: 0.8441 - val_acc: 0.6276\n",
      "Epoch 19/30\n",
      "1610844/1610844 [==============================] - 1966s 1ms/step - loss: 0.8973 - acc: 0.5993 - val_loss: 0.8421 - val_acc: 0.6272\n",
      "Epoch 20/30\n",
      "1610844/1610844 [==============================] - 1966s 1ms/step - loss: 0.8939 - acc: 0.6014 - val_loss: 0.8402 - val_acc: 0.6286\n",
      "Epoch 21/30\n",
      "1610844/1610844 [==============================] - 2002s 1ms/step - loss: 0.8904 - acc: 0.6026 - val_loss: 0.8392 - val_acc: 0.6290- loss: 0.890\n",
      "Epoch 22/30\n",
      "1610844/1610844 [==============================] - 2246s 1ms/step - loss: 0.8877 - acc: 0.6044 - val_loss: 0.8378 - val_acc: 0.6302\n",
      "Epoch 23/30\n",
      "1610844/1610844 [==============================] - 1925s 1ms/step - loss: 0.8859 - acc: 0.6057 - val_loss: 0.8357 - val_acc: 0.6309\n",
      "Epoch 24/30\n",
      "1610844/1610844 [==============================] - 1885s 1ms/step - loss: 0.8837 - acc: 0.6066 - val_loss: 0.8342 - val_acc: 0.6319os\n",
      "Epoch 25/30\n",
      "1610844/1610844 [==============================] - 1856s 1ms/step - loss: 0.8810 - acc: 0.6073 - val_loss: 0.8329 - val_acc: 0.6309\n",
      "Epoch 26/30\n",
      "1610844/1610844 [==============================] - 1585s 984us/step - loss: 0.8791 - acc: 0.6089 - val_loss: 0.8310 - val_acc: 0.6322\n",
      "Epoch 27/30\n",
      "1610844/1610844 [==============================] - 1643s 1ms/step - loss: 0.8781 - acc: 0.6094 - val_loss: 0.8313 - val_acc: 0.6317\n",
      "Epoch 28/30\n",
      "1610844/1610844 [==============================] - 1619s 1ms/step - loss: 0.8766 - acc: 0.6099 - val_loss: 0.8301 - val_acc: 0.6333\n",
      "Epoch 29/30\n",
      "1610844/1610844 [==============================] - 1629s 1ms/step - loss: 0.8749 - acc: 0.6110 - val_loss: 0.8300 - val_acc: 0.6318\n",
      "Epoch 30/30\n",
      "1610844/1610844 [==============================] - 1616s 1ms/step - loss: 0.8735 - acc: 0.6113 - val_loss: 0.8277 - val_acc: 0.6341\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'tfidf': 0.63405858280974203})\n"
     ]
    }
   ],
   "source": [
    "# run deep neural nets on tfidf features\n",
    "dnn_tfidf_results_11 = dnn_test_results_variable(tfidf_set, construct_architecture_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
