{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and determine working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import topic model \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Get stop words \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import NLP vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# Import models \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get current directory\n",
    "dir = os.path.dirname(os.path.abspath('__file__'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2876509 entries, 0 to 2876508\n",
      "Data columns (total 2 columns):\n",
      "stars_review        int64\n",
      "processed_review    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load df from a csv - all text to lower case, tokenize into list of strings, remove punctuation and lemmatize\n",
    "preprocessed_path = os.path.join(dir, '02_processed_data','review_text_stars.csv')\n",
    "preprocessed_df = pd.read_csv(preprocessed_path, index_col = False)\n",
    "preprocessed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training and test sets using a fixed seed for reproducibility \n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_df.processed_review, preprocessed_df.stars_review, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mini dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100678\n"
     ]
    }
   ],
   "source": [
    "# Create a mini data set for feature and model selection (for manageable training times)\n",
    "__, X_mini, ___, y_mini = train_test_split(X_train, y_train, test_size = 0.05, random_state = 42)\n",
    "print(len(X_mini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection using mini dataset\n",
    "Using the mini dataset, various types of feature engineering will be performed and tested on a variety of models in the next stage. \n",
    "\n",
    "## Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams and remove all standard stopwords \n",
    "# Source: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "count_vectorizer_mini = CountVectorizer(analyzer = 'word',\n",
    "                             stop_words = 'english',\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "count_mini = count_vectorizer_mini.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the count_vectorizer\n",
    "print( len(count_vectorizer_mini.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf vectorizer (weighted vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams and remove all standard stopwords \n",
    "tfidf_vectorizer_mini = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = 'english',\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "tfidf_mini = tfidf_vectorizer_mini.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer_mini.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Add neutral words related to restaurants to list of stop words\n",
    "stopWords.update(['restaurant', 'place', 'bar', 'service', 'food', 'lunch', 'breakfast', 'dinner', 'price', 'order', 'ordered'])\n",
    "\n",
    "# Remove stopwords that might reflect sentiment\n",
    "stopWords = [word for word in stopWords if word not in ['above', 'not', 'below', 't', 'off', 'no', 'again', 'against', 'under', 'hadn', 'up', 'shan', 'more', 'hasn', 'won','couldn', 'wasn', 'mustn', 'out', 'don','down', 'haven', 'price', 'mightn', 'isn', 'wouldn', 'needn', 'shouldn', 'weren', 'aren', 'didn', 'ain', 'doesn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature set with new stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4024\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams and customized stopwords \n",
    "count_vectorizer_mini__stop = CountVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "count_mini__stop = count_vectorizer_mini__stop.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the count_vectorizer\n",
    "print( len(count_vectorizer_mini__stop.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4024\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams and customized stopwords \n",
    "tfidf_vectorizer_mini__stop = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "tfidf_mini__stop = tfidf_vectorizer_mini__stop.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer_mini__stop.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text using unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8448\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams,bigrams and trigrams and customized stopwords \n",
    "count_vectorizer_mini__stop_ngram = CountVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             ngram_range = (1,3),\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "count_mini__stop_ngram = count_vectorizer_mini__stop_ngram.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the count_vectorizer\n",
    "print( len(count_vectorizer_mini__stop_ngram.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8448\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams,bigrams and trigrams and customized stopwords \n",
    "tfidf_vectorizer_mini__stop_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             ngram_range = (1,3),\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "tfidf_mini__stop_ngram = tfidf_vectorizer_mini__stop_ngram.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer_mini__stop_ngram.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling \n",
    "### Using Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Initialize LDA model\n",
    "lda = LatentDirichletAllocation(n_components=300,random_state=42) \n",
    "\n",
    "# Get topics for training data\n",
    "lda_mini = lda.fit_transform(count_mini__stop_ngram)\n",
    "\n",
    "# add topics to count vectorizer ngrams set \n",
    "count_mini__stop_ngram_lda = hstack((count_mini__stop_ngram, lda_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 1:\n",
      "best ever ive one ive ever bartender one best best ive eaten gotten\n",
      "Topic 2:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 3:\n",
      "three average korean although expecting would japanese give star hoping\n",
      "Topic 4:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 5:\n",
      "meat disappointed giving not star meat wa bone mention english wa disappointed\n",
      "Topic 6:\n",
      "ground noodle dish didnt give went saturday ground beef crumb wa ok wa pretty quick tasted like wa surprised see\n",
      "Topic 7:\n",
      "got far yummy else stopped everything serve calamari cuisine chewy\n",
      "Topic 8:\n",
      "again cream ice ice cream back again try though wa back cant vanilla\n",
      "Topic 9:\n",
      "wa minute good wife not finish margarita couldnt finish salsa wa not\n",
      "Topic 10:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 11:\n",
      "need oh music might lady loud without way more along\n",
      "Topic 12:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 13:\n",
      "area up seating ended bland ended up outdoor eating outside mess\n",
      "Topic 14:\n",
      "street inside strip hotel unique lamb located across scallop etc\n",
      "Topic 15:\n",
      "chef clean boyfriend truly changed favor sushi chef sooooo washroom benefit\n",
      "Topic 16:\n",
      "star call get phone 12 cannot chance play general personally\n",
      "Topic 17:\n",
      "dish family coming back crab seafood definitely coming back dish wa definitely back\n",
      "Topic 18:\n",
      "fast parking easy clean say wa say parking lot wa bad wa clean wa hot\n",
      "Topic 19:\n",
      "home treat seen charlotte write out way hang nothing pool\n",
      "Topic 20:\n",
      "customer business night employee last rude live local show walk\n",
      "Topic 21:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 22:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 23:\n",
      "price reasonable terrible life mom die true dine believe price reasonable\n",
      "Topic 24:\n",
      "also take recommend highly wa also highly recommend out huge take out delicious\n",
      "Topic 25:\n",
      "option était get busy genuine genuinely george german gesture get get again\n",
      "Topic 26:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 27:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 28:\n",
      "steak bacon lobster appetizer slice variety offer main cut oyster\n",
      "Topic 29:\n",
      "sweet taste était get another get bit get better get back get away get attention get again get chance\n",
      "Topic 30:\n",
      "out check cool check out up kitchen sure vibe menu im sure\n",
      "Topic 31:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 32:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 33:\n",
      "wait worth get well nacho shake more boy huge well worth\n",
      "Topic 34:\n",
      "grilled night tomato star give five forget phenomenal give star thursday\n",
      "Topic 35:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 36:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 37:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 38:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 39:\n",
      "ate slow point duck belly tiny weird pork belly odd pan\n",
      "Topic 40:\n",
      "nose était get busy genuine genuinely george german gesture get get again\n",
      "Topic 41:\n",
      "passable était get another get bit get better get back get away get attention get again get chance\n",
      "Topic 42:\n",
      "could could not placed dropped hit miss helping meant queen refund written\n",
      "Topic 43:\n",
      "buck clam concerned ala mousse bang way out popcorn 10 year everything tasted\n",
      "Topic 44:\n",
      "wa great delicious wa great really wa really wa delicious loved friendly came\n",
      "Topic 45:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 46:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 47:\n",
      "long sit quickly able establishment wait down forgot casino completely\n",
      "Topic 48:\n",
      "authenticity était get chance genuinely george german gesture get get again get another\n",
      "Topic 49:\n",
      "coffee size special nothing recommended cup perfection nothing special expectation bacon\n",
      "Topic 50:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 51:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 52:\n",
      "guy tasty thank due wa tasty go again mozzarella dough god goodness\n",
      "Topic 53:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 54:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 55:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 56:\n",
      "perfect ambiance wa perfect lol taking cozy beyond picture romantic atmosphere\n",
      "Topic 57:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 58:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 59:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 60:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 61:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 62:\n",
      "chip mexican salsa margarita number tortilla guacamole enchilada efficient chip salsa\n",
      "Topic 63:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 64:\n",
      "tea sweet green milk staff friendly mango iced wa way answer iced tea\n",
      "Topic 65:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 66:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 67:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 68:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 69:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 70:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 71:\n",
      "several several time sea product time followed bass sooo ha good sea bass\n",
      "Topic 72:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 73:\n",
      "cheese mac remember mac cheese tasting cheese wa great atmosphere goat cheesy side\n",
      "Topic 74:\n",
      "freezing était get busy get bit get better get back get away get attention get another get check\n",
      "Topic 75:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 76:\n",
      "week care pick horrible up im go glad next door\n",
      "Topic 77:\n",
      "down hand trip away right wow early better neighborhood even\n",
      "Topic 78:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 79:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 80:\n",
      "vega quality la price usually small high la vega youre overpriced\n",
      "Topic 81:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 82:\n",
      "wa good wa good ok pasta wa ok friday night good wa went\n",
      "Topic 83:\n",
      "kong hong kong était get bit genuine genuinely george german gesture get\n",
      "Topic 84:\n",
      "super tasty town serving friendly wa super coupon bagel super friendly delish\n",
      "Topic 85:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 86:\n",
      "chicken sauce wa spicy flavor rice tender crispy bean flavorful\n",
      "Topic 87:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 88:\n",
      "pretty good pretty good wa pretty burrito late cook pie potato sweet\n",
      "Topic 89:\n",
      "not wa wa not good thought leave not good thought wa experience world\n",
      "Topic 90:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 91:\n",
      "make sure ask make sure enough conversation suggest mood polite eat\n",
      "Topic 92:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 93:\n",
      "cheese wa mac cheese fondue smaller affordable girl fabulous try dish\n",
      "Topic 94:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 95:\n",
      "soup beef noodle pho roll dish good broth rice spring\n",
      "Topic 96:\n",
      "shrimp money use four run level word saying save paper\n",
      "Topic 97:\n",
      "fried rice chicken good homemade price fried rice daughter anywhere fried chicken\n",
      "Topic 98:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 99:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 100:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 101:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 102:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 103:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 104:\n",
      "cooked look joint perfectly typical look like wa cooked question like well\n",
      "Topic 105:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 106:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 107:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 108:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 109:\n",
      "course ramen lettuce asian beat rich miso cant pop step\n",
      "Topic 110:\n",
      "tater tater tot était get bit get back get away get attention get another get again get\n",
      "Topic 111:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 112:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 113:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 114:\n",
      "wa minute up seated table out got 10 around said\n",
      "Topic 115:\n",
      "two piece warm people anyone crowded corner since wa reservation since\n",
      "Topic 116:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 117:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 118:\n",
      "15 butter doesnt plus reason min turkey 15 minute 30 peanut\n",
      "Topic 119:\n",
      "im not im not store yes toronto make really fan presentation\n",
      "Topic 120:\n",
      "thai authentic couple pad pad thai environment little bit folk couple time spicy\n",
      "Topic 121:\n",
      "sunday may room type head face near girlfriend pastry known\n",
      "Topic 122:\n",
      "go back go back would definitely pancake pleasant space classic again\n",
      "Topic 123:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 124:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 125:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 126:\n",
      "kid delicious wa overwhelming fully amazingly create boiled cole cole slaw wa delicious wa\n",
      "Topic 127:\n",
      "return above chicken finger coleslaw chow chowder repeat above average degree\n",
      "Topic 128:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 129:\n",
      "buffet dessert option selection white group fairly set orange station\n",
      "Topic 130:\n",
      "portion large menu large portion edamame prix liquor wa large earth really delicious\n",
      "Topic 131:\n",
      "decent spot keep trying hit cute making healthy look wa decent\n",
      "Topic 132:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 133:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 134:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 135:\n",
      "glass water cocktail dog chili short experience checked great experience multiple\n",
      "Topic 136:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 137:\n",
      "wa didnt wasnt like tasted got decided really more one\n",
      "Topic 138:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 139:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 140:\n",
      "free start pay complaint understand ready attitude woman charge done\n",
      "Topic 141:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 142:\n",
      "bit wa more one small little would hungry surprised actually\n",
      "Topic 143:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 144:\n",
      "burger fry beer game good selection burger wa tv bun tap\n",
      "Topic 145:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 146:\n",
      "big bbq fan pretty solid pretty much much sauce nothing not big\n",
      "Topic 147:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 148:\n",
      "feel especially like sausage feel like together simple mostly cooking add\n",
      "Topic 149:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 150:\n",
      "server table server wa took twice birthday party brought time new\n",
      "Topic 151:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 152:\n",
      "charlotte would different bakery not thats time really ever seen last\n",
      "Topic 153:\n",
      "everything everyone end find everything wa girl dumpling slider 100 smaller\n",
      "Topic 154:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 155:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 156:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 157:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 158:\n",
      "friend pita alcohol another catch risotto upstairs boring friend got give another\n",
      "Topic 159:\n",
      "nice wa nice patio brunch seat atmosphere mother good time chill met\n",
      "Topic 160:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 161:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 162:\n",
      "fine noticed longer wa fine hair no quantity no longer college mark\n",
      "Topic 163:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 164:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 165:\n",
      "great atmosphere italian sometimes ha not great great great great price ha great really great\n",
      "Topic 166:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 167:\n",
      "not plate came entree out sure came out not sure good not name\n",
      "Topic 168:\n",
      "year last time visit phoenix many last time visited since omelette\n",
      "Topic 169:\n",
      "amount isnt right under outside chair spice basil positive aside\n",
      "Topic 170:\n",
      "experience extremely person salty dollar per experience wa buy wa extremely disappointment\n",
      "Topic 171:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 172:\n",
      "every time get wish go often doe every time poor more\n",
      "Topic 173:\n",
      "ive never bad not wonderful tried time not bad worth experience\n",
      "Topic 174:\n",
      "evening back try go cant wait quiet awful coconut go back wait go wait go back\n",
      "Topic 175:\n",
      "drink enjoyed priced really enjoyed reasonably reasonably priced omg everywhere well group\n",
      "Topic 176:\n",
      "waiter not wa would sitting red medium however two rare\n",
      "Topic 177:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 178:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 179:\n",
      "hashbrowns était get again get bit get better get back get away get attention get another get\n",
      "Topic 180:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 181:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 182:\n",
      "menu item want drive not greasy up go get good\n",
      "Topic 183:\n",
      "wine dining absolutely prepared experience menu bottle room sat incredible\n",
      "Topic 184:\n",
      "try different couldnt pricey finish excited roasted out try out addition\n",
      "Topic 185:\n",
      "ordering delivery maybe either like wa sent roast using earlier pesto\n",
      "Topic 186:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 187:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 188:\n",
      "wa no one get even never asked cold table manager\n",
      "Topic 189:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 190:\n",
      "remove even worse get again get better get back get away get attention get another get get busy\n",
      "Topic 191:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 192:\n",
      "sushi happy roll hour happy hour good price special menu consistently\n",
      "Topic 193:\n",
      "new found style real chinese eat takeout york new york ny\n",
      "Topic 194:\n",
      "awesome cake chocolate dry waffle dessert wa awesome frozen hash biscuit\n",
      "Topic 195:\n",
      "open price saw mind wanted 25 review fair value try\n",
      "Topic 196:\n",
      "wing part onion fun whole extra taste like bag ring soda\n",
      "Topic 197:\n",
      "french helpful toast try must banana french toast matter daily must try\n",
      "Topic 198:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 199:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 200:\n",
      "curry generous gone portion pas jerk tad admit yellow chicken\n",
      "Topic 201:\n",
      "ill cant say im back thing diner good thing thats try\n",
      "Topic 202:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 203:\n",
      "old give shop sour give try school likely usual board decided give\n",
      "Topic 204:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 205:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 206:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 207:\n",
      "would recommend tofu would recommend stay somewhere donut disappointing happen rate\n",
      "Topic 208:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 209:\n",
      "staff friendly havent yet gem friendly staff menu hidden chicago didnt like\n",
      "Topic 210:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 211:\n",
      "two meal était get again get bit get better get back get away get attention get another get\n",
      "Topic 212:\n",
      "especially price était get chance genuinely george german gesture get get again get another\n",
      "Topic 213:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 214:\n",
      "egg work fruit lack smoked morning usually combination montreal benedict\n",
      "Topic 215:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 216:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 217:\n",
      "though staff friendly even wa friendly felt staff wa even though looking wa\n",
      "Topic 218:\n",
      "fresh bowl counter veggie behind ingredient get arent poke also\n",
      "Topic 219:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 220:\n",
      "fish change stand talking smell gyro market west youve fish taco\n",
      "Topic 221:\n",
      "like regular around table était get attention get busy get bit get better get back get away get another\n",
      "Topic 222:\n",
      "meal yum meal wa mouth cheesecake fabulous enjoyed hubby eggplant spend\n",
      "Topic 223:\n",
      "view window figured 14 16 many people yogurt good chicken pickup produce\n",
      "Topic 224:\n",
      "meanwhile était get bit get better get back get away get attention get another get again get chance\n",
      "Topic 225:\n",
      "show up good burger rib eye majority cant even 7pm burger good organized celery booze\n",
      "Topic 226:\n",
      "toddler était get another get bit get better get back get away get attention get again get chance\n",
      "Topic 227:\n",
      "sandwich fantastic chicken salad choose blue sandwich wa sub many avocado\n",
      "Topic 228:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 229:\n",
      "time first next wa first time next time back second went visit\n",
      "Topic 230:\n",
      "taco line vegetarian go wrong vegan cant go wrong asada gluten\n",
      "Topic 231:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 232:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 233:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 234:\n",
      "location juice closed opening ha never hardly mint lime wait long\n",
      "Topic 235:\n",
      "salad bread wa served salad wa mushroom dressing small corn no\n",
      "Topic 236:\n",
      "light please crowd totally obviously caesar hummus chipotle welcome state\n",
      "Topic 237:\n",
      "excellent enjoy wa excellent scottsdale needed enjoyable wednesday continue well jalapeno\n",
      "Topic 238:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 239:\n",
      "looked something bite try son eat raw quick looked like filling\n",
      "Topic 240:\n",
      "box delivered spectacular 11 larger taste wa diet bc seated right mr\n",
      "Topic 241:\n",
      "going back going back professional never going dish would fettuccine wa spectacular branch not first\n",
      "Topic 242:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 243:\n",
      "know dont like think get not really good review up\n",
      "Topic 244:\n",
      "special around cafe recently cant many drop lounge flat get\n",
      "Topic 245:\n",
      "hot sauce based pot traditional ball chinese steamed soy hot sauce\n",
      "Topic 246:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 247:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 248:\n",
      "20 could off make told work get take asked list\n",
      "Topic 249:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 250:\n",
      "love favorite definitely stop one delicious spot would definitely beautiful recommend\n",
      "Topic 251:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 252:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 253:\n",
      "come back come back would out definitely come out definitely come way negative\n",
      "Topic 254:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 255:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 256:\n",
      "pizza amazing wa amazing pizza wa wa pepperoni not fan oven loaded garlic\n",
      "Topic 257:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 258:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 259:\n",
      "price really around table était get again get better get back get away get attention get another get\n",
      "Topic 260:\n",
      "weekend past card shot credit company hadnt latte missing definitely\n",
      "Topic 261:\n",
      "made house black ingredient original honey impressive freshly difference taste\n",
      "Topic 262:\n",
      "always theyre ha good time chain come weve dipping delicious\n",
      "Topic 263:\n",
      "better much more taste tuna standard quality city definitely much better\n",
      "Topic 264:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 265:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 266:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 267:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 268:\n",
      "one day out even least would not front like more\n",
      "Topic 269:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 270:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 271:\n",
      "de le et la avoid pour un many time est au\n",
      "Topic 272:\n",
      "pork rib split expensive pulled brisket surprise short chop pulled pork\n",
      "Topic 273:\n",
      "full combo tell flavor tip indian including half south fare\n",
      "Topic 274:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 275:\n",
      "600 était get chance genuinely george german gesture get get again get another\n",
      "Topic 276:\n",
      "good really really good quick wouldnt miss case wa really good thanks double\n",
      "Topic 277:\n",
      "owner wife deal interesting bean yelp seasoning tuesday american quesadilla\n",
      "Topic 278:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 279:\n",
      "le impressed desert section pleased worked creative spoon not impressed good quality\n",
      "Topic 280:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 281:\n",
      "shared truffle opted app parmesan creme agreed nasty kobe wedding\n",
      "Topic 282:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 283:\n",
      "husband rib baked date filet prime side prime rib strawberry ranch\n",
      "Topic 284:\n",
      "wont today sliced key super nice today wa recipe ok not coworker 20 min\n",
      "Topic 285:\n",
      "soft sashimi meet affordable shell apps also really carte crab trio\n",
      "Topic 286:\n",
      "still curry wa charging still no chicken strip yep 36 could take even worse back said\n",
      "Topic 287:\n",
      "top crust thin soon sauce meatball really nice notch top notch topping\n",
      "Topic 288:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 289:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 290:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 291:\n",
      "must cheap downtown watch north mine hour open 24 satisfying\n",
      "Topic 292:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 293:\n",
      "dont thing like expect best packed dont like really like thing wa imagine\n",
      "Topic 294:\n",
      "little id wa little 35 alright future shopping center wa still dry\n",
      "Topic 295:\n",
      "another put offered available plenty 50 unfortunately up added straight\n",
      "Topic 296:\n",
      "wa waitress quite seemed waitress wa done well wa quite slightly steak wa\n",
      "Topic 297:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 298:\n",
      "était get chance genuine genuinely george german gesture get get again get another\n",
      "Topic 299:\n",
      "ago topping seem ha bun soggy wa like occasion year ago quality\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "display_topics(lda, tfidf_vectorizer_mini__stop_ngram.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize NMF model\n",
    "nmf = NMF(n_components=300, random_state=42)\n",
    "\n",
    "# Get topics for training data\n",
    "nmf_mini = nmf.fit_transform(tfidf_mini__stop_ngram)\n",
    "\n",
    "# add topics to tfidf ngrams set \n",
    "tfidf_train_mini__stop_ngram_nmf = hstack((tfidf_mini__stop_ngram, nmf_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "wa wa not wa nice wa delicious thought wa amazing wa wa wa excellent wa pretty wa also\n",
      "Topic 1:\n",
      "always always good always great always get always friendly staff always always fresh ha always time always always come\n",
      "Topic 2:\n",
      "great great great great price great atmosphere always great not great good great great staff great experience ha great\n",
      "Topic 3:\n",
      "pizza pizza wa slice best pizza good pizza pepperoni great pizza topping pizza good oven\n",
      "Topic 4:\n",
      "however however wa good however extremely great however however not overall disappointing rather wa extremely\n",
      "Topic 5:\n",
      "burger burger wa bun best burger shake patty good burger burger joint great burger burger fry\n",
      "Topic 6:\n",
      "good good good not good good price always good great good pretty good good not good great good time\n",
      "Topic 7:\n",
      "sushi sushi wa best sushi ayce sashimi japanese sushi chef great sushi good sushi sushi roll\n",
      "Topic 8:\n",
      "night friday late late night friday night night wa last night saturday saturday night date\n",
      "Topic 9:\n",
      "taco fish taco taco wa street taco street taco bell bell chicken taco tuesday shrimp taco\n",
      "Topic 10:\n",
      "de le et un est la pour pa très une\n",
      "Topic 11:\n",
      "wait wait staff worth wait wait time long wait wait wa hour cant wait minute wait no wait\n",
      "Topic 12:\n",
      "best one best wa best far not best best ive best pizza far best best part best sushi\n",
      "Topic 13:\n",
      "back go back come back ill definitely back soon back again ill back back soon wont\n",
      "Topic 14:\n",
      "time time wa second time second several time several time time couple time good time last time\n",
      "Topic 15:\n",
      "sandwich sandwich wa chicken sandwich turkey great sandwich best sandwich deli sandwich shop good sandwich turkey sandwich\n",
      "Topic 16:\n",
      "chicken chicken wa fried chicken chicken sandwich jerk grilled chicken chicken wing jerk chicken chicken salad breast\n",
      "Topic 17:\n",
      "thing good thing thing wa best thing one thing thats thing menu heard great thing bad thing\n",
      "Topic 18:\n",
      "love love love love love love absolutely love love great good love love atmosphere great love also love love coming\n",
      "Topic 19:\n",
      "happy hour happy hour great happy great happy hour hour price happy hour price hour menu happy hour menu happy hour special\n",
      "Topic 20:\n",
      "really good really good wa really good also really good also really really good wa really really good really really good not\n",
      "Topic 21:\n",
      "recommend highly recommend highly would highly would highly recommend would recommend recommend anyone definitely recommend anyone not recommend\n",
      "Topic 22:\n",
      "price great price good price price wa fair price great price good worth price affordable fair price\n",
      "Topic 23:\n",
      "crust thin thin crust topping crust wa crust pizza style thin crust pizza crispy pizza crust\n",
      "Topic 24:\n",
      "go go back definitely go would go definitely go back go again not go time go go out dont go\n",
      "Topic 25:\n",
      "amazing wa amazing amazing wa amazing great wa amazing wa amazing love amazing staff absolutely amazing great amazing die\n",
      "Topic 26:\n",
      "thai pad pad thai best thai thai wa pad thai wa good thai basil pad see tom\n",
      "Topic 27:\n",
      "coffee shop coffee shop coffee wa cup latte great coffee good coffee cup coffee starbucks\n",
      "Topic 28:\n",
      "really really nice really great really like really really liked really liked really enjoyed not really really friendly\n",
      "Topic 29:\n",
      "come come back definitely come definitely come back would come come out come back again would come back would definitely come come again\n",
      "Topic 30:\n",
      "buffet buffet wa station variety best buffet buffet vega line crab leg leg asian\n",
      "Topic 31:\n",
      "salad salad wa dressing chicken salad caesar caesar salad lettuce side salad chopped kale\n",
      "Topic 32:\n",
      "steak steak wa medium rare steakhouse steak egg good steak steak house cheese steak medium rare\n",
      "Topic 33:\n",
      "reasonable reasonable price price reasonable price wa reasonable variety pricing neighborhood delicious price strip\n",
      "Topic 34:\n",
      "delicious wa delicious delicious wa delicious great fresh delicious wa delicious wa everything wa delicious absolutely delicious delicious well delicious also\n",
      "Topic 35:\n",
      "like look look like like wa felt like felt really like seems seems like looked like\n",
      "Topic 36:\n",
      "ice cream ice cream cream wa ice cream wa vanilla scoop cone chocolate vanilla ice\n",
      "Topic 37:\n",
      "egg benedict egg benedict egg roll steak egg poached egg wa scrambled poached egg scrambled egg\n",
      "Topic 38:\n",
      "drink great drink drink wa good drink drink great drink good get drink drink menu cocktail drink not\n",
      "Topic 39:\n",
      "location great location location wa new location location ha good location convenient location great downtown location not\n",
      "Topic 40:\n",
      "excellent wa excellent excellent wa excellent great wa excellent wa excellent price excellent customer excellent well always excellent also excellent\n",
      "Topic 41:\n",
      "work keep keep up people work guy good work up good need work week up great\n",
      "Topic 42:\n",
      "wing chicken wing buffalo pizza wing wing good hot wing crispy ranch boneless wild\n",
      "Topic 43:\n",
      "awesome wa awesome awesome great awesome wa great awesome wa awesome wa pretty awesome totally thanks bomb\n",
      "Topic 44:\n",
      "salmon tuna salmon wa sashimi smoked smoked salmon piece ahi poke japanese\n",
      "Topic 45:\n",
      "staff friendly staff staff friendly wait staff great staff staff wa staff great staff always staff nice good staff\n",
      "Topic 46:\n",
      "pretty pretty good wa pretty wa pretty good pretty much pretty decent pretty tasty good also pretty overall\n",
      "Topic 47:\n",
      "try back try try out decided decided try give try must try wanted try try again try more\n",
      "Topic 48:\n",
      "vega la la vega strip trip vega strip stop vega wa casino im vega\n",
      "Topic 49:\n",
      "out came out come out out wa hang out hang try out world out world take out\n",
      "Topic 50:\n",
      "pho vietnamese broth spring spring roll pho wa best pho mi broth wa bo\n",
      "Topic 51:\n",
      "get always get dont get good get didnt get get out early pay time get get better\n",
      "Topic 52:\n",
      "cheese grilled grilled cheese cheese wa goat tomato goat cheese blue blue cheese cream cheese\n",
      "Topic 53:\n",
      "ive ive ever best ive ive never time ive ive tried ive eaten eaten best ive ever twice\n",
      "Topic 54:\n",
      "would would recommend would definitely would not would go recommend would come good would would like would go back\n",
      "Topic 55:\n",
      "outstanding wa outstanding return definitely return not return would return return again rock never return wait return\n",
      "Topic 56:\n",
      "soup soup wa noodle soup miso miso soup wonton soup salad onion soup wonton soup sour soup\n",
      "Topic 57:\n",
      "choice great choice good choice many choice lot choice healthy choose better choice choice wa variety\n",
      "Topic 58:\n",
      "say say wa would say must say cant say needle needle say let id say let say\n",
      "Topic 59:\n",
      "better much better even better wa better get better way better ive better one better better wa could better\n",
      "Topic 60:\n",
      "im im not fan im glad glad time im im not sure im sure im going ill\n",
      "Topic 61:\n",
      "hot hot sauce wa hot hot chocolate hot dog pot hot pot chocolate hot fresh piping\n",
      "Topic 62:\n",
      "fresh wa fresh ingredient always fresh fresh delicious fresh ingredient fresh tasty made fresh great fresh good fresh\n",
      "Topic 63:\n",
      "star five five star reason one star giving give star rating four gave\n",
      "Topic 64:\n",
      "little wa little great little little pricey little bit pricey little more little slow nice little cute\n",
      "Topic 65:\n",
      "dont dont know dont like dont get dont think dont go expect dont want forget dont expect\n",
      "Topic 66:\n",
      "worth not worth well worth definitely worth worth wait wa worth money worth price pricey worth money\n",
      "Topic 67:\n",
      "option vegetarian healthy vegetarian option many option great option lot option healthy option good option better option\n",
      "Topic 68:\n",
      "meal meal wa great meal good meal best meal bad meal enjoyed meal end meal meal great kid meal\n",
      "Topic 69:\n",
      "fry french fry burger fry potato fry sweet potato fry fry wa fry good french cheese fry home fry\n",
      "Topic 70:\n",
      "no no one wa no complaint no complaint no longer longer matter no matter no flavor\n",
      "Topic 71:\n",
      "eat eat again could eat great eat not eat dont eat favorite eat would eat want eat good eat\n",
      "Topic 72:\n",
      "dish dish wa side dish deep dish deep favorite dish main dish main every dish pasta dish\n",
      "Topic 73:\n",
      "first first time wa first wa first time first time wa time first visit second went first first time trying\n",
      "Topic 74:\n",
      "rice fried rice rice wa fried rice wa brown rice white rice brown white rice bean sticky rice\n",
      "Topic 75:\n",
      "atmosphere great atmosphere atmosphere wa atmosphere great nice atmosphere good atmosphere atmosphere good love atmosphere fun atmosphere atmosphere friendly\n",
      "Topic 76:\n",
      "nice wa nice really nice nice atmosphere good nice great nice nice wa staff nice super nice nice staff\n",
      "Topic 77:\n",
      "special nothing special special wa daily occasion special occasion daily special great special drink special happy hour special\n",
      "Topic 78:\n",
      "cant beat cant beat cant wait cant get cant say believe cant believe remember cant wait come\n",
      "Topic 79:\n",
      "portion huge size portion size generous huge portion portion huge good portion generous portion large portion\n",
      "Topic 80:\n",
      "friendly friendly staff staff friendly great friendly super friendly always friendly fast friendly friendly helpful friendly great good friendly\n",
      "Topic 81:\n",
      "well done well done well worth good well wa well great well prepared well back well prepared\n",
      "Topic 82:\n",
      "more wa more little more more like much more wish back more bit more more often try more\n",
      "Topic 83:\n",
      "minute waited 20 minute 15 15 minute 20 30 minute 30 waiting minute later\n",
      "Topic 84:\n",
      "wrong go wrong cant go cant go wrong cant go wa wrong anything really cant get wrong\n",
      "Topic 85:\n",
      "pork belly pork belly chop pork chop bun bbq pork duck pork wa mi\n",
      "Topic 86:\n",
      "ha ha great ha always ha good menu ha ha best gone time ha become ha gone\n",
      "Topic 87:\n",
      "server server wa wa server great server ask water good server server wa friendly wa server wa server wa great\n",
      "Topic 88:\n",
      "menu menu wa everything menu menu ha tasting menu tasting menu item thing menu off menu great menu\n",
      "Topic 89:\n",
      "also wa also also good also great good also also got wa also good also really also tried delicious also\n",
      "Topic 90:\n",
      "cant wait wait go cant wait go wait go back go back cant wait back go back try back try\n",
      "Topic 91:\n",
      "roll spring roll spring roll wa egg roll sushi roll tuna california california roll tempura\n",
      "Topic 92:\n",
      "super wa super super friendly super nice staff super wa super friendly staff super friendly super good super fresh staff wa super\n",
      "Topic 93:\n",
      "even not even even though even better didnt even dont even even more wa even even get couldnt\n",
      "Topic 94:\n",
      "friend friend wa friend got friend family group friend meet recommend friend went friend good friend group\n",
      "Topic 95:\n",
      "experience dining great experience dining experience overall experience wa good experience bad experience overall experience pleasant\n",
      "Topic 96:\n",
      "bad not bad wa bad bad experience never bad wasnt bad bad not really bad bad wa bad meal\n",
      "Topic 97:\n",
      "never ive never never disappointed never go never bad never again would never anything time never never disappoints\n",
      "Topic 98:\n",
      "everything everything wa everything else everything wa delicious everything menu everything wa good love everything else everything wa fresh everything wa great\n",
      "Topic 99:\n",
      "ok wa ok ok not ok wa ok nothing wa ok wa wa ok not wa ok nothing ok nothing special not great\n",
      "Topic 100:\n",
      "burrito burrito wa chipotle chicken burrito asada burrito tortilla carne asada burrito guacamole bean cheese chorizo\n",
      "Topic 101:\n",
      "pie slice pot pot pie pie wa apple banana whole key lime\n",
      "Topic 102:\n",
      "gravy biscuit sausage hash poutine brown biscuit gravy hash brown gravy wa corned\n",
      "Topic 103:\n",
      "fast fast friendly wa fast friendly fast good fast out fast fast wa super fast wa fast friendly joint\n",
      "Topic 104:\n",
      "sweet sweet potato sweet potato fry potato fry wa sweet sour sweet sour not sweet savory sweet tea\n",
      "Topic 105:\n",
      "wine glass list wine list bottle glass wine wine selection bottle wine great wine wine wa\n",
      "Topic 106:\n",
      "next next time back next ill back next time next door next time im door time im time\n",
      "Topic 107:\n",
      "customer great customer customer wa employee rude good customer business excellent customer care manager\n",
      "Topic 108:\n",
      "not wa not not good good not not great doe not doe would not not worth not bad\n",
      "Topic 109:\n",
      "coming coming back definitely coming definitely coming back keep keep coming keep coming back back not coming not coming back\n",
      "Topic 110:\n",
      "every every time every time come every time go time come time go every single single every day worth every\n",
      "Topic 111:\n",
      "chinese best chinese good chinese authentic chinese takeout chow china mein sour typical\n",
      "Topic 112:\n",
      "meat meat wa smoked meat smoked tender cut meat lover quality meat piece lover\n",
      "Topic 113:\n",
      "tea boba milk iced iced tea milk tea tea wa bubble green tea bubble tea\n",
      "Topic 114:\n",
      "beef corned corned beef beef wa beef sandwich roast beef hash roast beef mongolian mongolian beef\n",
      "Topic 115:\n",
      "crab leg crab leg seafood crab cake oyster king king crab snow crab snow\n",
      "Topic 116:\n",
      "favorite one favorite new favorite favorite spot favorite eat favorite dish wa favorite favorite wa far favorite go\n",
      "Topic 117:\n",
      "table table wa empty dirty sat reservation sit seat seated get table\n",
      "Topic 118:\n",
      "ever ive ever worst best ever best ive ever ever eaten worst ever ever wa life eaten\n",
      "Topic 119:\n",
      "going going back definitely going definitely going back not going wa going not going back im going wont going keep going\n",
      "Topic 120:\n",
      "spot great spot hit spot favorite spot good spot little spot go spot local spot hit spot wa\n",
      "Topic 121:\n",
      "again back again go again again again come back again go back again come again eat again back again again try again\n",
      "Topic 122:\n",
      "didnt didnt even didnt like didnt get anything didnt want didnt know didnt seem didnt care care\n",
      "Topic 123:\n",
      "area dining area dining area wa seating area stop phoenix seating phoenix area youre area\n",
      "Topic 124:\n",
      "down hand hand down down best hand down best sit down sit street down street let\n",
      "Topic 125:\n",
      "new york new york new favorite new location style something new new menu great new try new\n",
      "Topic 126:\n",
      "up ended ended up pick pick up fill fill up end up make up end\n",
      "Topic 127:\n",
      "got also got got wa got chicken time got good got friend got finally got got home husband got\n",
      "Topic 128:\n",
      "rib prime prime rib short short rib rib wa prime rib wa rib eye bone eye\n",
      "Topic 129:\n",
      "french toast french toast toast wa french toast wa french fry banana french onion stuffed french onion soup\n",
      "Topic 130:\n",
      "tasty wa tasty fresh tasty really tasty tasty wa pretty tasty quite tasty tasty not tasty well tasty fresh\n",
      "Topic 131:\n",
      "pulled pulled pork pork brisket pork sandwich pulled pork sandwich pork wa pulled pork wa slaw poutine\n",
      "Topic 132:\n",
      "bit wa bit little bit pricey bit pricey bit more bit slow overall bit expensive price bit\n",
      "Topic 133:\n",
      "lot parking parking lot not lot lot people lot more ha lot lot option lot better choose\n",
      "Topic 134:\n",
      "fantastic wa fantastic fantastic wa cocktail view unique incredible week extremely wow\n",
      "Topic 135:\n",
      "year ago year ago old year old past last year coming year couple 10 year\n",
      "Topic 136:\n",
      "one one best wa one one favorite one wa no one one star not one one thing good one\n",
      "Topic 137:\n",
      "fish fish chip fish taco fish wa chip batter fresh fish good fish cod piece\n",
      "Topic 138:\n",
      "take take out home take home take care care take long take time great take dont take\n",
      "Topic 139:\n",
      "side side dish side salad side wa side town west west side one side pricey side table side\n",
      "Topic 140:\n",
      "yummy wa yummy super yummy really yummy cute oh soooo delish huge sooo\n",
      "Topic 141:\n",
      "taste taste like taste good taste wa taste great taste bud taste fresh bud didnt taste everything taste\n",
      "Topic 142:\n",
      "mexican authentic mexican best mexican margarita enchilada good mexican favorite mexican tortilla fajitas guacamole\n",
      "Topic 143:\n",
      "spicy spice not spicy tuna wa spicy spicy tuna spicy chicken like spicy level spicy sauce\n",
      "Topic 144:\n",
      "family owned family owned run friend family family run great family family friendly family friend style\n",
      "Topic 145:\n",
      "high end high quality high end expectation price high high expectation high price little high school\n",
      "Topic 146:\n",
      "visit first visit visit wa visit again next visit last visit time visit second second visit worth visit\n",
      "Topic 147:\n",
      "chip salsa chip salsa margarita guacamole salsa wa fish chip enchilada tortilla nacho\n",
      "Topic 148:\n",
      "way out way wa way way better go out way way go go out way much way overpriced way home\n",
      "Topic 149:\n",
      "sauce sauce wa hot sauce dipping dipping sauce bbq sauce tomato red tomato sauce extra\n",
      "Topic 150:\n",
      "waitress waitress wa water ask waitress wa nice waitress wa friendly brought great waitress asked wa waitress\n",
      "Topic 151:\n",
      "dumpling steamed pan filling boiled pan fried bao asian chive skin\n",
      "Topic 152:\n",
      "noodle noodle soup broth beef noodle rice noodle noodle dish noodle wa wonton drunken noodle asian\n",
      "Topic 153:\n",
      "bbq brisket bbq sauce bbq chicken best bbq korean bbq bbq pork good bbq rib brisket wa\n",
      "Topic 154:\n",
      "beer tap beer selection great beer beer wa beer tap craft good beer craft beer draft\n",
      "Topic 155:\n",
      "make make sure reservation make reservation make up make feel make great always make stop would make\n",
      "Topic 156:\n",
      "perfect wa perfect perfect amount amount perfect wa date cooked perfect balance combination size\n",
      "Topic 157:\n",
      "clean wa clean nice clean bathroom clean staff always clean clean friendly clean wa super clean clean well\n",
      "Topic 158:\n",
      "much much better not much pretty much wa much much more way much like much wa much better doesnt\n",
      "Topic 159:\n",
      "youre looking youre looking forward looking forward youre not looking good wa looking youre going youll\n",
      "Topic 160:\n",
      "flavor flavor wa full no flavor great flavor full flavor good flavor bland lacked flavor not\n",
      "Topic 161:\n",
      "review yelp yelp review read based reading write review wa good review read review\n",
      "Topic 162:\n",
      "selection beer selection great selection good selection selection wa wine selection selection beer selection great wide nice selection\n",
      "Topic 163:\n",
      "ramen broth ramen wa miso japanese gyoza broth wa salty extra japan\n",
      "Topic 164:\n",
      "priced reasonably reasonably priced well priced fairly decently strip consistently moderately great tasting\n",
      "Topic 165:\n",
      "want dont want want go didnt want want try want eat want good not want really want want come\n",
      "Topic 166:\n",
      "wa great great wa great wa wa great wa server wa great everything wa great wa great well wa great server atmosphere wa\n",
      "Topic 167:\n",
      "garlic knot garlic knot garlic bread garlic sauce naan garlic naan oil pepper butter\n",
      "Topic 168:\n",
      "free gluten gluten free get free gf wifi coupon free wifi offer allergy\n",
      "Topic 169:\n",
      "shrimp shrimp wa cocktail seafood shrimp taco grit shrimp cocktail shrimp grit tempura coconut\n",
      "Topic 170:\n",
      "still wa still still good still great still not im still hungry used still waiting wa still good\n",
      "Topic 171:\n",
      "quick wa quick bite quick bite quick friendly good quick stop grab pretty quick friendly quick\n",
      "Topic 172:\n",
      "loved absolutely loved loved wa loved everything really loved daughter boyfriend soon liked hubby\n",
      "Topic 173:\n",
      "top notch top notch wa top wa top notch top off list one top top wa egg top\n",
      "Topic 174:\n",
      "waiter waiter wa ask water bill brought great waiter waiter wa friendly seated wa waiter\n",
      "Topic 175:\n",
      "onion ring onion ring pepper onion soup tomato french onion french onion soup lettuce mushroom\n",
      "Topic 176:\n",
      "fried fried chicken fried rice deep deep fried chicken fried pickle fried pickle fried steak fried rice wa\n",
      "Topic 177:\n",
      "dim sum sum dim cart bun mai weekend pork bun custard chinatown\n",
      "Topic 178:\n",
      "big fan big fan not big deal big portion not big fan big deal wa big im not big\n",
      "Topic 179:\n",
      "wonderful wa wonderful wonderful experience thank beautiful fabulous incredible soon truly back soon\n",
      "Topic 180:\n",
      "people great people nice people two people watching people watching working many people people working people work\n",
      "Topic 181:\n",
      "tried ive tried havent havent tried also tried tried wa yet never tried everything tried time tried\n",
      "Topic 182:\n",
      "could wish could not wish could use could eat could give could use see could get\n",
      "Topic 183:\n",
      "decent wa decent decent price pretty decent decent not decent wa price decent overall decent nothing wa pretty decent\n",
      "Topic 184:\n",
      "room dining hotel dining room stay room wa floor view casino stayed\n",
      "Topic 185:\n",
      "music live live music loud music wa playing band club play dance\n",
      "Topic 186:\n",
      "two three two people two star piece four one two two time slice enough two\n",
      "Topic 187:\n",
      "bean black black bean red rice bean green bean bean rice tortilla red bean refried\n",
      "Topic 188:\n",
      "day next day day wa every day one day week time day valentine valentine day mother day\n",
      "Topic 189:\n",
      "enjoyed really enjoyed enjoyed meal thoroughly enjoyed thoroughly enjoyed experience also enjoyed enjoyed wa enjoyed much variety\n",
      "Topic 190:\n",
      "brunch sunday sunday brunch mimosa brunch wa morning weekend brunch menu benedict sunday morning\n",
      "Topic 191:\n",
      "must must try must say must go must visit must stop stop must say wa die admit\n",
      "Topic 192:\n",
      "know dont know let didnt know know wa let know name guy not know know good\n",
      "Topic 193:\n",
      "dog hot dog chili dog wa chicago bun hotdog chili cheese mustard tot\n",
      "Topic 194:\n",
      "crepe crepe wa nutella savory banana strawberry fruit chocolate paris savoury\n",
      "Topic 195:\n",
      "asada carne carne asada asada taco carne asada taco asada burrito carne asada burrito nacho quesadilla pastor\n",
      "Topic 196:\n",
      "bread bread wa butter pudding bread pudding garlic bread served soft warm baked\n",
      "Topic 197:\n",
      "recommended highly recommended highly wa recommended wa highly week world out world reservation phenomenal\n",
      "Topic 198:\n",
      "feel feel like make feel home make feel like made feel dont feel didnt feel welcome feel welcome\n",
      "Topic 199:\n",
      "fun wa fun fun atmosphere lot fun much fun chow karaoke environment crowd date\n",
      "Topic 200:\n",
      "came came out came back came wa quickly time came never came came table bill good came\n",
      "Topic 201:\n",
      "find hard hard find find good find out great find find better find something wa hard parking\n",
      "Topic 202:\n",
      "wa good good wa good wa wa good wa wa good not good not good wa good overall everything wa good\n",
      "Topic 203:\n",
      "cooked perfectly wa cooked cooked perfectly perfection cooked perfection perfectly cooked wa cooked perfectly seasoned medium\n",
      "Topic 204:\n",
      "average wa average above above average below below average average best overpriced better average wa above\n",
      "Topic 205:\n",
      "off strip off strip off menu top off first off half start start off half off\n",
      "Topic 206:\n",
      "town out town old best town old town side town im town time town wa town visiting\n",
      "Topic 207:\n",
      "made home wa made home made made fresh reservation made feel made sure made reservation freshly made\n",
      "Topic 208:\n",
      "disappointed wont not disappointed wont disappointed never disappointed wa disappointed wa not disappointed wont back disappointed wa really disappointed\n",
      "Topic 209:\n",
      "something else something else try something anything somewhere somewhere else everything else anything else try something else\n",
      "Topic 210:\n",
      "kid adult kid meal kid menu old son play child daughter year old\n",
      "Topic 211:\n",
      "gyro greek pita hummus greek salad falafel gyro meat tzatziki baklava feta\n",
      "Topic 212:\n",
      "busy wa busy not busy always busy get busy busy wa weekend pretty busy super busy busy time\n",
      "Topic 213:\n",
      "check check out out definitely check definitely check out decided check decided check out decided go check sure check\n",
      "Topic 214:\n",
      "bagel cream cheese cream morning shop man deli toasted ny spread\n",
      "Topic 215:\n",
      "decor ambiance beautiful decor wa great ambiance cute date nice decor ambiance wa modern\n",
      "Topic 216:\n",
      "slow wa slow little slow bit slow extremely slow wa wa little slow extremely slow wa extremely overpriced\n",
      "Topic 217:\n",
      "though even though though wa good though even though wa though not wa good though great though though didnt though im\n",
      "Topic 218:\n",
      "mary bloody bloody mary mimosa benedict vodka saturday bottomless mix hash\n",
      "Topic 219:\n",
      "took forever care took forever took long took care forever get took hour took forever get bite\n",
      "Topic 220:\n",
      "indian naan best indian masala tikka paneer tikka masala butter chicken tikka cuisine\n",
      "Topic 221:\n",
      "game watch tv sport watch game play football screen nacho game wa\n",
      "Topic 222:\n",
      "right away right away amount right amount wa right seated right seated right away seated street\n",
      "Topic 223:\n",
      "lobster bisque lobster roll lobster bisque seafood oyster lobster wa tail lobster tail lobster mac\n",
      "Topic 224:\n",
      "sure not sure make sure im not sure back sure im sure sure wa made sure sure get im not\n",
      "Topic 225:\n",
      "around corner around corner walk came around walking walk around walking around around wa see\n",
      "Topic 226:\n",
      "owner owner wa business new owner care guy owner came welcoming see extremely\n",
      "Topic 227:\n",
      "absolutely wa absolutely absolutely delicious absolutely love absolutely amazing wa absolutely delicious absolutely loved absolutely no incredible extremely\n",
      "Topic 228:\n",
      "drive thru drive thru worth drive car window home employee mcdonalds inside\n",
      "Topic 229:\n",
      "think dont think think wa think would ill think ill thats didnt think would think maybe\n",
      "Topic 230:\n",
      "curry curry wa curry chicken green curry chicken curry panang red curry yellow red yellow curry\n",
      "Topic 231:\n",
      "wasnt impressed wasnt bad wasnt good wasnt great either wasnt impressed wasnt even wasnt busy expecting\n",
      "Topic 232:\n",
      "lamb chop lamb chop lamb wa kabob tender skewer shank duck rack\n",
      "Topic 233:\n",
      "need need more need get really need dont need need go need work need try no need help\n",
      "Topic 234:\n",
      "different something different type variety different type many different wa different try different different kind try something different\n",
      "Topic 235:\n",
      "nothing nothing special home write home write nothing write nothing write home wa nothing good nothing ok nothing\n",
      "Topic 236:\n",
      "quite wa quite quite good wa quite good quite tasty not quite quite bit actually quite time quite nice\n",
      "Topic 237:\n",
      "husband husband got husband love husband went husband wa son weve ate anniversary liked\n",
      "Topic 238:\n",
      "bowl poke bowl wa rice bowl teriyaki healthy chipotle juice topping extra\n",
      "Topic 239:\n",
      "give another give try give star give another would give shot decided give decided chance\n",
      "Topic 240:\n",
      "waffle chicken waffle waffle wa syrup waffle fry butter crispy belgian banana strawberry\n",
      "Topic 241:\n",
      "pasta pasta wa pasta dish meatball tomato pizza pasta spaghetti pasta salad alfredo seafood\n",
      "Topic 242:\n",
      "pancake blueberry omelette omelet fluffy red velvet velvet pancake wa red banana\n",
      "Topic 243:\n",
      "italian best italian authentic italian italy good italian real meatball sausage veal spaghetti\n",
      "Topic 244:\n",
      "cold wa cold warm served cold beer cold wa ice cold out cold worst brew\n",
      "Topic 245:\n",
      "since since wa opened since opened time since havent moved decided ever since especially since\n",
      "Topic 246:\n",
      "wife wife got stopped wife wa daughter ate split liked son anniversary\n",
      "Topic 247:\n",
      "wa friendly staff wa staff wa friendly friendly wa helpful staff friendly helpful wa friendly wa friendly wa\n",
      "Topic 248:\n",
      "local business local business support great local support local local spot chain supporting neighborhood\n",
      "Topic 249:\n",
      "delivery guy called ordering driver phone delivery wa call deliver hour\n",
      "Topic 250:\n",
      "item menu item item menu item wa try item many item item not item like store favorite item\n",
      "Topic 251:\n",
      "small small portion wa small small plate portion small piece small price size space pretty small\n",
      "Topic 252:\n",
      "green chili chile red green chile green tea green chili green bean green curry pepper\n",
      "Topic 253:\n",
      "dessert dessert wa chocolate cheesecake pudding creme good dessert gelato course room dessert\n",
      "Topic 254:\n",
      "many many time not many many option many people many many many different ive many many year many choice\n",
      "Topic 255:\n",
      "bartender bartender wa cocktail sat margarita guy rude shot walked patron\n",
      "Topic 256:\n",
      "donut dozen glazed fritter glaze apple maple shop topping sugar\n",
      "Topic 257:\n",
      "tasted tasted like bland tasted good ever tasted tasted great water old tasted fresh everything tasted\n",
      "Topic 258:\n",
      "cake chocolate crab cake cake wa chocolate cake hot chocolate birthday bakery butter pastry\n",
      "Topic 259:\n",
      "enough not enough good enough wa enough say enough share more enough get enough big enough cant say enough\n",
      "Topic 260:\n",
      "last last time last night week last week last time wa time last went last last visit last year\n",
      "Topic 261:\n",
      "usually usually get usually go usually pretty often usually good today usually dont sometimes usually not\n",
      "Topic 262:\n",
      "min 20 20 min waited 30 min 30 waiting 15 min 15 10 min\n",
      "Topic 263:\n",
      "veggie veggie burger grilled vegetarian wrap tofu fresh veggie meat veggie mushroom vegetable\n",
      "Topic 264:\n",
      "authentic authentic mexican japanese not authentic authentic italian cuisine real style authentic chinese vietnamese\n",
      "Topic 265:\n",
      "potato sweet potato mashed mashed potato potato fry sweet potato fry potato salad baked baked potato potato wa\n",
      "Topic 266:\n",
      "mac mac cheese cheese mac cheese wa cheese wa lobster mac lobster mac cheese brisket cheesy cornbread\n",
      "Topic 267:\n",
      "okay wa okay okay not okay wa okay nothing wa okay not thought overall bland thought wa\n",
      "Topic 268:\n",
      "cheap not cheap good cheap cheap price pay look isnt eats wall hole\n",
      "Topic 269:\n",
      "said asked manager told wanted called said wa wa told card rude\n",
      "Topic 270:\n",
      "went went back time went today went wa went today went first went last went friend ago\n",
      "Topic 271:\n",
      "vegan vegetarian vegan option tofu nacho raw organic vegetarian option cupcake soy\n",
      "Topic 272:\n",
      "enjoy really enjoy sit enjoy meal youll not enjoy didnt enjoy relax able job\n",
      "Topic 273:\n",
      "korean korean bbq kimchi tofu bulgogi japanese ayce side dish bibimbap grill\n",
      "Topic 274:\n",
      "attentive wa attentive friendly attentive attentive friendly server wa attentive attentive wa wa friendly attentive staff wa attentive staff attentive nice attentive\n",
      "Topic 275:\n",
      "chef sushi chef course chef wa kitchen cook tasting show see cooking\n",
      "Topic 276:\n",
      "found gem hidden hidden gem glad yelp found out little gem found yelp ive found\n",
      "Topic 277:\n",
      "terrible wa terrible worst awful overpriced mediocre bland not terrible poor rude\n",
      "Topic 278:\n",
      "large group large group large portion portion large large party wa large large selection group friend group people\n",
      "Topic 279:\n",
      "kind wa kind different kind kind like store actually weird stuff wall inside\n",
      "Topic 280:\n",
      "plate plate wa small plate served cheese plate full brought combo tapa put\n",
      "Topic 281:\n",
      "yum yum yum tom tom yum fabulous oh glad seriously die meatball\n",
      "Topic 282:\n",
      "long line long time long line long wait waiting line wa wait long wa long took long\n",
      "Topic 283:\n",
      "10 10 minute 10 year under 10 min under 10 minute another 10 waited 10 out 10\n",
      "Topic 284:\n",
      "cool vibe really cool pretty cool wa cool old hang hang out look hipster\n",
      "Topic 285:\n",
      "sub meatball shop sub par par subway john turkey jimmy store\n",
      "Topic 286:\n",
      "party everyone birthday reservation everyone wa party wa large party birthday party seated everyone else\n",
      "Topic 287:\n",
      "appetizer entree main course appetizer wa main course calamari appetizer entree dip scallop\n",
      "Topic 288:\n",
      "probably id probably best wont probably wont wa probably probably not probably one would probably id probably\n",
      "Topic 289:\n",
      "patio outside inside seating outdoor sit view outdoor seating beautiful sit outside\n",
      "Topic 290:\n",
      "wa really wa really good wa wa really nice really nice wa really friendly good wa really good wa really friendly wa really really\n",
      "Topic 291:\n",
      "hit miss hit miss hit spot sometimes dont miss hit up wa hit craving depending\n",
      "Topic 292:\n",
      "definitely would definitely definitely back definitely recommend definitely go definitely come definitely come back definitely go back definitely not would definitely recommend\n",
      "Topic 293:\n",
      "quality good quality high quality ingredient quality wa great quality quality ingredient quantity quality ha quality good\n",
      "Topic 294:\n",
      "house close steak house close house margarita house made house salad house special house wa made house\n",
      "Topic 295:\n",
      "bacon bacon wa wrapped bacon wrapped egg bacon turkey avocado cheeseburger bacon egg tomato\n",
      "Topic 296:\n",
      "open late closed open late late night door theyre 24 hour 24 hour\n",
      "Topic 297:\n",
      "horrible rude money wa horrible worst waste wa rude waste money waste time disgusting\n",
      "Topic 298:\n",
      "eating stop wa eating like eating time eating up eating first time eating sick eating wa stop eating\n",
      "Topic 299:\n",
      "especially good especially great especially especially since especially price weekend especially considering considering liked summer\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "display_topics(nmf, tfidf_vectorizer_mini__stop_ngram.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create corpus of sentences from mini\n",
    "sentence_corpus_mini = []\n",
    "for review in X_mini:\n",
    "    words = review.split(\"', '\")\n",
    "    words[0] = words[0][2:]\n",
    "    words[-1] = words[-1][:-2]\n",
    "    sentence_corpus_mini.append(' '.join(words))\n",
    "\n",
    "# Create tokenized corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in sentence_corpus_mini]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions to create a feature array\n",
    "def average_word_vectors(words, model, vocabulary, num_features):   \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.   \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    return feature_vector\n",
    "    \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100678, 100)\n"
     ]
    }
   ],
   "source": [
    "# Create word 2 vec model \n",
    "feature_size = 100\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, window=5, min_count=10, workers=4)\n",
    "# w2v_dictionary = dict(zip(w2v_model.wv.index2word, w2v_model.wv.syn0))\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n",
    "                                             num_features=feature_size)\n",
    "w2v_mini = pd.DataFrame(w2v_feature_array)\n",
    "print(w2v_mini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100678, 200)\n"
     ]
    }
   ],
   "source": [
    "# Create word 2 vec model with 200 features\n",
    "feature_size_2 = 200\n",
    "w2v_model_2 = word2vec.Word2Vec(tokenized_corpus, size=feature_size_2, window=5, min_count=10, workers=4)\n",
    "# w2v_dictionary_2 = dict(zip(w2v_model_2.wv.index2word, w2v_model_2.wv.syn0))\n",
    "w2v_feature_array_2 = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model_2,\n",
    "                                             num_features=feature_size_2)\n",
    "w2v_mini_2 = pd.DataFrame(w2v_feature_array_2)\n",
    "print(w2v_mini_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100678, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Create word 2 vec model with 1000 features\n",
    "feature_size_3 = 1000\n",
    "w2v_model_3 = word2vec.Word2Vec(tokenized_corpus, size=feature_size_3, window=5, min_count=10, workers=4)\n",
    "# w2v_dictionary_3 = dict(zip(w2v_model_3.wv.index2word, w2v_model_3.wv.syn0))\n",
    "w2v_feature_array_3 = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model_3,\n",
    "                                             num_features=feature_size_3)\n",
    "w2v_mini_3 = pd.DataFrame(w2v_feature_array_3)\n",
    "print(w2v_mini_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using mini dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mini dataset, a variety of models will be trained on a variety of feature sets to identify promising candidates. The promising combinations will then be tuned in the following section and trained on the full training data set. \n",
    "\n",
    "It should be noted that to assess model performance, the classification accuracy will be the primary metric. \n",
    "A Confusion matrix will be created using the best performing parameters from the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model tuning\n",
    "def cross_validation_tuning(classifier, param_grid, X_trn, y_trn):\n",
    "    classifier_cv = GridSearchCV(classifier, param_grid, cv=3)\n",
    "    classifier_cv.fit(X_trn, y_trn)\n",
    "    # Print the optimal parameters and best score\n",
    "    print(\"Tuned Classifier Parameters: {}\".format(classifier_cv.best_params_))\n",
    "    print(\"Tuned Classifier Accuracy: {:.3f}\".format(classifier_cv.best_score_))\n",
    "    # Predict the labels\n",
    "    pred = classifier_cv.predict(X_trn)\n",
    "    # Compute accuracy\n",
    "    score = metrics.accuracy_score(y_trn, pred)\n",
    "    # Calculate and print the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_trn, pred, labels=[1,2,3,4,5])\n",
    "    print('For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.')\n",
    "    print(cm)\n",
    "    return classifier_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Naive_bayes model\n",
    "def nb_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'alpha': np.arange(0, 1, 0.333)}\n",
    "    # Iterate over the alphas and print the corresponding score\n",
    "    nb_classifier = MultinomialNB()\n",
    "    tuned_nb_classifier = cross_validation_tuning(nb_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_nb_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Logistic regression model\n",
    "def logreg_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'C': [0.0001, 1, 100], 'penalty': ['l1', 'l2']} #param_grid = {'C': np.logspace(-5, 8, 15), 'penalty': ['l1', 'l2']}\n",
    "    logreg_classifier = LogisticRegression()\n",
    "    tuned_logreg_classifier = cross_validation_tuning(logreg_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_logreg_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define SVM model\n",
    "def svm_model(X_trn, y_trn):\n",
    "    # Create parameters \n",
    "    param_grid = {'kernel': ['rbf', 'linear']} #{'kernel': ['rbf', 'linear'], 'C': [1, 10, 100], 'gamma': [0.1, 0.01]} \n",
    "    svm_classifier = SVC()\n",
    "    tuned_svm_classifier = cross_validation_tuning(svm_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_svm_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Random forest model\n",
    "def ranforest_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {\"n_estimators\": [150, 300, 500],\n",
    "                  \"min_samples_leaf\": [5, 10]}\n",
    "    # param_grid = {\"n_estimators\": [2, 10, 100, 300, 1000],\"max_depth\": [2, 10, 100, 300], \"min_samples_split\": [2, 10, 100],\"min_samples_leaf\": [1, 10, 100]}\n",
    "    ranforest_classifier = RandomForestClassifier()\n",
    "    tuned_ranforest_classifier = cross_validation_tuning(ranforest_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_ranforest_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define SGD model \n",
    "def sgd_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {\"penalty\": ['l1', 'l2', 'elasticnet'],\n",
    "                  \"l1_ratio\": [0.1, 0.3, 0.5] }\n",
    "    sgd_classifier = SGDClassifier(random_state= 42, max_iter=4)\n",
    "    tuned_sgd_classifier = cross_validation_tuning(sgd_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_sgd_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define XGBoost model \n",
    "def xgb_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'min_child_weight': [3],\n",
    "                 'max_depth': [4]}\n",
    "    xgb_classifier = XGBClassifier(learning_rate =0.2, seed=42)\n",
    "    tuned_xgb_classifier = cross_validation_tuning(xgb_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_xgb_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "def construct_dnn(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation ='relu', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(512, activation ='relu'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "#     dnn_model.add(Dense(512, activation ='relu'))\n",
    "#     dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model\n",
    "\n",
    "# Build model\n",
    "def dnn_model(X_trn, y_trn):\n",
    "    n_cols = X_trn.shape[1]\n",
    "    input_shape =(n_cols, )\n",
    "    model = construct_dnn(input_shape)\n",
    "    \n",
    "    # Define early_stopping_monitor\n",
    "    early_stopping_monitor = EarlyStopping(patience=2)\n",
    "    # Define fit\n",
    "    history = model.fit(X_trn, pd.get_dummies(y_trn), epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline assumes review is a 5 star rating (the most common class of data). The corresponding baseline accuracy is ~ 36.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36932596992391586\n"
     ]
    }
   ],
   "source": [
    "# Calculate baseline\n",
    "length = len(y_mini)\n",
    "correct_pred = len(y_mini[y_mini == 5])\n",
    "baseline_accuracy = correct_pred / length \n",
    "print(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_sets = {'count': count_mini, \n",
    "                'tfidf': tfidf_mini, \n",
    "                'count stop': count_mini__stop, \n",
    "                'tfidf stop': tfidf_mini__stop, \n",
    "                'count stop ngram': count_mini__stop_ngram, \n",
    "                'tfidf stop ngram': tfidf_mini__stop_ngram,\n",
    "                'count stop ngram lda': count_mini__stop_ngram_lda.tocsr(), \n",
    "                'tfidf stop ngram nmf': tfidf_train_mini__stop_ngram_nmf.tocsr(),\n",
    "                'word to vec': w2v_mini, \n",
    "                'word to vec 2': w2v_mini_2, \n",
    "                'word to vec 3': w2v_mini_3}\n",
    "non_negative_feature_sets = feature_sets.copy()\n",
    "del non_negative_feature_sets['word to vec']\n",
    "del non_negative_feature_sets['word to vec 2']\n",
    "del non_negative_feature_sets['word to vec 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define test for feature sets\n",
    "def test_features(model, sets):\n",
    "    results = defaultdict(float)\n",
    "    for key, x_values_mini in sets.items():\n",
    "        print(key)\n",
    "        model_instance = model(x_values_mini, y_mini)\n",
    "        results[key] = model_instance.best_score_\n",
    "        print('')\n",
    "    print('--------------------------')\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define deep neural net tests for feature sets\n",
    "def dnn_test_results(sets):\n",
    "    results = defaultdict(float)\n",
    "    for key, x_values_mini in sets.items():\n",
    "        print(key)\n",
    "        model_instance, history = dnn_model(x_values_mini, y_mini)\n",
    "        results[key] = max(history.history['val_acc'])\n",
    "        print('')\n",
    "    print('--------------------------')\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.66600000000000004}\n",
      "Tuned Classifier Accuracy: 0.580\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8099  2080   594   222   276]\n",
      " [ 2621  3829  2388   656   361]\n",
      " [ 1330  1785  6371  3815  1278]\n",
      " [  646   747  3059 13365  9973]\n",
      " [  680   325   804  6087 29287]]\n",
      "\n",
      "tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.0}\n",
      "Tuned Classifier Accuracy: 0.555\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 7964   924   582   912   889]\n",
      " [ 2505  1861  1832  2622  1035]\n",
      " [ 1150   431  2759  7601  2638]\n",
      " [  404    76   422 13522 13366]\n",
      " [  280    19    66  4593 32225]]\n",
      "\n",
      "count stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.66600000000000004}\n",
      "Tuned Classifier Accuracy: 0.590\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8176  2114   587   184   210]\n",
      " [ 2612  3977  2442   547   277]\n",
      " [ 1270  1825  6694  3734  1056]\n",
      " [  631   754  3043 13626  9736]\n",
      " [  655   295   747  6104 29382]]\n",
      "\n",
      "tfidf stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.0}\n",
      "Tuned Classifier Accuracy: 0.564\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8078   941   570   896   786]\n",
      " [ 2504  1911  1971  2582   887]\n",
      " [ 1092   452  3121  7626  2288]\n",
      " [  390    69   464 13897 12970]\n",
      " [  267    18    62  4629 32207]]\n",
      "\n",
      "count stop ngram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.33300000000000002}\n",
      "Tuned Classifier Accuracy: 0.606\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8402  2129   472   129   139]\n",
      " [ 2394  4664  2196   394   207]\n",
      " [ 1075  1863  7607  3151   883]\n",
      " [  494   656  3242 14355  9043]\n",
      " [  511   280   728  6373 29291]]\n",
      "\n",
      "tfidf stop ngram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.0}\n",
      "Tuned Classifier Accuracy: 0.602\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8538  1332   596   450   355]\n",
      " [ 2293  3436  2335  1373   418]\n",
      " [  952   845  5489  5925  1368]\n",
      " [  281   143  1107 15697 10562]\n",
      " [  230    41   159  5543 31210]]\n",
      "\n",
      "count stop ngram lda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.33300000000000002}\n",
      "Tuned Classifier Accuracy: 0.606\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8401  2110   468   132   160]\n",
      " [ 2407  4628  2188   411   221]\n",
      " [ 1078  1855  7556  3153   937]\n",
      " [  493   646  3229 14211  9211]\n",
      " [  506   282   727  6262 29406]]\n",
      "\n",
      "tfidf stop ngram nmf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.0}\n",
      "Tuned Classifier Accuracy: 0.603\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8545  1343   593   444   346]\n",
      " [ 2296  3461  2334  1352   412]\n",
      " [  955   853  5564  5858  1349]\n",
      " [  282   148  1136 15707 10517]\n",
      " [  231    43   167  5581 31161]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.58025586523371542, 'tfidf': 0.5547289378017044, 'count stop': 0.59016865650886985, 'tfidf stop': 0.56352927153896581, 'count stop ngram': 0.60640855003079119, 'tfidf stop ngram': 0.60210770972804384, 'count stop ngram lda': 0.60632908877808456, 'tfidf stop ngram nmf': 0.60290232225511031})\n"
     ]
    }
   ],
   "source": [
    "# run tests for Naive Bayes\n",
    "NB_mini_results = test_features(nb_model, non_negative_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.580\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9032   912   404   342   581]\n",
      " [ 2217  4029  1740  1122   747]\n",
      " [  792  1124  6057  4603  2003]\n",
      " [  207   242  1558 14362 11421]\n",
      " [  127    79   295  4608 32074]]\n",
      "\n",
      "tfidf\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.596\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8871  1120   475   368   437]\n",
      " [ 2520  3401  2133  1200   601]\n",
      " [  879  1090  5797  5189  1624]\n",
      " [  256   222  1525 15239 10548]\n",
      " [  175    67   309  5462 31170]]\n",
      "\n",
      "count stop\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.588\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9133   921   396   311   510]\n",
      " [ 2193  4171  1832  1026   633]\n",
      " [  763  1141  6408  4481  1786]\n",
      " [  211   233  1557 14650 11139]\n",
      " [  118    75   282  4489 32219]]\n",
      "\n",
      "tfidf stop\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.608\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8965  1137   474   312   383]\n",
      " [ 2497  3519  2254  1080   505]\n",
      " [  809  1072  6244  5022  1432]\n",
      " [  246   209  1605 15299 10431]\n",
      " [  163    62   301  5330 31327]]\n",
      "\n",
      "count stop ngram\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.594\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9810   693   302   194   272]\n",
      " [ 1575  5799  1360   702   419]\n",
      " [  523  1018  8135  3547  1356]\n",
      " [  158   201  1432 16551  9448]\n",
      " [   81    43   251  4124 32684]]\n",
      "\n",
      "tfidf stop ngram\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.621\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9174  1118   433   265   281]\n",
      " [ 2372  4063  2108   913   399]\n",
      " [  721  1059  6915  4626  1258]\n",
      " [  198   163  1494 16180  9755]\n",
      " [  124    45   271  5009 31734]]\n",
      "\n",
      "count stop ngram lda\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.595\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9826   703   330   199   213]\n",
      " [ 1575  5812  1406   706   356]\n",
      " [  549   990  8299  3518  1223]\n",
      " [  159   207  1510 16780  9134]\n",
      " [  103    56   274  4354 32396]]\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.621\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9173  1119   433   265   281]\n",
      " [ 2372  4061  2110   913   399]\n",
      " [  721  1061  6913  4628  1256]\n",
      " [  198   163  1495 16177  9757]\n",
      " [  122    45   271  5012 31733]]\n",
      "\n",
      "word to vec\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.571\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8453   639   683   736   760]\n",
      " [ 3256  1233  2281  2255   830]\n",
      " [ 1281   604  3897  6895  1902]\n",
      " [  536   145  1391 13741 11977]\n",
      " [  443    37   314  5957 30432]]\n",
      "\n",
      "word to vec 2\n",
      "Tuned Classifier Parameters: {'C': 100, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.587\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8735   838   575   533   590]\n",
      " [ 3091  1776  2308  1906   774]\n",
      " [ 1093   775  4463  6485  1763]\n",
      " [  404   190  1528 14167 11501]\n",
      " [  336    58   359  5999 30431]]\n",
      "\n",
      "word to vec 3\n"
     ]
    }
   ],
   "source": [
    "# run tests for logistic regression \n",
    "logreg_mini_results = test_features(logreg_model, feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 112s 2ms/step - loss: 0.9655 - acc: 0.5776 - val_loss: 0.9140 - val_acc: 0.6024\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 109s 2ms/step - loss: 0.8105 - acc: 0.6450 - val_loss: 0.9334 - val_acc: 0.5942\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 108s 2ms/step - loss: 0.6271 - acc: 0.7328 - val_loss: 1.0474 - val_acc: 0.5739\n",
      "\n",
      "tfidf\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 107s 2ms/step - loss: 0.9507 - acc: 0.5805 - val_loss: 0.9064 - val_acc: 0.6019\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 108s 2ms/step - loss: 0.8158 - acc: 0.6472 - val_loss: 0.9140 - val_acc: 0.6033\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 107s 2ms/step - loss: 0.6338 - acc: 0.7365 - val_loss: 1.0232 - val_acc: 0.5890\n",
      "\n",
      "count stop\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 114s 2ms/step - loss: 0.9416 - acc: 0.5886 - val_loss: 0.8996 - val_acc: 0.6084\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 113s 2ms/step - loss: 0.7856 - acc: 0.6552 - val_loss: 0.8968 - val_acc: 0.6110\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 113s 2ms/step - loss: 0.6022 - acc: 0.7427 - val_loss: 1.0121 - val_acc: 0.6016\n",
      "Epoch 4/30\n",
      "70474/70474 [==============================] - 113s 2ms/step - loss: 0.4031 - acc: 0.8368 - val_loss: 1.2867 - val_acc: 0.5911\n",
      "\n",
      "tfidf stop\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 115s 2ms/step - loss: 0.9292 - acc: 0.5868 - val_loss: 0.8723 - val_acc: 0.6178\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 115s 2ms/step - loss: 0.7907 - acc: 0.6547 - val_loss: 0.9092 - val_acc: 0.5977\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 112s 2ms/step - loss: 0.6061 - acc: 0.7475 - val_loss: 1.0130 - val_acc: 0.6002\n",
      "\n",
      "count stop ngram\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 215s 3ms/step - loss: 0.9144 - acc: 0.5988 - val_loss: 0.8487 - val_acc: 0.6302\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 214s 3ms/step - loss: 0.7179 - acc: 0.6893 - val_loss: 0.8722 - val_acc: 0.6244\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 216s 3ms/step - loss: 0.4718 - acc: 0.8071 - val_loss: 1.0901 - val_acc: 0.6081\n",
      "\n",
      "tfidf stop ngram\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 220s 3ms/step - loss: 0.8979 - acc: 0.6012 - val_loss: 0.8444 - val_acc: 0.6277\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 231s 3ms/step - loss: 0.7198 - acc: 0.6886 - val_loss: 0.8785 - val_acc: 0.6255\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 220s 3ms/step - loss: 0.4756 - acc: 0.8083 - val_loss: 1.0405 - val_acc: 0.6078\n",
      "\n",
      "count stop ngram lda\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 226s 3ms/step - loss: 0.9140 - acc: 0.5995 - val_loss: 0.8664 - val_acc: 0.6118\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 227s 3ms/step - loss: 0.7173 - acc: 0.6902 - val_loss: 0.8790 - val_acc: 0.6150\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 228s 3ms/step - loss: 0.4737 - acc: 0.8054 - val_loss: 1.0622 - val_acc: 0.6002\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 230s 3ms/step - loss: 0.8983 - acc: 0.5998 - val_loss: 0.8470 - val_acc: 0.6234\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 221s 3ms/step - loss: 0.7222 - acc: 0.6875 - val_loss: 0.8691 - val_acc: 0.6211\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 176s 2ms/step - loss: 0.4812 - acc: 0.8059 - val_loss: 1.0282 - val_acc: 0.6076\n",
      "\n",
      "word to vec\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 19s 272us/step - loss: 1.0169 - acc: 0.5562 - val_loss: 0.9688 - val_acc: 0.5769\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 18s 259us/step - loss: 0.9763 - acc: 0.5742 - val_loss: 0.9601 - val_acc: 0.5791\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 18s 259us/step - loss: 0.9662 - acc: 0.5766 - val_loss: 0.9594 - val_acc: 0.5802\n",
      "Epoch 4/30\n",
      "70474/70474 [==============================] - 18s 263us/step - loss: 0.9588 - acc: 0.5805 - val_loss: 0.9522 - val_acc: 0.5858\n",
      "Epoch 5/30\n",
      "70474/70474 [==============================] - 18s 261us/step - loss: 0.9507 - acc: 0.5821 - val_loss: 0.9582 - val_acc: 0.5787\n",
      "Epoch 6/30\n",
      "70474/70474 [==============================] - 18s 261us/step - loss: 0.9454 - acc: 0.5850 - val_loss: 0.9499 - val_acc: 0.5811\n",
      "Epoch 7/30\n",
      "70474/70474 [==============================] - 18s 259us/step - loss: 0.9391 - acc: 0.5868 - val_loss: 0.9651 - val_acc: 0.5840\n",
      "Epoch 8/30\n",
      "70474/70474 [==============================] - 18s 261us/step - loss: 0.9357 - acc: 0.5864 - val_loss: 0.9492 - val_acc: 0.5871\n",
      "Epoch 9/30\n",
      "70474/70474 [==============================] - 18s 259us/step - loss: 0.9311 - acc: 0.5897 - val_loss: 0.9511 - val_acc: 0.5855\n",
      "Epoch 10/30\n",
      "70474/70474 [==============================] - 18s 260us/step - loss: 0.9260 - acc: 0.5912 - val_loss: 0.9499 - val_acc: 0.5876\n",
      "\n",
      "word to vec 2\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 21s 302us/step - loss: 0.9974 - acc: 0.5637 - val_loss: 0.9431 - val_acc: 0.5890\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 20s 277us/step - loss: 0.9530 - acc: 0.5837 - val_loss: 0.9491 - val_acc: 0.5870\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 20s 277us/step - loss: 0.9403 - acc: 0.5873 - val_loss: 0.9332 - val_acc: 0.5897\n",
      "Epoch 4/30\n",
      "70474/70474 [==============================] - 20s 278us/step - loss: 0.9324 - acc: 0.5916 - val_loss: 0.9200 - val_acc: 0.5962\n",
      "Epoch 5/30\n",
      "70474/70474 [==============================] - 20s 280us/step - loss: 0.9242 - acc: 0.5935 - val_loss: 0.9161 - val_acc: 0.5998\n",
      "Epoch 6/30\n",
      "70474/70474 [==============================] - 20s 277us/step - loss: 0.9166 - acc: 0.5973 - val_loss: 0.9273 - val_acc: 0.5960\n",
      "Epoch 7/30\n",
      "70474/70474 [==============================] - 20s 278us/step - loss: 0.9120 - acc: 0.5970 - val_loss: 0.9184 - val_acc: 0.5978\n",
      "\n",
      "word to vec 3\n",
      "Train on 70474 samples, validate on 30204 samples\n",
      "Epoch 1/30\n",
      "70474/70474 [==============================] - 38s 538us/step - loss: 0.9932 - acc: 0.5654 - val_loss: 0.9406 - val_acc: 0.5890\n",
      "Epoch 2/30\n",
      "70474/70474 [==============================] - 33s 468us/step - loss: 0.9514 - acc: 0.5845 - val_loss: 0.9255 - val_acc: 0.5909\n",
      "Epoch 3/30\n",
      "70474/70474 [==============================] - 33s 465us/step - loss: 0.9389 - acc: 0.5902 - val_loss: 0.9245 - val_acc: 0.5952\n",
      "Epoch 4/30\n",
      "70474/70474 [==============================] - 33s 464us/step - loss: 0.9310 - acc: 0.5915 - val_loss: 0.9246 - val_acc: 0.5916\n",
      "Epoch 5/30\n",
      "70474/70474 [==============================] - 33s 468us/step - loss: 0.9230 - acc: 0.5950 - val_loss: 0.9311 - val_acc: 0.5877\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.60240365512133309, 'tfidf': 0.60333068468541973, 'count stop': 0.61097867829707142, 'tfidf stop': 0.61783207519814409, 'count stop ngram': 0.63024764931007748, 'tfidf stop ngram': 0.62766520988229191, 'count stop ngram lda': 0.61501787841880484, 'tfidf stop ngram nmf': 0.62339425239321755, 'word to vec': 0.58763739902789092, 'word to vec 2': 0.59975499934572962, 'word to vec 3': 0.59518606806256058})\n"
     ]
    }
   ],
   "source": [
    "# run tests for deep neural nets\n",
    "dnn_mini_results = dnn_test_results(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.5, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.550\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 7911  1718   370   621   651]\n",
      " [ 2077  4489   941  1614   734]\n",
      " [  794  2823  2975  6040  1947]\n",
      " [  251   808  1010 14984 10737]\n",
      " [  161   263   275  6936 29548]]\n",
      "\n",
      "tfidf\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.569\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9879   342   214   260   576]\n",
      " [ 4357  2010  1159  1336   993]\n",
      " [ 1740   889  3336  5577  3037]\n",
      " [  609   222   722  9614 16623]\n",
      " [  283    84   164  2128 34524]]\n",
      "\n",
      "count stop\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.3, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.559\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 7694  2124   290   546   617]\n",
      " [ 1811  5173   845  1323   703]\n",
      " [  623  3530  3075  5366  1985]\n",
      " [  219  1042   977 14314 11238]\n",
      " [  129   290   244  6027 30493]]\n",
      "\n",
      "tfidf stop\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.578\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9993   346   237   211   484]\n",
      " [ 4403  2071  1319  1204   858]\n",
      " [ 1624   858  4043  5381  2673]\n",
      " [  550   257   786  9913 16284]\n",
      " [  278    71   174  2092 34568]]\n",
      "\n",
      "count stop ngram\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.3, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.575\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8186  1831   311   431   512]\n",
      " [ 1661  5434   997  1156   607]\n",
      " [  557  2557  4865  4922  1678]\n",
      " [  171   642  1232 15590 10155]\n",
      " [   98   173   229  5629 31054]]\n",
      "\n",
      "tfidf stop ngram\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.594\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[10179   293   221   152   426]\n",
      " [ 4297  2573  1340   901   744]\n",
      " [ 1519   716  5246  4637  2461]\n",
      " [  446   157   777 10639 15771]\n",
      " [  225    59   123  1917 34859]]\n",
      "\n",
      "count stop ngram lda\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.3, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.573\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8498  1674   293   377   429]\n",
      " [ 1748  5506   951  1092   558]\n",
      " [  641  2558  4788  4965  1627]\n",
      " [  207   657  1052 15589 10285]\n",
      " [  109   157   211  5385 31321]]\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.594\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[10177   299   215   158   422]\n",
      " [ 4273  2593  1336   919   734]\n",
      " [ 1515   735  5153  4736  2440]\n",
      " [  437   170   745 10722 15716]\n",
      " [  220    58   117  1947 34841]]\n",
      "\n",
      "word to vec\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.536\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 6420  3532     0    93  1226]\n",
      " [ 1824  6536     2   369  1124]\n",
      " [  566  8649     2  2348  3014]\n",
      " [  204  5905     2  5191 16488]\n",
      " [  197  1993     1  2050 32942]]\n",
      "\n",
      "word to vec 2\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.5, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.551\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 7594  3050     2   211   414]\n",
      " [ 2074  6665     7   632   477]\n",
      " [  616  8328    18  3985  1632]\n",
      " [  236  5560    12  9230 12752]\n",
      " [  264  2156     5  4008 30750]]\n",
      "\n",
      "word to vec 3\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.562\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8013  2487     2   165   604]\n",
      " [ 2382  6222    10   551   690]\n",
      " [  701  7917    38  3868  2055]\n",
      " [  241  4569    18  8590 14372]\n",
      " [  210  1533     6  3171 32263]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.54966328294165556, 'tfidf': 0.56912135719819623, 'count stop': 0.55894038419515679, 'tfidf stop': 0.57793162359204597, 'count stop ngram': 0.57514054709072493, 'tfidf stop ngram': 0.59350602912254913, 'count stop ngram lda': 0.57335266890482528, 'tfidf stop ngram nmf': 0.59357555771866743, 'word to vec': 0.53606547607223032, 'word to vec 2': 0.55097439361131528, 'word to vec 3': 0.56243667931424934})\n"
     ]
    }
   ],
   "source": [
    "# run tests for sgd\n",
    "sgd_mini_results = test_features(sgd_model, feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.540\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9210    75   286   608  1092]\n",
      " [ 2003  3549   728  1998  1577]\n",
      " [  692    34  6667  4037  3149]\n",
      " [  177     7   100 17765  9741]\n",
      " [  117     4    57  1205 35800]]\n",
      "\n",
      "tfidf\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.546\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9677    41   216   467   870]\n",
      " [ 1534  4977   501  1526  1317]\n",
      " [  560    30  8463  2898  2628]\n",
      " [  140     6    59 20259  7326]\n",
      " [   84     3    47   634 36415]]\n",
      "\n",
      "count stop\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 300}\n",
      "Tuned Classifier Accuracy: 0.543\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9409    48   280   566   968]\n",
      " [ 1788  3950   741  1997  1379]\n",
      " [  581    21  7440  3716  2821]\n",
      " [  149     5    87 18196  9353]\n",
      " [   88     1    40   982 36072]]\n",
      "\n",
      "tfidf stop\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.552\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9867    33   221   422   728]\n",
      " [ 1314  5526   502  1419  1094]\n",
      " [  463    19  9265  2569  2263]\n",
      " [  103     4    56 20711  6916]\n",
      " [   65     1    30   480 36607]]\n",
      "\n",
      "count stop ngram\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.550\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9428    67   274   493  1009]\n",
      " [ 1875  3817   874  1934  1355]\n",
      " [  583    23  7186  3975  2812]\n",
      " [  129     4   108 17574  9975]\n",
      " [   72     3    39  1151 35918]]\n",
      "\n",
      "tfidf stop ngram\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.556\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9831    49   219   409   763]\n",
      " [ 1429  5212   617  1507  1090]\n",
      " [  469    27  8927  2831  2325]\n",
      " [   98     5    78 20053  7556]\n",
      " [   55     1    28   638 36461]]\n",
      "\n",
      "count stop ngram lda\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.508\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 9785     0    25   267  1194]\n",
      " [  618  6511    76   803  1847]\n",
      " [  174     0 10331   897  3177]\n",
      " [   38     0     3 21092  6657]\n",
      " [   10     0     3    33 37137]]\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.555\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[10496     1    50   207   517]\n",
      " [  515  7705    92   733   810]\n",
      " [  224     0 11533  1034  1788]\n",
      " [   48     3     1 23853  3885]\n",
      " [   15     0     7    53 37108]]\n",
      "\n",
      "word to vec\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 300}\n",
      "Tuned Classifier Accuracy: 0.531\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[10694     0    48   210   319]\n",
      " [  164  8852     2   405   432]\n",
      " [  243     0 13208   226   902]\n",
      " [  137     2     0 26970   681]\n",
      " [   80     1     7     0 37095]]\n",
      "\n",
      "word to vec 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1f9777197976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run tests for random forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrf_mini_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranforest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-85ff7942c728>\u001b[0m in \u001b[0;36mtest_features\u001b[0;34m(model, sets)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_values_mini\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmodel_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-1db011fd4cd5>\u001b[0m in \u001b[0;36mranforest_model\u001b[0;34m(X_trn, y_trn)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# param_grid = {\"n_estimators\": [2, 10, 100, 300, 1000],\"max_depth\": [2, 10, 100, 300], \"min_samples_split\": [2, 10, 100],\"min_samples_leaf\": [1, 10, 100]}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mranforest_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtuned_ranforest_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranforest_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtuned_ranforest_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-7670f5705fe1>\u001b[0m in \u001b[0;36mcross_validation_tuning\u001b[0;34m(classifier, param_grid, X_trn, y_trn)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcross_validation_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclassifier_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclassifier_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Print the optimal parameters and best score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tuned Classifier Parameters: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run tests for random forest\n",
    "rf_mini_results = test_features(ranforest_model, feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec 3\n",
      "Tuned Classifier Parameters: {'C': 100, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.601\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8866   999   520   431   455]\n",
      " [ 2981  2281  2436  1560   597]\n",
      " [  953   895  5082  6177  1472]\n",
      " [  314   212  1566 14694 11004]\n",
      " [  253    66   271  5927 30666]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec 3': 0.60142235642344899})\n"
     ]
    }
   ],
   "source": [
    "# run tests for logistic regression \n",
    "logreg_mini_results = test_features(logreg_model, {'word to vec 3': w2v_mini_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.559\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 7495   969   573   745  1489]\n",
      " [ 2062  2769  1795  1800  1429]\n",
      " [  763   799  4843  5361  2813]\n",
      " [  287   198  1250 13492 12563]\n",
      " [  201    68   285  5617 31012]]\n",
      "\n",
      "tfidf\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.561\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 7688   990   573   784  1236]\n",
      " [ 2085  3068  1680  1784  1238]\n",
      " [  774   814  5196  5258  2537]\n",
      " [  295   185  1205 14367 11738]\n",
      " [  239    68   281  5855 30740]]\n",
      "\n",
      "count stop\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.568\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 7671  1052   573   702  1273]\n",
      " [ 2115  2979  1884  1634  1243]\n",
      " [  789   832  5304  5189  2465]\n",
      " [  303   216  1384 13712 12175]\n",
      " [  212    84   316  5541 31030]]\n",
      "\n",
      "tfidf stop\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.568\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8014   980   572   653  1052]\n",
      " [ 2162  3238  1794  1607  1054]\n",
      " [  804   843  5649  5058  2225]\n",
      " [  337   191  1322 14504 11436]\n",
      " [  261    73   297  5609 30943]]\n",
      "\n",
      "count stop ngram\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.576\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 7765  1047   529   654  1276]\n",
      " [ 2091  3125  1848  1566  1225]\n",
      " [  750   873  5455  5109  2392]\n",
      " [  279   158  1352 13830 12171]\n",
      " [  187    61   267  5506 31162]]\n",
      "\n",
      "tfidf stop ngram\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.577\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8111   964   514   636  1046]\n",
      " [ 2145  3394  1740  1523  1053]\n",
      " [  805   852  5785  4951  2186]\n",
      " [  325   147  1293 14435 11590]\n",
      " [  257    59   271  5626 30970]]\n",
      "\n",
      "count stop ngram lda\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.577\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8169  1000   468   638   996]\n",
      " [ 2223  3402  1768  1496   966]\n",
      " [  884   913  5680  4964  2138]\n",
      " [  362   240  1290 14338 11560]\n",
      " [  287    99   260  5578 30959]]\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.593\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8647  1043   470   511   600]\n",
      " [ 2299  3944  1709  1302   601]\n",
      " [  861  1081  6150  4842  1645]\n",
      " [  358   280  1400 15341 10411]\n",
      " [  259   101   279  5484 31060]]\n",
      "\n",
      "word to vec\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.560\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8451  1060   635   522   603]\n",
      " [ 2344  3430  1885  1527   669]\n",
      " [ 1155  1184  5376  5178  1686]\n",
      " [  525   499  1736 15207  9823]\n",
      " [  475   181   520  6022 29985]]\n",
      "\n",
      "word to vec 2\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.565\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8685  1056   523   493   514]\n",
      " [ 2320  3728  1689  1513   605]\n",
      " [ 1062  1279  5598  5100  1540]\n",
      " [  529   526  1723 15515  9497]\n",
      " [  444   207   499  6015 30018]]\n",
      "\n",
      "word to vec 3\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.576\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 8900  1028   501   414   428]\n",
      " [ 2181  4195  1680  1279   520]\n",
      " [  940  1269  6214  4829  1327]\n",
      " [  420   517  1769 16007  9077]\n",
      " [  360   187   465  5847 30324]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.55888078825562681, 'tfidf': 0.56092691551282303, 'count stop': 0.56833667732771809, 'tfidf stop': 0.56846580186336637, 'count stop ngram': 0.57552792069766978, 'tfidf stop ngram': 0.57714694372156772, 'count stop ngram lda': 0.57729593357039277, 'tfidf stop ngram nmf': 0.59285047378771927, 'word to vec': 0.5602812928345815, 'word to vec 2': 0.56477085361250723, 'word to vec 3': 0.57598482290073305})\n"
     ]
    }
   ],
   "source": [
    "# run tests for XG Boost\n",
    "xgb_mini_results = test_features(xgb_model, feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec 2\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.534\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[10750     0    26   211   284]\n",
      " [  132  8948     0   375   400]\n",
      " [  209     0 13491   116   763]\n",
      " [  118     0     0 27184   488]\n",
      " [   81     0     2     0 37100]]\n",
      "\n",
      "word to vec 3\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.545\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[10907     0    19   136   209]\n",
      " [   79  9239     0   236   301]\n",
      " [  170     0 13733    69   607]\n",
      " [  109     0     0 27298   383]\n",
      " [   63     0     3     0 37117]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec 2': 0.53430739585609566, 'word to vec 3': 0.5453922406086732})\n"
     ]
    }
   ],
   "source": [
    "# run tests for random forest  \n",
    "rf_mini_results = test_features(ranforest_model, {'word to vec 2': w2v_mini_2, 'word to vec 3': w2v_mini_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n"
     ]
    }
   ],
   "source": [
    "# run tests for SVM\n",
    "svm_mini_results = test_features(svm_model, feature_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
