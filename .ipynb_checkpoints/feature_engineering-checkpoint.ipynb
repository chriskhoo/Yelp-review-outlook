{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and determine working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import topic model \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Get stop words \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import NLP vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "# Import models \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get current directory\n",
    "dir = os.path.dirname(os.path.abspath('__file__'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2876509 entries, 0 to 2876508\n",
      "Data columns (total 2 columns):\n",
      "stars_review        int64\n",
      "processed_review    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load df from a csv - all text to lower case, tokenize into list of strings, remove punctuation and lemmatize\n",
    "preprocessed_path = os.path.join(dir, '02_processed_data','review_text_stars.csv')\n",
    "preprocessed_df = pd.read_csv(preprocessed_path, index_col = False)\n",
    "preprocessed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training and test sets using a fixed seed for reproducibility \n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_df.processed_review, preprocessed_df.stars_review, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mini dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20136\n"
     ]
    }
   ],
   "source": [
    "# Create a mini data set for feature and model selection (for manageable training times)\n",
    "__, X_mini, ___, y_mini = train_test_split(X_train, y_train, test_size = 0.01, random_state = 42)\n",
    "print(len(X_mini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection using mini dataset\n",
    "Using the mini dataset, various types of feature engineering will be performed and tested on a variety of models in the next stage. \n",
    "\n",
    "## Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3828\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams and remove all standard stopwords \n",
    "# Source: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "count_vectorizer_mini = CountVectorizer(analyzer = 'word',\n",
    "                             stop_words = 'english',\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "count_mini = count_vectorizer_mini.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the count_vectorizer\n",
    "print( len(count_vectorizer_mini.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf vectorizer (weighted vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3828\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams and remove all standard stopwords \n",
    "tfidf_vectorizer_mini = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = 'english',\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "tfidf_mini = tfidf_vectorizer_mini.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer_mini.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Add neutral words related to restaurants to list of stop words\n",
    "stopWords.update(['restaurant', 'place', 'bar', 'service', 'food', 'lunch', 'breakfast', 'dinner', 'price', 'order', 'ordered'])\n",
    "\n",
    "# Remove stopwords that might reflect sentiment\n",
    "stopWords = [word for word in stopWords if word not in ['above', 'not', 'below', 't', 'off', 'no', 'again', 'against', 'under', 'hadn', 'up', 'shan', 'more', 'hasn', 'won','couldn', 'wasn', 'mustn', 'out', 'don','down', 'haven', 'price', 'mightn', 'isn', 'wouldn', 'needn', 'shouldn', 'weren', 'aren', 'didn', 'ain', 'doesn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature set with new stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams and customized stopwords \n",
    "count_vectorizer_mini__stop = CountVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "count_mini__stop = count_vectorizer_mini__stop.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the count_vectorizer\n",
    "print( len(count_vectorizer_mini__stop.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams and customized stopwords \n",
    "tfidf_vectorizer_mini__stop = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "tfidf_mini__stop = tfidf_vectorizer_mini__stop.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer_mini__stop.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text using unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8437\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams,bigrams and trigrams and customized stopwords \n",
    "count_vectorizer_mini__stop_ngram = CountVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             ngram_range = (1,3),\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "count_mini__stop_ngram = count_vectorizer_mini__stop_ngram.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the count_vectorizer\n",
    "print( len(count_vectorizer_mini__stop_ngram.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8437\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer using unigrams,bigrams and trigrams and customized stopwords \n",
    "tfidf_vectorizer_mini__stop_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             ngram_range = (1,3),\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)\n",
    "\n",
    "# Transform the training data (independent variables)\n",
    "tfidf_mini__stop_ngram = tfidf_vectorizer_mini__stop_ngram.fit_transform(X_mini)\n",
    "\n",
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer_mini__stop_ngram.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling \n",
    "### Using Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Initialize LDA model\n",
    "lda = LatentDirichletAllocation(n_components=300,random_state=42) \n",
    "\n",
    "# Get topics for training data\n",
    "lda_mini = lda.fit_transform(count_mini__stop_ngram)\n",
    "\n",
    "# add topics to count vectorizer ngrams set \n",
    "count_mini__stop_ngram_lda = hstack((count_mini__stop_ngram, lda_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "roll beef pho spring egg spring roll soup vietnamese good broth\n",
      "Topic 1:\n",
      "buffet dessert group worth course selection large variety went main\n",
      "Topic 2:\n",
      "wa server table great wa great server wa party attentive took brought\n",
      "Topic 3:\n",
      "made up statement sam pulled pork pit one table didnt much year since liking whats not\n",
      "Topic 4:\n",
      "going price reasonable card going back back die city credit forever\n",
      "Topic 5:\n",
      "sitting around smothered dish wa almost inside amazing definitely consistent love pizza stuff risk\n",
      "Topic 6:\n",
      "ha good well definitely little bland tends go wa not offer ha not though wa take out pretty empty\n",
      "Topic 7:\n",
      "bitter taste bud deserves up get back again content chicken enchilada horseradish think better wa literally\n",
      "Topic 8:\n",
      "season price good time took fryer flair part town town good part time\n",
      "Topic 9:\n",
      "water soda generous product fountain canned bottled portion generous array pepsi\n",
      "Topic 10:\n",
      "friday friday saturday fresher no joke really meal quality saturday cool much\n",
      "Topic 11:\n",
      "cashier wa like outrageous nice attentive settle breading bit expensive wa sad delivered get more\n",
      "Topic 12:\n",
      "picked no not really nice prepared white pizza burger ive thai iced know not chip wa falafel\n",
      "Topic 13:\n",
      "trip wow enjoyed even better valley alone chili neighborhood even better\n",
      "Topic 14:\n",
      "need go omelet wa coming back portion size oh coupon tooth price point white poor\n",
      "Topic 15:\n",
      "acknowledged called back excellent well prepared native price fair versus decor nice garlic fry mash\n",
      "Topic 16:\n",
      "worth money stranger potato fry record enjoyed not go back wa good well parmigiana hr ny\n",
      "Topic 17:\n",
      "sauce wa meat flavor chicken side fresh cooked well tender\n",
      "Topic 18:\n",
      "excellent wa world last still venue scottsdale spot gumbo best\n",
      "Topic 19:\n",
      "helpful definitely perfect patio back clean please wrap definitely back light\n",
      "Topic 20:\n",
      "tasted special like tasted like truffle poutine 11 anything later call\n",
      "Topic 21:\n",
      "wa wrong piece chicken well wa hour wait make up tater tot said wa good wa cooked perfection gallo cart\n",
      "Topic 22:\n",
      "every day made no even though wa go one even glad wa luckily natural ravioli used love\n",
      "Topic 23:\n",
      "legit not eating sound like lot great really pizza hut roll wa keep mind good meal seem like\n",
      "Topic 24:\n",
      "tea green milk iced mango sweet boba green tea make feel iced tea\n",
      "Topic 25:\n",
      "try something packed wanted dog hot one bite else fan\n",
      "Topic 26:\n",
      "dish shrimp tasty plate small vegetarian entree indian dish wa saying\n",
      "Topic 27:\n",
      "great ive plat wa always cheap price charcuterie la carte booked scratch effort salad soup\n",
      "Topic 28:\n",
      "plenty bibimbap wa plenty flavor wa best part wa not egg like ever\n",
      "Topic 29:\n",
      "up par spring roll wa late thrown told not speedy pork connoisseur long time great staff\n",
      "Topic 30:\n",
      "picnic friendly atmosphere atmosphere show wa enjoyed visit attentive friendly friendly attentive\n",
      "Topic 31:\n",
      "spring roll assorted unfortunate maki off night cheap price buddy owner wa across popcorn\n",
      "Topic 32:\n",
      "down side slaw opposite image onion wa behind counter not come never heard price not really friendly\n",
      "Topic 33:\n",
      "awesome mexican salsa good chip great margarita wa awesome tortilla drink\n",
      "Topic 34:\n",
      "since first front sweet wa bell pepper wa ok burger not processed not taste cloth little italy\n",
      "Topic 35:\n",
      "sweet potato late preparing wont going back two week somewhere else every time im piece wa two slice charming\n",
      "Topic 36:\n",
      "wa enjoyed show little more wa good ok birthday wa attentive knew wa good wa\n",
      "Topic 37:\n",
      "down sat sat down giant down wa pay extra table sat floor table wa\n",
      "Topic 38:\n",
      "much right better away able much better diner perfection casino long\n",
      "Topic 39:\n",
      "go not container desert out good delicious tried came out beet salad\n",
      "Topic 40:\n",
      "pork bbq wait worth pulled nacho belly well chop pulled pork\n",
      "Topic 41:\n",
      "experience overall overall experience unprofessional good overall overall experience wa not eating best fish usually pretty good\n",
      "Topic 42:\n",
      "dr always go no flavor response wa start saying macarons wont going back garlic main dish building\n",
      "Topic 43:\n",
      "love come yesterday premade fall off certainly not ive eaten rub came two grocery store sandwich wa\n",
      "Topic 44:\n",
      "cutting timely expecting more patient pant meet vega trip face wished chicken curry\n",
      "Topic 45:\n",
      "waiting table french onion afternoon wa sunny took home advised give star nice see flag ended up getting\n",
      "Topic 46:\n",
      "super town super friendly bagel friendly moving panera tasty sure would best town\n",
      "Topic 47:\n",
      "hollandaise caramel eat anything spin beef 1520 minute youre going conveniently work wa one best\n",
      "Topic 48:\n",
      "nice wa nice nice wa home made wa nice wa good quick wa good wa good like\n",
      "Topic 49:\n",
      "hour cheap spicy chicken 24 spot many must eats satisfying downtown\n",
      "Topic 50:\n",
      "taco tuesday wa charged five guy waiter ear attempted street parking keg bite wa box\n",
      "Topic 51:\n",
      "drink menu whenever im every time im train ingredient fresh la carte took one time year chicken shrimp started out\n",
      "Topic 52:\n",
      "would go again corn bread 2030 flag addicted hand roll wa well done best part meal seating wa get two\n",
      "Topic 53:\n",
      "wa nice wa blended saw thin welcoming appetizing around wa great always meal poker\n",
      "Topic 54:\n",
      "component white rice happens taking care server friendly blah forgotten featured back cant wait\n",
      "Topic 55:\n",
      "open cheap hour 24 cheddar absolute 24 hour peak iceberg must\n",
      "Topic 56:\n",
      "beer wing game selection tv great duck watch world tap\n",
      "Topic 57:\n",
      "better price decent price imo drive worst ever good not youve got watermelon credit card also serve\n",
      "Topic 58:\n",
      "thai never ive ive never pad curry pad thai coconut environment couple\n",
      "Topic 59:\n",
      "dont like up get not know out want really thing\n",
      "Topic 60:\n",
      "oh yeah real deal table inside mimosa wa cold again next time writing great wa congee lamb chop\n",
      "Topic 61:\n",
      "pretty good pretty good though ton get standard good though still get little\n",
      "Topic 62:\n",
      "go back go back again leave wait back again want wait staff cash\n",
      "Topic 63:\n",
      "wouldnt go back bit high overbearing empty appetizer wa yelp certain couldnt believe mongolian beef freakin\n",
      "Topic 64:\n",
      "taste house difference house made really taste made really wa ingredient great\n",
      "Topic 65:\n",
      "inexpensive thicker somewhat raw min wait wa overcooked sriracha ending indoor out world\n",
      "Topic 66:\n",
      "macaroni cheese bonne wa delivered seated right away charcuterie phoenix spectacular little hard pie wa no way\n",
      "Topic 67:\n",
      "highlight out town price point wa able im talking lot time patty wa wa shocked mon deliciously\n",
      "Topic 68:\n",
      "jar dont want strange near future better time definitely dont really yonge im giving star willing\n",
      "Topic 69:\n",
      "wa said macaroni rib surprise pork animal rice horrible best\n",
      "Topic 70:\n",
      "wa loud definite hand wa delivered break took twenty eel anywhere else alfredo\n",
      "Topic 71:\n",
      "regularly attentive friendly cabbage bit slow relative ive never seen selection roll good excellent great 110\n",
      "Topic 72:\n",
      "burger wa one try didnt good side friend loved bf\n",
      "Topic 73:\n",
      "rest lot different rest menu menu staff wa super cant burger back come ice cream\n",
      "Topic 74:\n",
      "bacon french toast ive recommended egg best ive greek outside cup\n",
      "Topic 75:\n",
      "home hesitant lacked flavor wa crispy bill time come tremendous also really good felt like wa kind enough\n",
      "Topic 76:\n",
      "great fun atmosphere great great music date location ha great great location friend\n",
      "Topic 77:\n",
      "ha ha best im usually best family sever meat drink back always\n",
      "Topic 78:\n",
      "vega la quality la vega high overpriced price normal high quality visiting\n",
      "Topic 79:\n",
      "decided come buttermilk diner ended up eating stone wait table 2013 great especially wont love love love\n",
      "Topic 80:\n",
      "shocked yum yum melting vietnamese tasty well wa beyond toddler smiling well seasoned send\n",
      "Topic 81:\n",
      "pizza italian delivery size thank oven portion size pepperoni mozzarella creme\n",
      "Topic 82:\n",
      "option not like good 14 lot would although consider time\n",
      "Topic 83:\n",
      "week care customer today pick horrible manager pick up bomb up\n",
      "Topic 84:\n",
      "again try dish would nice little gripe wa sooo back again overlook could barely edamame pricier\n",
      "Topic 85:\n",
      "crab rib seafood bone prime leg filet prime rib king crab leg\n",
      "Topic 86:\n",
      "night late wish carne late night asada carne asada meant common heck\n",
      "Topic 87:\n",
      "platter wa pas finishing get crowded tasty well cater up one held varied run out\n",
      "Topic 88:\n",
      "wa soggy arm must visit already good coffee power put up overlooking finish neither\n",
      "Topic 89:\n",
      "get full staff seemed ambiance wa half off hotel room comfy checked tasty mini surf\n",
      "Topic 90:\n",
      "remarkable bite 10 minute got around large selection dragon really hit bit more sky brought out\n",
      "Topic 91:\n",
      "wa time small crepe bit much yes one 3pm dont\n",
      "Topic 92:\n",
      "inhouse joint crispy chicken bueno accommodate growing craft beer get rid artichoke dip select\n",
      "Topic 93:\n",
      "yummy got else far looking everything also calamari lol everything else\n",
      "Topic 94:\n",
      "must go also great ppl mimosa id come back busier hoped anyhow crafted overall great\n",
      "Topic 95:\n",
      "noodle chinese hot dumpling takeout eat pot ball go out soy\n",
      "Topic 96:\n",
      "bread wa fried steak horribly vast one waitress more could meh wa not love love everything tried\n",
      "Topic 97:\n",
      "versus nigiri example red wine husband wa fake music wa thing really airport traditional\n",
      "Topic 98:\n",
      "pork belly tomato basil especially since actually wa bottom line wa cute vega trip mozzarella stick tomato wa enough\n",
      "Topic 99:\n",
      "took care seafood well constantly took coming ha wa amazing bringing dish wa\n",
      "Topic 100:\n",
      "building day ago steak sandwich side wa people wa high quality dont worry first glance bad wa way better\n",
      "Topic 101:\n",
      "head treated great find good day nice enough smile wa crowded not least amount ha ever\n",
      "Topic 102:\n",
      "answer yum soup frankly mushroom balance designed drinking score filling menu pretty\n",
      "Topic 103:\n",
      "curry juice stand gyro smell staff friendly gone market west jerk\n",
      "Topic 104:\n",
      "egg combination usually shake benedict montreal potato smoked youve salmon\n",
      "Topic 105:\n",
      "im also yesterday wa light entrance go elsewhere need get go back try forced dozen theyre\n",
      "Topic 106:\n",
      "mouth checked multiple leaving great experience ran third ha always melted ran out\n",
      "Topic 107:\n",
      "say one waitress wa nice ha not every night chicken salad counter wa dime show taste pretty tasty well\n",
      "Topic 108:\n",
      "waitress quite done mediocre waitress wa wa quite lovely well overcooked ask\n",
      "Topic 109:\n",
      "wa liked not wife good wa not good not good minute water selection\n",
      "Topic 110:\n",
      "whole time sea needing wa fast recent visit crust pizza two different shoot lentil soup ala\n",
      "Topic 111:\n",
      "temperature gift huevos rancheros fois chili cheese paper im not sure kick id come back combo\n",
      "Topic 112:\n",
      "overall filet would suggest upon entering hoping mac lose morning doe not could hear\n",
      "Topic 113:\n",
      "tooth good however reason gave brown rice thicker instance mildly love manager came one else\n",
      "Topic 114:\n",
      "get star cashier wa no better star instead instead wa no wa water would no\n",
      "Topic 115:\n",
      "wa fabulous charcuterie didnt like worst bad thing took off hassle beef sandwich took long time forward next\n",
      "Topic 116:\n",
      "bowl burrito ingredient fresh counter not worth not worth chipotle behind\n",
      "Topic 117:\n",
      "soon chef crust topping pie meatball thin style really nice sauce\n",
      "Topic 118:\n",
      "score find strawberry friendly staff id give not really connected wasnt sure everywhere guy\n",
      "Topic 119:\n",
      "fish chip miss fish chip thanks office martini rush made sure comparison\n",
      "Topic 120:\n",
      "delicious wa delicious everything hit everything wa short affordable slider goat short rib\n",
      "Topic 121:\n",
      "wa good wa good bread mushroom potato enough fresh good wa everything\n",
      "Topic 122:\n",
      "im talking sorry open table sorbet grill ceviche close work sweet server never flavor like\n",
      "Topic 123:\n",
      "never really hour later definitely great decoration thing wa throw owns brownie lighting wa first\n",
      "Topic 124:\n",
      "wa also take recommend wa also wa prompt prompt take out out highly\n",
      "Topic 125:\n",
      "amazing wa amazing pizza wa view fan everything hubby complain garlic not fan\n",
      "Topic 126:\n",
      "come would back come back more definitely out would recommend come out would definitely\n",
      "Topic 127:\n",
      "look combo cute healthy door donut orange forward funny next door\n",
      "Topic 128:\n",
      "tasting name out dont loved fresh goat cheese really cheese side\n",
      "Topic 129:\n",
      "fast saturday room floor sit longer night saturday night afternoon sit down\n",
      "Topic 130:\n",
      "go eat minute waiting could probably pizza wa chile relleno tacky initial complaining bite wa frill\n",
      "Topic 131:\n",
      "birthday boyfriend brisket disappointing casual wa disappointing laid celebrate laid back dress\n",
      "Topic 132:\n",
      "cream favorite ice cant ice cream one come try wait definitely\n",
      "Topic 133:\n",
      "crowded bargain also gave sour cream tony would try salted took 15 filled believe\n",
      "Topic 134:\n",
      "excellent lobster sweet wa excellent potato sweet potato grill potato fry shrimp sweet potato fry\n",
      "Topic 135:\n",
      "since wa never problem savoury wa fun heard good wa didnt table not taken care came out quickly call\n",
      "Topic 136:\n",
      "yellowtail overall good brunch rainbow great waitress went tiny yes wa came around part town\n",
      "Topic 137:\n",
      "positive review done wa chipotle scoop make great burger well sounded spot menu said confirmed\n",
      "Topic 138:\n",
      "water glass tried chicken destination plethora ie contrast boring cry cater server asked\n",
      "Topic 139:\n",
      "rank super fresh wasnt spicy cashier somewhere 30 perfect wa really loved rio waiter said\n",
      "Topic 140:\n",
      "issue menu option smokey bad meal wa huge yesterday wa shell make better would better sun\n",
      "Topic 141:\n",
      "many cool turkey management avoid many time school bruschetta old school brings\n",
      "Topic 142:\n",
      "ever come wa told switch taco great definitely would well seasoned time eating wa hard game pricier\n",
      "Topic 143:\n",
      "flavor wa onion burger course menu took miso egg ramen\n",
      "Topic 144:\n",
      "take no one get could even asked not more time\n",
      "Topic 145:\n",
      "water wa garlic person get burger extremely no not instead\n",
      "Topic 146:\n",
      "cup let know lunchtime acknowledged said oh shame unless youre steak fry request room wa\n",
      "Topic 147:\n",
      "nose asada huevos im going lot choice design not tasty wa perfect came friend ricotta\n",
      "Topic 148:\n",
      "one also out love im get little mac enjoy mac cheese\n",
      "Topic 149:\n",
      "charlotte would not lived out bakery thats awesome different way\n",
      "Topic 150:\n",
      "popular wednesday great good good definitely butternut butternut squash definitely advertising wowed attentive not\n",
      "Topic 151:\n",
      "charlotte lived im always out time definitely better white rice consists minute server wa professional always\n",
      "Topic 152:\n",
      "love ive year ever ive ever hand weve eaten truly hand down\n",
      "Topic 153:\n",
      "900 cauliflower depends ambience bit expensive listed everyone know steak cooked wa first rave\n",
      "Topic 154:\n",
      "chocolate extremely waffle frozen flavor wa extremely dry red hot beef\n",
      "Topic 155:\n",
      "went wa sandwich ive explain aussi good server value money nice selection enjoyable ipad great wine\n",
      "Topic 156:\n",
      "pleasure luke warm super fresh off papaya lightly good first toward friday saturday never even\n",
      "Topic 157:\n",
      "long way get again curry chicken no one came delicious would last week poached egg great definitely rib recommend going\n",
      "Topic 158:\n",
      "not delicious go out desert wa salad sure good container\n",
      "Topic 159:\n",
      "always get love favorite decent cafe people local sometimes especially\n",
      "Topic 160:\n",
      "know youre back up coke wa gross get meal cant beat well good another one noisy above average\n",
      "Topic 161:\n",
      "sample no wonder pay waiter said ridiculously menu well first time last tikka desired frank\n",
      "Topic 162:\n",
      "up one first experience try next sour chicken pretty quickly month ago fast friendly entree dessert im usually mongolian\n",
      "Topic 163:\n",
      "make happy problem scratch side fry mentioning yam receipt frustrated cover grandmother\n",
      "Topic 164:\n",
      "morning omelet nice staff great nice omelet wa print nice staff great fruit\n",
      "Topic 165:\n",
      "wa fry big burger cheese good able sit group choice\n",
      "Topic 166:\n",
      "wa tip not leave wait staff cheese back cash tell\n",
      "Topic 167:\n",
      "wine absolutely glass bottle prepared yum incredible dining fabulous mouth\n",
      "Topic 168:\n",
      "strip street hotel unique across scallop located parking rock mall\n",
      "Topic 169:\n",
      "not wa bad ok no star nothing one good out\n",
      "Topic 170:\n",
      "saving grace back location real deal fry drink little one concerned though dont dog delivery wa ive twice\n",
      "Topic 171:\n",
      "every time doe every time poor always gluten doe not gluten free visit\n",
      "Topic 172:\n",
      "caesar sink preparation patron twenty somebody handed noticed one worst time love\n",
      "Topic 173:\n",
      "wa would bean huge eat something salsa wa medium bite eat good would\n",
      "Topic 174:\n",
      "ease doesnt meal type becoming around time charcuterie da make good culinary\n",
      "Topic 175:\n",
      "arugula importantly loss long waiter wa rustic garlic bread glass spectacular welcome\n",
      "Topic 176:\n",
      "im not sure im not not sure id still write review disappoint\n",
      "Topic 177:\n",
      "thanks clientele kim wa alright bbq block whipped cream never disappointed almost always day\n",
      "Topic 178:\n",
      "part worst best part part wa attention desired could give theyd wa worst control\n",
      "Topic 179:\n",
      "not find ride comfortable around wa 700 flavourful high expectation hefty really nice basil\n",
      "Topic 180:\n",
      "great delicious plethora get something nom seating inside gym stranger tough time im vega could say\n",
      "Topic 181:\n",
      "dim sum locally recommend getting go go opted grapefruit salsa blow away not always right across\n",
      "Topic 182:\n",
      "wa friendly made top went wa friendly everyone staff wa say out\n",
      "Topic 183:\n",
      "burger drink best soup section attentive happy hour almost also great happy hour\n",
      "Topic 184:\n",
      "authenticity cheese wa affordable try girl mac cheese smaller fondue mac\n",
      "Topic 185:\n",
      "portion huge returning prompt china portion huge dragon rule also delicious out not\n",
      "Topic 186:\n",
      "comfortable kudos ayce friendly accommodating great job treated told server wasnt bad bank weight\n",
      "Topic 187:\n",
      "street didnt seem wait time would like risk way get wa gross took wa border want make\n",
      "Topic 188:\n",
      "name atmosphere remember tasting great atmosphere air waitstaff tasting menu name wa cant remember\n",
      "Topic 189:\n",
      "theyre open insane nice wa wa bad spin warmly de la ohio since last philly cheese\n",
      "Topic 190:\n",
      "coffee shop crepe strawberry sugar clam covered banana gelato using\n",
      "Topic 191:\n",
      "rice wife family owner bean interesting deal outstanding fried rice tuesday\n",
      "Topic 192:\n",
      "tone security mole wa accommodating cant wait go lo con bill wa hole wall taste good\n",
      "Topic 193:\n",
      "menu waiter drink item pasta several not looking available also\n",
      "Topic 194:\n",
      "go out way gumbo right front could share harder really really exceptionally advertise thai iced\n",
      "Topic 195:\n",
      "fresh hot sin great tasting hopefully time staff crushed joy laid disappointment wa well nice\n",
      "Topic 196:\n",
      "coming attentive back coming back wa attentive favourite definitely coming dip artichoke definitely coming back\n",
      "Topic 197:\n",
      "exception lazy sock go always option standing around get wa mash vega\n",
      "Topic 198:\n",
      "cheese star grilled tomato give cheese wa phenomenal give star thursday grilled cheese\n",
      "Topic 199:\n",
      "sexy run down understood wa little slow good large large roll around table good hard\n",
      "Topic 200:\n",
      "fusion ever seen well done come out leftover sharing pride true server wa nice annoyed\n",
      "Topic 201:\n",
      "soup spicy ramen tried also soup wa tofu son wa sour\n",
      "Topic 202:\n",
      "sunday pancake pleasant fried solid definitely deep classic pickle chicago\n",
      "Topic 203:\n",
      "best ever one best wa best best ever meal long great meal heading long time\n",
      "Topic 204:\n",
      "scottsdale range grit venue wifi winter shrimp grit back not meatloaf dont waste\n",
      "Topic 205:\n",
      "wouldnt mind conference wa little bit blossom wa perfectly hung attentive staff home wa custom window\n",
      "Topic 206:\n",
      "definitely not worth dennys absolutely sauce also border wa patient ahead wa already tartar sauce crust\n",
      "Topic 207:\n",
      "egg drop energetic hubby comforting seeing up get wont going refried bean took forever braised\n",
      "Topic 208:\n",
      "26 wa macaroni rib pork rice surprise said animal spoiled\n",
      "Topic 209:\n",
      "hazelnut out wa latte beverage light believe wa milk missing out piping\n",
      "Topic 210:\n",
      "liked kid express pastrami panda waste money panda express pet wasted liked wa\n",
      "Topic 211:\n",
      "complaint attitude woman quiet comment fully wa done not eat complaint wa complained\n",
      "Topic 212:\n",
      "time first next ive first time next time tried visit second try\n",
      "Topic 213:\n",
      "sweet taste eye contact good not great no more embarrassed morning wa still good out kitchen usually ranging\n",
      "Topic 214:\n",
      "wa meal every yum dining yum yum absolutely enjoyed small experience\n",
      "Topic 215:\n",
      "unfriendly gyoza wa super friendly calling shellfish not typical price definitely papaya salad always friendly presented\n",
      "Topic 216:\n",
      "free cake drive greasy always made fill fresh stick always good\n",
      "Topic 217:\n",
      "got around diner table seated phone tasted better lack fried pickle absolutely delicious hour fresh delicious\n",
      "Topic 218:\n",
      "not like option simple 14 bread view sandwich lot doesnt\n",
      "Topic 219:\n",
      "eating again yellow curry thing didnt 23 like everything since opened situation id go wa real wa bit slow\n",
      "Topic 220:\n",
      "greatly warmed ive ever cup wa bad taste green curry wa favorite mandalay bay affordable price not overwhelming\n",
      "Topic 221:\n",
      "thats sure hour half waffle fry return try terrible try wa nigiri see people sorry uncle\n",
      "Topic 222:\n",
      "new spot regular consistency schedule surf turf baseball new spot regular customer\n",
      "Topic 223:\n",
      "outside wa cheese burger rule 800 quiche main course cone really cool noisy wa wa\n",
      "Topic 224:\n",
      "dare chip salsa better one expect much diced mac cheese sushi amazing chicken tomato onion back room\n",
      "Topic 225:\n",
      "staff friendly quick gem friendly staff hidden hidden gem always friendly cozy favor\n",
      "Topic 226:\n",
      "sandwich fantastic warm sandwich wa wa fantastic hard chicken salad wa hard nice atmosphere lemonade\n",
      "Topic 227:\n",
      "taste wa diet didnt taste wa something went one melting day went wa one burger\n",
      "Topic 228:\n",
      "taco cant wrong go go wrong suck fish taco cant go cant go wrong location\n",
      "Topic 229:\n",
      "valentine day table server none not busy best fish element salad probably favorite splitting well definitely\n",
      "Topic 230:\n",
      "option really would not person could expect lot like more\n",
      "Topic 231:\n",
      "deserved owner walk away good variety staff ha looking great hassle didnt wait long say best purchase\n",
      "Topic 232:\n",
      "forward coming back opposed cinnamon menu selection everything ive time came judging delivery deep dish saying\n",
      "Topic 233:\n",
      "great lot saw good day really enjoyed hanging smile face thin crust teriyaki chicken item not out get\n",
      "Topic 234:\n",
      "minute wait crab divine little off water wa consistent try different no idea calling cest\n",
      "Topic 235:\n",
      "off bill organized escape tall discount end meal toward insane written drive thru\n",
      "Topic 236:\n",
      "great get usually pretty thai chicken today great get quiet great love chicken tasted\n",
      "Topic 237:\n",
      "wa enjoyed show went birthday back next still good more sure\n",
      "Topic 238:\n",
      "check out check out disappointed wont sub havent movie suggestion area\n",
      "Topic 239:\n",
      "good sushi wa happy hour time drink happy hour bill sushi spot treat\n",
      "Topic 240:\n",
      "wonderful everyone bad low gourmet wa wonderful bad experience never staple best burger\n",
      "Topic 241:\n",
      "roll strip hard rock sauce tuna cooked across hard rock street\n",
      "Topic 242:\n",
      "sat table sour soup took time paneer baba said not though not inch above average come get\n",
      "Topic 243:\n",
      "try different girlfriend grabbed dinning never come back star off going go plate wa wa wonderful slight\n",
      "Topic 244:\n",
      "dining priced well desert dining experience omg reasonably exceptional reasonably priced good well\n",
      "Topic 245:\n",
      "seeing downside crab kobe done wa annoyed asada taco portion good 11am cant say\n",
      "Topic 246:\n",
      "dragon roll brulee time trying really made hr gyoza wa surprised peanut sauce would get wa friendly helpful\n",
      "Topic 247:\n",
      "ill go back much more charm tomato sauce wa taking little much youve got not sure though dont back location\n",
      "Topic 248:\n",
      "wa not came would got out meal also didnt little\n",
      "Topic 249:\n",
      "homemade gravy daughter biscuit roast adult potato roast beef home biscuit gravy\n",
      "Topic 250:\n",
      "man cupcake everytime chilli occasionally man wa wanted get get im much\n",
      "Topic 251:\n",
      "channel excellent wa brussel extraordinary green tea higher again not ill go friendly well massive\n",
      "Topic 252:\n",
      "skin spirit text better one dad table not haha come again info ingredient fresh\n",
      "Topic 253:\n",
      "great take wasnt great must visit department enough make paid bc solo serving wa saturday morning\n",
      "Topic 254:\n",
      "missed care customer yellowtail dimly caramelized onion person wa wait another randomly unable working wa\n",
      "Topic 255:\n",
      "last time time wa last time visited hoping year coke wrong wa wrong\n",
      "Topic 256:\n",
      "something else hour shouldve dont like ribeye great dont great especially different flavor out fast wa another\n",
      "Topic 257:\n",
      "mexican ive embarrassed mongolian beef 1520 much sauce john finally came combined bean dip frankly\n",
      "Topic 258:\n",
      "de est la le je et vous sur drink une\n",
      "Topic 259:\n",
      "maybe thats small side friendly helpful doughy youve dont even seating wa personal favorite hour overwhelming\n",
      "Topic 260:\n",
      "moved wa super nice started off black hate paneer shredded dragon somewhere cilantro\n",
      "Topic 261:\n",
      "long enough speed didnt come wa supposed different type ceiling smell not again not always friendly attentive\n",
      "Topic 262:\n",
      "chicken fried brown fried chicken anywhere hash bloody cooky mary bloody mary\n",
      "Topic 263:\n",
      "sushi happy roll hour happy hour salmon special sashimi price drink\n",
      "Topic 264:\n",
      "brunch seat mimosa super nice patio bottomless wynn olive garden 11am still great\n",
      "Topic 265:\n",
      "no tikka shrimp professional took time great little ha wa dessert basically accompanying\n",
      "Topic 266:\n",
      "mountain option not offer 14 view like doesnt time lot\n",
      "Topic 267:\n",
      "wa made meat like burger good hamburger husband up favorite\n",
      "Topic 268:\n",
      "really good really good wa really fresh quality japanese also wa really good ayce\n",
      "Topic 269:\n",
      "let get better price ive gotten steakhouse walked away direction elsewhere fork couple year also liked\n",
      "Topic 270:\n",
      "get good roasted not good making reservation asks age wa right dont expect decided try also really\n",
      "Topic 271:\n",
      "cant wait wait 20 good overall since cash irish scared wa great experience tom wa seated\n",
      "Topic 272:\n",
      "smoked best pizza ordinary youd prawn wa under like see different option hazelnut bien\n",
      "Topic 273:\n",
      "wa time wa took server time table brunch attentive server wa right\n",
      "Topic 274:\n",
      "old fashioned whatever player 50 concert crepe wa aok school great experience bistro\n",
      "Topic 275:\n",
      "scallion push yeah neighborhood pic grace combination tad burger well server\n",
      "Topic 276:\n",
      "recommend highly make highly recommend sure make sure delicious naan also eat\n",
      "Topic 277:\n",
      "burger fry good bun burger wa patty joint good price price hamburger\n",
      "Topic 278:\n",
      "mole mom way start shy done right herb duck confit granted like one surprised wa\n",
      "Topic 279:\n",
      "go not go trash whip not go back looked pretty vanilla ice year since vanilla ice cream also asked\n",
      "Topic 280:\n",
      "le de et la un pour pa est que par\n",
      "Topic 281:\n",
      "decided come dead served wa heaven seared pork sandwich changing hotdog corned scampi\n",
      "Topic 282:\n",
      "cup coffee bomb go get no matter time time more option ill try pasta wa floor wa panang curry\n",
      "Topic 283:\n",
      "mai doesnt really dim sum sum dim wasnt wa friend friendly go\n",
      "Topic 284:\n",
      "checking out hot juicy nice friendly real world server wa wa thick really not take away bit salty\n",
      "Topic 285:\n",
      "dipping oreo out next german dont usually lack looking something nice ambiance whose fridge\n",
      "Topic 286:\n",
      "onion ago ring year year ago onion ring separate dried sounded memory\n",
      "Topic 287:\n",
      "girl wa overwhelming find good curry chicken good dessert bad wa would expect carried husband wa pot pie\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 288:\n",
      "wa minute wait came said table got took time seated\n",
      "Topic 289:\n",
      "steak must medium cooked oyster rare york new york must try new\n",
      "Topic 290:\n",
      "feel like sausage feel like authentic pudding 50 sticky fine draft\n",
      "Topic 291:\n",
      "beer 10pm mozzarella stick wa good well wa least nutella way sweet back room get star like go\n",
      "Topic 292:\n",
      "pretty late party probably fan night stay pretty good able tempe\n",
      "Topic 293:\n",
      "bartender no idea issue wa appears seasoned hall cramped wa recommended again again hop\n",
      "Topic 294:\n",
      "salad blue cheese personal often chopped blue cheese grand garden keep up\n",
      "Topic 295:\n",
      "wa bit like more though even one lot okay better\n",
      "Topic 296:\n",
      "ny chinese style ny style tried ok nothing ive az enjoyed\n",
      "Topic 297:\n",
      "price location area better downtown not quality half small little\n",
      "Topic 298:\n",
      "choice return broccoli chow mixed darn inedible part meal would return curry chicken\n",
      "Topic 299:\n",
      "definitely come back decor wa wa mushy mac cheese lip sorry carne asada si limp one star\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "display_topics(lda, tfidf_vectorizer_mini__stop_ngram.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize NMF model\n",
    "nmf = NMF(n_components=300, random_state=42)\n",
    "\n",
    "# Get topics for training data\n",
    "nmf_mini = nmf.fit_transform(tfidf_mini__stop_ngram)\n",
    "\n",
    "# add topics to tfidf ngrams set \n",
    "tfidf_train_mini__stop_ngram_nmf = hstack((tfidf_mini__stop_ngram, nmf_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "wa wa great wa not wa really wa nice wa amazing wa delicious wa wa great wa thought\n",
      "Topic 1:\n",
      "favorite one favorite new favorite favorite spot favorite go wa favorite favorite dish favorite eat favorite pizza favorite sushi\n",
      "Topic 2:\n",
      "great wa great great wa wa great wa always great not great great good great experience good great great drink\n",
      "Topic 3:\n",
      "pizza pizza wa slice best pizza good pizza topping great pizza pepperoni garlic crust\n",
      "Topic 4:\n",
      "burger burger wa best burger bun shake good burger burger joint burger fry joint patty\n",
      "Topic 5:\n",
      "chicken chicken wa fried chicken chicken sandwich chicken wing chicken salad jerk tender dry jerk chicken\n",
      "Topic 6:\n",
      "good good good not good always good good price good not good great great good pretty good good time\n",
      "Topic 7:\n",
      "sushi best sushi sushi wa ayce sashimi sushi chef good sushi japanese tempura great sushi\n",
      "Topic 8:\n",
      "de le et un la est pa pour que tr√®s\n",
      "Topic 9:\n",
      "great great great great price great beer price great great atmosphere love great beer great atmosphere great thanks\n",
      "Topic 10:\n",
      "taco fish taco street taco shrimp taco street chicken taco taco wa tuesday taco tuesday taco great\n",
      "Topic 11:\n",
      "first first time wa first wa first time first time wa time second great first first visit first off\n",
      "Topic 12:\n",
      "ive ever ever ive best ive best ive ever one best ive ive ever eaten worst ive ever pizza ive wa best ive\n",
      "Topic 13:\n",
      "hour happy hour 24 hour 24 hour wait hour wa half hour open 24 hour hour later hour get\n",
      "Topic 14:\n",
      "back ill definitely back come back soon back soon ill back going back came back back try\n",
      "Topic 15:\n",
      "sandwich sandwich wa chicken sandwich turkey great sandwich best sandwich deli club good sandwich sandwich shop\n",
      "Topic 16:\n",
      "staff friendly staff staff wa staff friendly wait staff great staff staff great staff always staff nice nice staff\n",
      "Topic 17:\n",
      "really really nice really enjoyed enjoyed really really really like really great wa really liked really liked\n",
      "Topic 18:\n",
      "love love love love love love love great love atmosphere absolutely love great love love coming amazing love good love\n",
      "Topic 19:\n",
      "beer tap beer selection great beer beer tap good beer craft beer wa craft beer pub\n",
      "Topic 20:\n",
      "soup soup wa noodle soup sour sour soup hot sour hot sour soup wonton miso tomato soup\n",
      "Topic 21:\n",
      "wait wait staff worth wait long wait cant wait wait time wait wa seated minute wait long\n",
      "Topic 22:\n",
      "happy happy hour great happy great happy hour wa happy happy hour menu hour menu happy hour price hour price happy hour special\n",
      "Topic 23:\n",
      "crust pie thin thin crust topping crust pizza thin crust pizza crust wa crispy pot\n",
      "Topic 24:\n",
      "amazing wa amazing amazing wa amazing great wa amazing wa amazing love die absolutely amazing amazing staff ha amazing\n",
      "Topic 25:\n",
      "go dont go go again time go go go go out not go go wa great go want go\n",
      "Topic 26:\n",
      "thai pad pad thai best thai thai wa basil thai ive pad thai wa panang spice\n",
      "Topic 27:\n",
      "cant cant wait beat cant beat cant wait go wait go wait go back enough cant say cant wait come\n",
      "Topic 28:\n",
      "not wa not not good good not not great would not not worth not much not best definitely not\n",
      "Topic 29:\n",
      "always always good always great always friendly always get ha always always fresh staff always time always always nice\n",
      "Topic 30:\n",
      "recommend highly recommend highly recommend anyone anyone would recommend definitely recommend would highly recommend would highly highly recommend anyone\n",
      "Topic 31:\n",
      "pretty pretty good wa pretty wa pretty good pretty much much good also pretty pretty decent pretty cool\n",
      "Topic 32:\n",
      "cream ice ice cream cream wa ice cream wa strawberry cream cheese vanilla tea ice tea ice cream\n",
      "Topic 33:\n",
      "drink great drink drink wa drink great good drink drink good get drink refill drink special drink menu\n",
      "Topic 34:\n",
      "delicious wa delicious delicious wa wa delicious wa fresh delicious everything wa delicious delicious great absolutely delicious delicious well delicious also\n",
      "Topic 35:\n",
      "steak steak wa steak egg rare cheese steak filet medium best steak steak house steakhouse\n",
      "Topic 36:\n",
      "excellent wa excellent excellent wa excellent great wa excellent wa great excellent excellent customer excellent price also excellent margarita\n",
      "Topic 37:\n",
      "nice wa nice really nice nice atmosphere nice staff nice wa good nice staff nice nice little super nice\n",
      "Topic 38:\n",
      "menu menu wa everything menu tasting menu menu ha tasting choice variety menu item thing menu\n",
      "Topic 39:\n",
      "mac mac cheese cheese mac cheese wa cheese wa lobster mac lobster mac cheese truffle die rib\n",
      "Topic 40:\n",
      "salad salad wa dressing chicken salad caesar lettuce kale chopped caesar salad side salad\n",
      "Topic 41:\n",
      "ive ive never time ive ive tried ive eaten eaten ive gone ive always twice good ive\n",
      "Topic 42:\n",
      "pho broth vietnamese pho wa spring spring roll best pho mi broth wa bo\n",
      "Topic 43:\n",
      "wing chicken wing buffalo pizza wing wild wing ranch wild hot boneless garlic\n",
      "Topic 44:\n",
      "awesome wa awesome awesome great awesome wa great awesome wa awesome wa awesome well totally pretty awesome thank\n",
      "Topic 45:\n",
      "im im not fan im glad glad time im im not sure im going im sure im giving\n",
      "Topic 46:\n",
      "dog hot hot dog dog wa chili hot sauce wa hot hotdog chicago hot pot\n",
      "Topic 47:\n",
      "get always get dont get usually didnt get good get usually get pay get drink get out\n",
      "Topic 48:\n",
      "location great location location wa new location downtown location ha good location convenient like location location great\n",
      "Topic 49:\n",
      "one one best wa one one favorite no one one wa one star one thing good one not one\n",
      "Topic 50:\n",
      "bean black black bean green bean rice bean bean rice refried bean dip bean cheese refried bean\n",
      "Topic 51:\n",
      "vega la la vega strip casino trip vega wa vega strip im vega must\n",
      "Topic 52:\n",
      "would would recommend would not would go would definitely recommend would come would like good would would say\n",
      "Topic 53:\n",
      "try must must try back try try out decided decided try give try wanted try try again\n",
      "Topic 54:\n",
      "coffee coffee shop shop cup coffee wa good coffee great coffee latte starbucks cup coffee\n",
      "Topic 55:\n",
      "come come back come out would come time come would come back come again always come not come every time come\n",
      "Topic 56:\n",
      "best one best wa best not best best part best pizza part city best sushi best burger\n",
      "Topic 57:\n",
      "better much much better not much even better wa much pretty much wa better get better way better\n",
      "Topic 58:\n",
      "atmosphere great atmosphere atmosphere wa atmosphere great nice atmosphere love atmosphere good atmosphere atmosphere good fun atmosphere friendly atmosphere\n",
      "Topic 59:\n",
      "no no one complaint wa no no complaint no longer no flavor longer problem no matter\n",
      "Topic 60:\n",
      "got also got got chicken got wa good got time got finally got got try husband got got seated\n",
      "Topic 61:\n",
      "sure not sure make sure back sure im not sure im sure sure get sure wa made sure make sure get\n",
      "Topic 62:\n",
      "worth not worth well worth definitely worth worth wait wa worth worth price money worth drive worth trip\n",
      "Topic 63:\n",
      "eat eat again could eat great eat good eat not eat eat sushi want eat favorite eat would eat\n",
      "Topic 64:\n",
      "roll spring roll spring roll wa egg roll sushi roll tuna california roll california spicy tuna\n",
      "Topic 65:\n",
      "night night wa late night friday late saturday saturday night friday night last night night out\n",
      "Topic 66:\n",
      "dont dont know dont get dont like dont go dont think forget dont mind dont care dont forget\n",
      "Topic 67:\n",
      "up ended ended up pick pick up fill fill up end up end make up\n",
      "Topic 68:\n",
      "little wa little little pricey little bit pricey little more great little good little little slow nice little\n",
      "Topic 69:\n",
      "make make sure make reservation reservation make up make feel always make make great make good make sure get\n",
      "Topic 70:\n",
      "server server wa great server wa server ask friendly server good server server wa friendly brought bill\n",
      "Topic 71:\n",
      "fry french fry burger fry potato fry fry wa french shake cheese fry sweet potato fry cheeseburger\n",
      "Topic 72:\n",
      "fish fish chip fish taco chip fish wa best fish batter fresh fish cod tartar\n",
      "Topic 73:\n",
      "price great price good price price great price wa fair cheap worth price price point price good\n",
      "Topic 74:\n",
      "well done well worth well done good well wa well great well prepared delicious well well prepared\n",
      "Topic 75:\n",
      "portion size huge portion size large generous huge portion portion huge good portion large portion\n",
      "Topic 76:\n",
      "experience great experience experience wa overall good experience dining experience bad experience overall experience pleasant thank\n",
      "Topic 77:\n",
      "every every time every time come every time go time come time go every time im every day every single worth every\n",
      "Topic 78:\n",
      "wrong go wrong cant go cant go wrong cant go anything anything menu really cant wa wrong\n",
      "Topic 79:\n",
      "everything everything wa everything wa delicious everything else everything menu love everything everything wa good everything wa great everything wa fresh everything else wa\n",
      "Topic 80:\n",
      "dish dish wa deep dish side dish deep favorite dish main dish main chicken dish spice\n",
      "Topic 81:\n",
      "ha ha great ha always gone ha best menu ha time ha ha good ha gone become\n",
      "Topic 82:\n",
      "coming coming back keep definitely coming definitely coming back keep coming keep coming back back love coming coming back again\n",
      "Topic 83:\n",
      "fast fast friendly wa fast friendly fast out fast fast good super fast fast wa efficient pretty fast\n",
      "Topic 84:\n",
      "more wa more much more little more more like expensive more expensive more often back more often\n",
      "Topic 85:\n",
      "even even better even though not even didnt even couldnt even more dont even wa even even get\n",
      "Topic 86:\n",
      "also wa also also good also great good also also really also got also tried wa also good delicious also\n",
      "Topic 87:\n",
      "super wa super super friendly staff super super nice staff super friendly wa super friendly super busy super good super fresh\n",
      "Topic 88:\n",
      "meal meal wa great meal enjoyed good meal enjoyed meal bad meal best meal meal great end meal\n",
      "Topic 89:\n",
      "ok wa ok ok not ok nothing ok wa wa ok not wa ok wa wa ok nothing not great bland\n",
      "Topic 90:\n",
      "next next time back next ill next door door next time im time im back next time next day\n",
      "Topic 91:\n",
      "going going back definitely going definitely going back wa going not going im going not going back wont going never going\n",
      "Topic 92:\n",
      "meat meat wa tender smoked cut quality meat meat cheese grill good meat veggie\n",
      "Topic 93:\n",
      "special nothing special daily special wa daily special great special occasion special occasion drink special special good\n",
      "Topic 94:\n",
      "though even though though wa good though even though wa great though though not wa good though others id\n",
      "Topic 95:\n",
      "bad not bad wa bad bad experience never bad bad not bad wa bad meal wasnt bad really bad\n",
      "Topic 96:\n",
      "never ive never never go never disappointed never again time never never bad never go back never tried would never\n",
      "Topic 97:\n",
      "free gluten gluten free get free coupon offer expensive wifi free refill gf\n",
      "Topic 98:\n",
      "customer great customer customer wa rude good customer business excellent customer attitude care customer customer not\n",
      "Topic 99:\n",
      "friend group friend wa recommend friend friend got friend family good friend went friend meet family friend\n",
      "Topic 100:\n",
      "sweet potato sweet potato potato fry sweet potato fry mashed mashed potato wa sweet sour tot\n",
      "Topic 101:\n",
      "sauce sauce wa hot sauce hot dipping dipping sauce bbq sauce garlic sauce good sauce not\n",
      "Topic 102:\n",
      "spot hit hit spot great spot favorite spot good spot regular wa spot go spot sushi spot\n",
      "Topic 103:\n",
      "chef sushi chef chef wa course vegan beautiful put incredible grill hibachi\n",
      "Topic 104:\n",
      "didnt didnt even didnt get didnt like didnt want care didnt care didnt know anything didnt seem\n",
      "Topic 105:\n",
      "mexican best mexican authentic mexican joint enchilada good mexican tortilla mexican joint mexican ive phoenix\n",
      "Topic 106:\n",
      "ever best ever eaten ever eaten wa best ever never ever ever tasted ever go ever wa ever since\n",
      "Topic 107:\n",
      "people group two people many people people work nice people people wa great people people working working\n",
      "Topic 108:\n",
      "manager manager wa care management manager came rude speak situation thank apologized\n",
      "Topic 109:\n",
      "rib prime prime rib rib wa rib eye bone eye cocktail beef rib dip\n",
      "Topic 110:\n",
      "tea iced iced tea boba tea wa milk green tea milk tea bubble bubble tea\n",
      "Topic 111:\n",
      "burrito burrito wa chipotle guacamole tortilla huge bean cheese california sour cream bomb\n",
      "Topic 112:\n",
      "again back again again again go back again come back again go again back again again eat again try again come again\n",
      "Topic 113:\n",
      "take take out care take care take home take time take forever take long forever get take\n",
      "Topic 114:\n",
      "flavor flavor wa no flavor full full flavor great flavor combination lacked good flavor lacked flavor\n",
      "Topic 115:\n",
      "wont probably probably wont wont back wont disappointed wont go wont going wont go back wont going back wont regret\n",
      "Topic 116:\n",
      "day next day day wa every day one day week bagel valentine valentine day day week\n",
      "Topic 117:\n",
      "down hand hand down hand down best down best sit down sit down street street sat down\n",
      "Topic 118:\n",
      "definitely definitely back definitely recommend would definitely definitely go definitely not definitely worth would definitely recommend definitely go back definitely return\n",
      "Topic 119:\n",
      "patio outside inside seating outdoor sit beautiful view outdoor seating patio wa\n",
      "Topic 120:\n",
      "worst rude worst ever wa worst worst ive worst experience life one worst worst ive ever avoid\n",
      "Topic 121:\n",
      "want dont want want go didnt want want try cheap want eat not want really want want come\n",
      "Topic 122:\n",
      "quality good quality high quality great quality quality wa ingredient value quality ha low quality meat\n",
      "Topic 123:\n",
      "pancake hash red velvet velvet red blueberry syrup hash brown brown biscuit\n",
      "Topic 124:\n",
      "came came out came back came wa time came quickly looked never came good came came table\n",
      "Topic 125:\n",
      "area dining area area wa seating area area great patio area downtown phoenix area around area youre area\n",
      "Topic 126:\n",
      "new york new york new favorite new location something new try new under new style new menu\n",
      "Topic 127:\n",
      "say say wa would say must say must needle say needle cant say id id say\n",
      "Topic 128:\n",
      "fantastic wa fantastic fantastic wa margarita lamb cocktail phoenix pittsburgh bake incredible\n",
      "Topic 129:\n",
      "short rib short rib braised tender mouth bone full rib wa fall\n",
      "Topic 130:\n",
      "dim sum sum dim cart bun mai custard tart weekend 11\n",
      "Topic 131:\n",
      "option vegan vegetarian vegan option vegetarian option healthy many option good option lot option menu option\n",
      "Topic 132:\n",
      "french toast french toast french fry toast wa french toast wa morning stuffed french onion french onion soup\n",
      "Topic 133:\n",
      "many many time choice many people many different many option many choice choose not many different\n",
      "Topic 134:\n",
      "spicy level wa spicy like spicy spicy sauce spice not spicy tuna spicy tuna spicy chicken\n",
      "Topic 135:\n",
      "way out way wa way way better go out way way much way out way go go out great way\n",
      "Topic 136:\n",
      "music live live music loud music wa band playing crowd club good music\n",
      "Topic 137:\n",
      "side side dish side salad side wa side town west one side sauce side west side potato\n",
      "Topic 138:\n",
      "lot parking parking lot choice choose wa lot lot option lot fun lot better ha lot\n",
      "Topic 139:\n",
      "tasty wa tasty fresh tasty really tasty pretty tasty tasty not tasty wa tasty well quite tasty also tasty\n",
      "Topic 140:\n",
      "youre looking youre looking youre not youll looking forward thats youre going looking great looking good\n",
      "Topic 141:\n",
      "right away right away amount right amount wa right seated right seated right away right next get right\n",
      "Topic 142:\n",
      "home close close home write home write home made nothing write nothing write home take home back home\n",
      "Topic 143:\n",
      "family run owned family owned family run like family friend family style great family family friend\n",
      "Topic 144:\n",
      "quick bite quick bite cheap wa quick quick friendly good quick looking quick pretty quick out quick\n",
      "Topic 145:\n",
      "noodle noodle soup broth wonton beef noodle noodle dish noodle wa drunken noodle drunken chicken noodle\n",
      "Topic 146:\n",
      "surprised pleasantly pleasantly surprised wa pleasantly wa pleasantly surprised wa surprised good wa wa stopped surprised wa\n",
      "Topic 147:\n",
      "small small portion wa small overpriced portion small small plate small size piece seating space\n",
      "Topic 148:\n",
      "tried ive tried havent havent tried also tried tried wa never tried everything tried time tried yet\n",
      "Topic 149:\n",
      "clean wa clean nice clean clean friendly clean staff always clean bathroom dirty clean wa clean well\n",
      "Topic 150:\n",
      "priced reasonably reasonably priced well priced terrific decently decently priced fairly plus choose\n",
      "Topic 151:\n",
      "disappointed not disappointed never disappointed wa disappointed wont disappointed wa not disappointed disappointed wa yet wa not excited\n",
      "Topic 152:\n",
      "selection beer selection great selection good selection selection beer selection wa wide great beer selection wine selection selection good\n",
      "Topic 153:\n",
      "bbq bbq sauce brisket best bbq korean bbq bbq chicken bbq pork good bbq rib real\n",
      "Topic 154:\n",
      "fried fried chicken fried rice deep fried deep chicken fried fried steak pickle chicken fried steak fried pickle\n",
      "Topic 155:\n",
      "big fan big fan not big big portion deal big deal not big fan group wa big\n",
      "Topic 156:\n",
      "find hard hard find find out find good youll find youll great find find something wa hard\n",
      "Topic 157:\n",
      "pulled pulled pork pork brisket pork wa pulled pork wa pulled pork sandwich pork sandwich slaw corn\n",
      "Topic 158:\n",
      "made made fresh reservation wa made made reservation freshly made freshly made feel home made made sure\n",
      "Topic 159:\n",
      "friendly friendly staff staff friendly great friendly super friendly always friendly friendly great good friendly fast friendly friendly good\n",
      "Topic 160:\n",
      "decent wa decent decent price pretty decent decent wa price decent decent not good decent cheap decent nothing\n",
      "Topic 161:\n",
      "ramen broth ramen wa miso japanese gyoza broth wa tonkatsu salty japan\n",
      "Topic 162:\n",
      "shrimp shrimp taco shrimp wa grit cocktail shrimp grit shrimp cocktail tempura coconut shrimp tempura\n",
      "Topic 163:\n",
      "waitress waitress wa waitress wa friendly great waitress waitress wa nice bill kept asked brought tip\n",
      "Topic 164:\n",
      "buffet buffet wa variety station asian crab leg leg bellagio wynn choose\n",
      "Topic 165:\n",
      "review yelp read yelp review based review wa reading good review read review write\n",
      "Topic 166:\n",
      "rice fried rice rice wa brown rice fried rice wa brown chicken rice rice bean rice dish rice bowl\n",
      "Topic 167:\n",
      "chinese best chinese good chinese takeout china panda typical style general cheap\n",
      "Topic 168:\n",
      "egg benedict egg benedict egg roll steak egg sausage ham poached omelet hash\n",
      "Topic 169:\n",
      "thing good thing one thing thing wa best thing bad thing thing menu first thing great thing thats\n",
      "Topic 170:\n",
      "cooked perfectly wa cooked cooked perfectly perfection perfectly cooked cooked perfection medium wa cooked perfectly seasoned\n",
      "Topic 171:\n",
      "pork belly pork belly chop bun pork chop pork wa bbq pork pork bun belly wa\n",
      "Topic 172:\n",
      "taste taste like taste good taste wa taste bud bud taste great taste fresh didnt taste everything taste\n",
      "Topic 173:\n",
      "duck confit duck confit poutine crispy gras foie foie gras duck wa peking\n",
      "Topic 174:\n",
      "stop stopped great stop decided stop phoenix downtown airport regular pit whenever\n",
      "Topic 175:\n",
      "drive thru drive thru worth drive car window correct mcdonalds park inside\n",
      "Topic 176:\n",
      "visit first visit visit wa next visit visit again time visit last visit return second visit second\n",
      "Topic 177:\n",
      "yummy wa yummy huge garlic super yummy pricey goodness meatball sausage dip\n",
      "Topic 178:\n",
      "last last time last night week last week time last last night wa last time wa wa last went last\n",
      "Topic 179:\n",
      "dining dining experience dining room dining area fine fine dining casual great dining great dining experience incredible\n",
      "Topic 180:\n",
      "bread bread wa garlic butter garlic bread pudding corn bread white bread pudding corn\n",
      "Topic 181:\n",
      "store grocery grocery store real deli buy bought huge parking asian\n",
      "Topic 182:\n",
      "table table wa seat empty get table reservation sat hostess party seated\n",
      "Topic 183:\n",
      "crepe crepe wa nutella sweet strawberry savory banana pastry savoury cafe\n",
      "Topic 184:\n",
      "fresh wa fresh ingredient fresh ingredient always fresh fresh delicious great fresh made fresh good fresh fruit\n",
      "Topic 185:\n",
      "two two people three slice two star enough piece four one two two time\n",
      "Topic 186:\n",
      "chocolate hot chocolate frozen hot chocolate chip cookie white strawberry cooky white chocolate\n",
      "Topic 187:\n",
      "wine glass list wine list bottle glass wine great wine bottle wine good wine wine selection\n",
      "Topic 188:\n",
      "still wa still still good still not im still still great still waiting wa still good still love later\n",
      "Topic 189:\n",
      "water glass ask refill water glass water wa soda glass water asked bottle\n",
      "Topic 190:\n",
      "sub meatball subway shop sub par par john jimmy different wow\n",
      "Topic 191:\n",
      "horrible wa horrible money waste waste time attitude bland mediocre stay disgusting\n",
      "Topic 192:\n",
      "top notch top notch wa top wa top notch top off die list egg top staff always\n",
      "Topic 193:\n",
      "town out town best town old town old side town im town wa town part town square\n",
      "Topic 194:\n",
      "nothing nothing special wa nothing good nothing spectacular nothing write ok nothing nothing write home write home nothing spectacular\n",
      "Topic 195:\n",
      "bit wa bit pricey bit pricey little bit bit more bit slow price bit bit dry good bit\n",
      "Topic 196:\n",
      "seafood oyster clam scallop platter mussel chowder seafood wa clam chowder pan\n",
      "Topic 197:\n",
      "beef corned beef corned beef wa beef hash mongolian beef mongolian roast roast beef corned beef hash\n",
      "Topic 198:\n",
      "recommended highly recommended highly wa recommended price great delicious price month egg benedict benedict disappoint\n",
      "Topic 199:\n",
      "perfect wa perfect amount perfect amount combination perfect size perfection date ambiance perfect wa\n",
      "Topic 200:\n",
      "grilled veggie grilled cheese grilled chicken wrap tomato mi grilled veggie banh banh mi\n",
      "Topic 201:\n",
      "owner owner wa business owner friendly care rude see everyone super nice extremely\n",
      "Topic 202:\n",
      "out check check out came out come out out wa try out hang out hang out world\n",
      "Topic 203:\n",
      "wonderful wa wonderful thank wonderful experience homemade cocktail job efficient change carry\n",
      "Topic 204:\n",
      "around corner around corner right around right around corner wa around around area around wa standing around walking around\n",
      "Topic 205:\n",
      "feel feel like make feel make feel like made feel didnt feel welcome made feel like dont feel feel welcome\n",
      "Topic 206:\n",
      "fun wa fun fun atmosphere lot fun birthday party much fun really fun cocktail unique\n",
      "Topic 207:\n",
      "salsa chip chip salsa margarita guacamole tortilla salsa wa fish chip dip enchilada\n",
      "Topic 208:\n",
      "room hotel stay room wa dining room floor casino stayed hotel room pool\n",
      "Topic 209:\n",
      "give another give try give star give another decided give would give decided could give shot\n",
      "Topic 210:\n",
      "wasnt wasnt good impressed wasnt bad wasnt impressed wasnt great enough either id wasnt even\n",
      "Topic 211:\n",
      "off strip off strip first off off menu top off start better off mall start off\n",
      "Topic 212:\n",
      "waffle chicken waffle waffle wa syrup waffle fry crispy fried chicken inside hash juicy\n",
      "Topic 213:\n",
      "minute 10 waited 10 minute 15 20 minute 15 minute 20 30 minute 30\n",
      "Topic 214:\n",
      "absolutely wa absolutely absolutely love absolutely delicious absolutely amazing wa absolutely delicious absolutely no incredible absolutely loved add\n",
      "Topic 215:\n",
      "attentive wa attentive friendly attentive server wa attentive staff wa attentive attentive wa enjoyed attentive staff attentive friendly wa friendly attentive\n",
      "Topic 216:\n",
      "green chile green chile red green tea green bean chili green chili enchilada tomato\n",
      "Topic 217:\n",
      "brunch sunday sunday brunch mimosa brunch wa brunch menu sunday morning benedict weekend morning\n",
      "Topic 218:\n",
      "average wa average above below below average above average average best average not overpriced average nothing\n",
      "Topic 219:\n",
      "indian naan masala best indian tikka south tikka masala lamb paneer chicken tikka\n",
      "Topic 220:\n",
      "told wa told asked told wa called reservation told would left wanted call\n",
      "Topic 221:\n",
      "today today wa went today time today full ate turkey def stopped son\n",
      "Topic 222:\n",
      "dessert dessert wa main cheesecake course creme dessert menu good dessert tiramisu gelato\n",
      "Topic 223:\n",
      "donut glazed dozen glaze fritter hole filled apple bakery couple\n",
      "Topic 224:\n",
      "onion ring onion ring tomato onion soup french onion pepper lettuce french onion soup cheeseburger\n",
      "Topic 225:\n",
      "item menu item item menu item wa item not several every item try item item like many item\n",
      "Topic 226:\n",
      "lobster bisque lobster bisque lobster mac lobster roll lobster mac cheese lobster wa lobster tail tail ravioli\n",
      "Topic 227:\n",
      "busy wa busy not busy always busy get busy busy wa super busy busy time weekend busy night\n",
      "Topic 228:\n",
      "carne asada carne asada asada taco carne asada taco nacho chorizo shop california grease\n",
      "Topic 229:\n",
      "gyro greek pita hummus lamb falafel greek salad gyro wa wrap feta\n",
      "Topic 230:\n",
      "loved loved everything husband loved really loved loved wa absolutely loved able thanks die enjoyed\n",
      "Topic 231:\n",
      "tasted tasted like tasted good ever tasted bland tasted great old everything tasted frozen tasted fresh\n",
      "Topic 232:\n",
      "cheese cheese wa cream cheese cheese steak goat goat cheese blue cheese grilled cheese bagel blue\n",
      "Topic 233:\n",
      "cake crab cake cake wa birthday chocolate cake bakery pastry layer red velvet\n",
      "Topic 234:\n",
      "bartender bartender wa sat margarita rude cocktail point great drink name glass\n",
      "Topic 235:\n",
      "waiter waiter wa ask hostess bill waiter came check asked seated another\n",
      "Topic 236:\n",
      "know dont know let didnt know know wa let know not know thats name know know\n",
      "Topic 237:\n",
      "appetizer entree main appetizer wa course calamari main course reservation nacho appetizer entree\n",
      "Topic 238:\n",
      "plate plate wa small plate kitchen cheese plate brought put full plate came piece\n",
      "Topic 239:\n",
      "said asked said wa wanted said no said would walked yes called girl\n",
      "Topic 240:\n",
      "pasta pasta wa pasta dish meatball tomato spaghetti al pasta salad olive mushroom\n",
      "Topic 241:\n",
      "korean korean bbq tofu bulgogi kimchi bibimbap side dish ayce garlic japanese\n",
      "Topic 242:\n",
      "slow wa slow little slow bit slow extremely slow extremely wa little slow working wa bit slow wa extremely\n",
      "Topic 243:\n",
      "star reason giving five five star one star star wa give star giving star gave\n",
      "Topic 244:\n",
      "salmon sashimi salmon wa tuna smoked smoked salmon japanese poke piece box\n",
      "Topic 245:\n",
      "look forward look like look forward trying looking looking forward forward coming doesnt forward going\n",
      "Topic 246:\n",
      "took forever took forever birthday finally forever get took long took time care took forever get\n",
      "Topic 247:\n",
      "cold wa cold warm cold beer ice cold hot cold wa soggy sitting disappointing\n",
      "Topic 248:\n",
      "game watch tv nacho sport watch game play football arcade show\n",
      "Topic 249:\n",
      "delivery ordering driver called deliver call delivered delivery wa phone online\n",
      "Topic 250:\n",
      "okay wa okay okay wa okay not bland wa okay wa wa okay not okay nothing wa bland thought\n",
      "Topic 251:\n",
      "curry chicken curry curry wa yellow yellow curry curry chicken green curry panang panang curry red curry\n",
      "Topic 252:\n",
      "crab leg crab leg crab cake king king crab king crab leg snow snow crab crab meat\n",
      "Topic 253:\n",
      "italian best italian good italian italy meatball deli tomato sausage spaghetti veal\n",
      "Topic 254:\n",
      "husband husband got son husband love husband loved husband went anniversary enjoyed husband wa liked\n",
      "Topic 255:\n",
      "else something everyone everything else anything else wa something else somewhere somewhere else anything else\n",
      "Topic 256:\n",
      "guy counter behind five five guy working behind counter girl guy wa counter wa\n",
      "Topic 257:\n",
      "bacon bacon wa wrapped enjoyed sausage egg bacon bacon wrapped gravy biscuit bacon egg\n",
      "Topic 258:\n",
      "wa good good wa good wa wa good wa wa good not good not overall burger wa good wa good well\n",
      "Topic 259:\n",
      "served wa served warm quickly served wa served hot large serve temperature course\n",
      "Topic 260:\n",
      "need work keep need work keep up people work good work keep up good up good work up good\n",
      "Topic 261:\n",
      "think dont think think wa maybe ill think ill thats dont think ill think would didnt think\n",
      "Topic 262:\n",
      "like felt like felt like wa really like seems wa like looked looked like taste like\n",
      "Topic 263:\n",
      "quite wa quite quite good wa quite good quite tasty quite nice actually quite bit not quite quite large\n",
      "Topic 264:\n",
      "authentic japanese authentic mexican style cuisine not authentic vietnamese southern eastern die\n",
      "Topic 265:\n",
      "line long long time long line long wait line wa wa long waited line up wait line\n",
      "Topic 266:\n",
      "local business support great local local business support local pub strip chain shop\n",
      "Topic 267:\n",
      "since since wa opened since opened time since decided ever since used havent since first\n",
      "Topic 268:\n",
      "really good really wa really wa really good good also really good really good wa really good also really really good also really\n",
      "Topic 269:\n",
      "dumpling steamed bao toronto boiled variety frozen asian vegetarian pork\n",
      "Topic 270:\n",
      "wa friendly staff wa staff wa friendly friendly wa wa friendly wa friendly wa wa friendly attentive wa friendly helpful friendly attentive\n",
      "Topic 271:\n",
      "definitely come definitely come back come back would definitely come would definitely definitely come back come back again definitely come again\n",
      "Topic 272:\n",
      "half half hour half price wa half full empty hour half half off half full least\n",
      "Topic 273:\n",
      "open late open late theyre late night closed 24 open 24 door 24 hour\n",
      "Topic 274:\n",
      "wife daughter wife got found birthday liked keep brought son 25\n",
      "Topic 275:\n",
      "time time wa second time second several time several couple time couple good time long time\n",
      "Topic 276:\n",
      "helpful friendly helpful extremely wa helpful wa extremely wa friendly helpful nice helpful helpful friendly extremely friendly suggestion\n",
      "Topic 277:\n",
      "house margarita thanks steak house house made house special close house salad close house near\n",
      "Topic 278:\n",
      "employee employee friendly rude attitude door working seem greeted boyfriend walked\n",
      "Topic 279:\n",
      "however however wa good however overall extremely however not group dry large fan\n",
      "Topic 280:\n",
      "bloody mary bloody mary saturday mimosa vodka soon thank margarita great vibe\n",
      "Topic 281:\n",
      "went time went went back went wa birthday week good went ago went friend went saturday\n",
      "Topic 282:\n",
      "cool vibe decor wa cool really cool pretty cool cocktail hip unique super cool\n",
      "Topic 283:\n",
      "could wish wish could could not could eat could give use could go could use could better\n",
      "Topic 284:\n",
      "min waiting 30 min 20 waited 30 20 min 10 min 15 min 10\n",
      "Topic 285:\n",
      "kind different wa kind different kind kind like something different seen etc kind enough classic\n",
      "Topic 286:\n",
      "reasonable reasonable price price reasonable price outstanding wa reasonable price good delicious price environment ambiance\n",
      "Topic 287:\n",
      "extra charge charged card topping extra star pay extra charge bill charge extra\n",
      "Topic 288:\n",
      "kid kid menu adult kid meal play child son year old change margarita\n",
      "Topic 289:\n",
      "eating like eating wa eating time eating sick eating again up eating felt first time eating eating wa\n",
      "Topic 290:\n",
      "especially great especially good especially especially since shawarma liked especially considering solid view group\n",
      "Topic 291:\n",
      "far far best wa far far better toronto far away city far favorite far worst concerned\n",
      "Topic 292:\n",
      "yum yum yum tom yum tom yum soup tom yum soup bagel ate see fabulous\n",
      "Topic 293:\n",
      "terrible wa terrible rude awful poor waste time waste frozen worse mediocre\n",
      "Topic 294:\n",
      "enjoy really enjoy sit enjoy meal view youll much plus truly sit down\n",
      "Topic 295:\n",
      "year old ago year ago year old past 10 year last year lived time year\n",
      "Topic 296:\n",
      "go back back go definitely go back definitely go wait go back wait go cant wait go would go back go back again\n",
      "Topic 297:\n",
      "high high quality end expectation high end high expectation price high high price little high high hope\n",
      "Topic 298:\n",
      "gem hidden hidden gem found little gem real true inexpensive truly ambiance\n",
      "Topic 299:\n",
      "bowl bowl wa poke rice bowl chipotle teriyaki brown rice brown juice smoothie\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "display_topics(nmf, tfidf_vectorizer_mini__stop_ngram.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create corpus of sentences from mini\n",
    "sentence_corpus_mini = []\n",
    "for review in X_mini:\n",
    "    words = review.split(\"', '\")\n",
    "    words[0] = words[0][2:]\n",
    "    words[-1] = words[-1][:-2]\n",
    "    sentence_corpus_mini.append(' '.join(words))\n",
    "\n",
    "# Create tokenized corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in sentence_corpus_mini]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions to create a feature array\n",
    "def average_word_vectors(words, model, vocabulary, num_features):   \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.   \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    return feature_vector\n",
    "    \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Create word 2 vec model \n",
    "feature_size = 100\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, window=5, min_count=10, workers=4)\n",
    "w2v_dictionary = dict(zip(w2v_model.wv.index2word, w2v_model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20136, 100)\n"
     ]
    }
   ],
   "source": [
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n",
    "                                             num_features=feature_size)\n",
    "w2v_mini = pd.DataFrame(w2v_feature_array)\n",
    "print(w2v_mini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Create word 2 vec model with 200 features\n",
    "feature_size_2 = 200\n",
    "w2v_model_2 = word2vec.Word2Vec(tokenized_corpus, size=feature_size_2, window=5, min_count=10, workers=4)\n",
    "w2v_dictionary_2 = dict(zip(w2v_model_2.wv.index2word, w2v_model_2.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20136, 200)\n"
     ]
    }
   ],
   "source": [
    "w2v_feature_array_2 = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model_2,\n",
    "                                             num_features=feature_size_2)\n",
    "w2v_mini_2 = pd.DataFrame(w2v_feature_array_2)\n",
    "print(w2v_mini_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20136, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Create word 2 vec model with 1000 features\n",
    "feature_size_3 = 1000\n",
    "w2v_model_3 = word2vec.Word2Vec(tokenized_corpus, size=feature_size_3, window=5, min_count=10, workers=4)\n",
    "w2v_dictionary_3 = dict(zip(w2v_model_3.wv.index2word, w2v_model_3.wv.syn0))\n",
    "w2v_feature_array_3 = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model_3,\n",
    "                                             num_features=feature_size_3)\n",
    "w2v_mini_3 = pd.DataFrame(w2v_feature_array_3)\n",
    "print(w2v_mini_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Google news word 2 vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = os.path.join(dir, '02_processed_data','GoogleNews-vectors-negative300.bin.gz') \n",
    "word2vec_google = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, num_features):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(num_features)\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(num_features) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(corpus, model):\n",
    "    num_features=300\n",
    "    features = [get_average_word2vec(tokenized_sentence, model, num_features) for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_feature_array_google = get_word2vec_embeddings(corpus=tokenized_corpus, model= word2vec_google)\n",
    "w2v_mini_google = pd.DataFrame(w2v_feature_array_google)\n",
    "print(w2v_mini_google.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using mini dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mini dataset, a variety of models will be trained on a variety of feature sets to identify promising candidates. The promising combinations will then be tuned in the following section and trained on the full training data set. \n",
    "\n",
    "It should be noted that to assess model performance, the classification accuracy will be the primary metric. \n",
    "A Confusion matrix will be created using the best performing parameters from the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model tuning\n",
    "def cross_validation_tuning(classifier, param_grid, X_trn, y_trn):\n",
    "    classifier_cv = GridSearchCV(classifier, param_grid, cv=3)\n",
    "    classifier_cv.fit(X_trn, y_trn)\n",
    "    # Print the optimal parameters and best score\n",
    "    print(\"Tuned Classifier Parameters: {}\".format(classifier_cv.best_params_))\n",
    "    print(\"Tuned Classifier Accuracy: {:.3f}\".format(classifier_cv.best_score_))\n",
    "    # Predict the labels\n",
    "    pred = classifier_cv.predict(X_trn)\n",
    "    # Compute accuracy\n",
    "    score = metrics.accuracy_score(y_trn, pred)\n",
    "    # Calculate and print the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_trn, pred, labels=[1,2,3,4,5])\n",
    "    print('For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.')\n",
    "    print(cm)\n",
    "    return classifier_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Naive_bayes model\n",
    "def nb_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'alpha': np.arange(0, 1, 0.333)}\n",
    "    # Iterate over the alphas and print the corresponding score\n",
    "    nb_classifier = MultinomialNB()\n",
    "    tuned_nb_classifier = cross_validation_tuning(nb_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_nb_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Logistic regression model\n",
    "def logreg_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'C': [0.0001, 1, 100], 'penalty': ['l1', 'l2']} #param_grid = {'C': np.logspace(-5, 8, 15), 'penalty': ['l1', 'l2']}\n",
    "    logreg_classifier = LogisticRegression()\n",
    "    tuned_logreg_classifier = cross_validation_tuning(logreg_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_logreg_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define SVM model\n",
    "def svm_model(X_trn, y_trn):\n",
    "    # Create parameters \n",
    "    param_grid = {'kernel': ['rbf', 'linear']} #{'kernel': ['rbf', 'linear'], 'C': [1, 10, 100], 'gamma': [0.1, 0.01]} \n",
    "    svm_classifier = SVC()\n",
    "    tuned_svm_classifier = cross_validation_tuning(svm_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_svm_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Random forest model\n",
    "def ranforest_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {\"n_estimators\": [150, 300, 500],\n",
    "                  \"min_samples_leaf\": [5, 10]}\n",
    "    # param_grid = {\"n_estimators\": [2, 10, 100, 300, 1000],\"max_depth\": [2, 10, 100, 300], \"min_samples_split\": [2, 10, 100],\"min_samples_leaf\": [1, 10, 100]}\n",
    "    ranforest_classifier = RandomForestClassifier()\n",
    "    tuned_ranforest_classifier = cross_validation_tuning(ranforest_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_ranforest_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define SGD model \n",
    "def sgd_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {\"penalty\": ['l1', 'l2', 'elasticnet'],\n",
    "                  \"l1_ratio\": [0.1, 0.3, 0.5] }\n",
    "    sgd_classifier = SGDClassifier(random_state= 42, max_iter=4)\n",
    "    tuned_sgd_classifier = cross_validation_tuning(sgd_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_sgd_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define XGBoost model \n",
    "def xgb_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'min_child_weight': [3],\n",
    "                 'max_depth': [4]}\n",
    "    xgb_classifier = XGBClassifier(learning_rate =0.2, seed=42)\n",
    "    tuned_xgb_classifier = cross_validation_tuning(xgb_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_xgb_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "def construct_dnn(input_shape):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation ='relu', input_shape=input_shape ))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "    dnn_model.add(Dense(512, activation ='relu'))\n",
    "    dnn_model.add(Dropout(0.3))\n",
    "#     dnn_model.add(Dense(512, activation ='relu'))\n",
    "#     dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(5, activation='softmax'))\n",
    "    dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return dnn_model\n",
    "\n",
    "# Build model\n",
    "def dnn_model(X_trn, y_trn):\n",
    "    n_cols = X_trn.shape[1]\n",
    "    input_shape =(n_cols, )\n",
    "    model = construct_dnn(input_shape)\n",
    "    \n",
    "    # Define early_stopping_monitor\n",
    "    early_stopping_monitor = EarlyStopping(patience=2)\n",
    "    # Define fit\n",
    "    history = model.fit(X_trn, pd.get_dummies(y_trn), epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline assumes review is a 5 star rating (the most common class of data). The corresponding baseline accuracy is ~ 36.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36730234406038936\n"
     ]
    }
   ],
   "source": [
    "# Calculate baseline\n",
    "length = len(y_mini)\n",
    "correct_pred = len(y_mini[y_mini == 5])\n",
    "baseline_accuracy = correct_pred / length \n",
    "print(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_sets = {'count': count_mini, \n",
    "                'tfidf': tfidf_mini, \n",
    "                'count stop': count_mini__stop, \n",
    "                'tfidf stop': tfidf_mini__stop, \n",
    "                'count stop ngram': count_mini__stop_ngram, \n",
    "                'tfidf stop ngram': tfidf_mini__stop_ngram,\n",
    "                'count stop ngram lda': count_mini__stop_ngram_lda.tocsr(), \n",
    "                'tfidf stop ngram nmf': tfidf_train_mini__stop_ngram_nmf.tocsr(),\n",
    "                'word to vec': w2v_mini, \n",
    "                'word to vec 2': w2v_mini_2}\n",
    "non_negative_feature_sets = feature_sets.copy()\n",
    "del non_negative_feature_sets['word to vec']\n",
    "del non_negative_feature_sets['word to vec 2']\n",
    "w2v_feature_sets = {'word to vec': w2v_mini, \n",
    "                   'word to vec 2': w2v_mini_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_feature_set_1000 = {'word to vec 3': w2v_mini_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_feature_set_google = {'word to vec google': w2v_mini_google} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define test for feature sets\n",
    "def test_features(model, sets):\n",
    "    results = defaultdict(float)\n",
    "    for key, x_values_mini in sets.items():\n",
    "        print(key)\n",
    "        model_instance = model(x_values_mini, y_mini)\n",
    "        results[key] = model_instance.best_score_\n",
    "        print('')\n",
    "    print('--------------------------')\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define deep neural net tests for feature sets\n",
    "def dnn_test_results(sets):\n",
    "    results = defaultdict(float)\n",
    "    for key, x_values_mini in sets.items():\n",
    "        print(key)\n",
    "        model_instance, history = dnn_model(x_values_mini, y_mini)\n",
    "        results[key] = max(history.history['val_acc'])\n",
    "        print('')\n",
    "    print('--------------------------')\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.99900000000000011}\n",
      "Tuned Classifier Accuracy: 0.559\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1651  329  118   67   50]\n",
      " [ 450  992  345  109   83]\n",
      " [ 233  274 1571  584  266]\n",
      " [ 143  142  469 2992 1872]\n",
      " [ 119   75  146 1087 5969]]\n",
      "\n",
      "tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.33300000000000002}\n",
      "Tuned Classifier Accuracy: 0.527\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1618  117  104  207  169]\n",
      " [ 446  517  257  531  228]\n",
      " [ 171   56  783 1342  576]\n",
      " [  67   11   47 3085 2408]\n",
      " [  47    2   17  758 6572]]\n",
      "\n",
      "count stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.99900000000000011}\n",
      "Tuned Classifier Accuracy: 0.568\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1683  330  113   54   35]\n",
      " [ 450 1029  345   92   63]\n",
      " [ 230  292 1597  573  236]\n",
      " [ 132  148  501 3043 1794]\n",
      " [ 122   60  133 1078 6003]]\n",
      "\n",
      "tfidf stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.33300000000000002}\n",
      "Tuned Classifier Accuracy: 0.536\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1652  129  103  193  138]\n",
      " [ 439  554  265  528  193]\n",
      " [ 158   54  866 1339  511]\n",
      " [  60    9   63 3138 2348]\n",
      " [  41    3   14  768 6570]]\n",
      "\n",
      "count stop ngram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.99900000000000011}\n",
      "Tuned Classifier Accuracy: 0.583\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1785  281   99   26   24]\n",
      " [ 392 1235  245   60   47]\n",
      " [ 200  274 1848  425  181]\n",
      " [ 118  125  477 3263 1635]\n",
      " [  96   51  133  996 6120]]\n",
      "\n",
      "tfidf stop ngram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.33300000000000002}\n",
      "Tuned Classifier Accuracy: 0.573\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1808  133   93  110   71]\n",
      " [ 391  915  312  262   99]\n",
      " [ 148   77 1428  951  324]\n",
      " [  69   20  110 3560 1859]\n",
      " [  39    6   22  763 6566]]\n",
      "\n",
      "count stop ngram lda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.99900000000000011}\n",
      "Tuned Classifier Accuracy: 0.583\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1782  280  101   27   25]\n",
      " [ 391 1232  248   60   48]\n",
      " [ 201  270 1832  433  192]\n",
      " [ 121  121  472 3227 1677]\n",
      " [  94   51  136  982 6133]]\n",
      "\n",
      "tfidf stop ngram nmf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameters: {'alpha': 0.33300000000000002}\n",
      "Tuned Classifier Accuracy: 0.574\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1810  138   94  106   67]\n",
      " [ 398  924  311  249   97]\n",
      " [ 148   80 1445  931  324]\n",
      " [  75   18  113 3572 1840]\n",
      " [  41    6   24  804 6521]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.55904847040127137, 'tfidf': 0.52731426301152162, 'count stop': 0.56778903456495833, 'tfidf stop': 0.53615415176797776, 'count stop ngram': 0.58263806118394912, 'tfidf stop ngram': 0.5733015494636472, 'count stop ngram lda': 0.58338299562971796, 'tfidf stop ngram nmf': 0.57354986094557014})\n"
     ]
    }
   ],
   "source": [
    "# run tests for Naive Bayes\n",
    "NB_mini_results = test_features(nb_model, non_negative_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.540\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1957   78   44   65   71]\n",
      " [ 227 1315  195  133  109]\n",
      " [  83  130 1853  540  322]\n",
      " [  28   40  215 3670 1665]\n",
      " [  18   14   72  731 6561]]\n",
      "\n",
      "tfidf\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.570\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1692  187  100  125  111]\n",
      " [ 463  708  366  292  150]\n",
      " [ 133  201 1183 1008  403]\n",
      " [  54   47  253 3195 2069]\n",
      " [  34   20   68  996 6278]]\n",
      "\n",
      "count stop\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.548\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2007   66   33   58   51]\n",
      " [ 216 1380  178  116   89]\n",
      " [  69  127 1926  544  262]\n",
      " [  29   44  195 3765 1585]\n",
      " [  12   10   64  689 6621]]\n",
      "\n",
      "tfidf stop\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.579\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1720  190  100  105  100]\n",
      " [ 484  730  376  261  128]\n",
      " [ 133  183 1238 1040  334]\n",
      " [  57   42  257 3243 2019]\n",
      " [  31   16   59  977 6313]]\n",
      "\n",
      "count stop ngram\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.550\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2102   33   21   31   28]\n",
      " [ 105 1692   89   48   45]\n",
      " [  38   51 2442  251  146]\n",
      " [  13   13  101 4484 1007]\n",
      " [   7    1   30  388 6970]]\n",
      "\n",
      "tfidf stop ngram\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.587\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1720  184   99  111  101]\n",
      " [ 473  744  388  252  122]\n",
      " [ 133  190 1259 1019  327]\n",
      " [  50   39  240 3280 2009]\n",
      " [  33    7   50  973 6333]]\n",
      "\n",
      "count stop ngram lda\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.550\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2108   31   26   25   25]\n",
      " [ 108 1693   92   50   36]\n",
      " [  39   53 2440  252  144]\n",
      " [  14   13   98 4511  982]\n",
      " [  10    2   28  382 6974]]\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Tuned Classifier Parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.587\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1720  184   99  111  101]\n",
      " [ 473  744  389  253  120]\n",
      " [ 133  190 1260 1019  326]\n",
      " [  50   39  239 3282 2008]\n",
      " [  33    7   50  974 6332]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.54007747318235999, 'tfidf': 0.56982518871672627, 'count stop': 0.54787445371473975, 'tfidf stop': 0.57936034962256655, 'count stop ngram': 0.54961263408820027, 'tfidf stop ngram': 0.58730631704410008, 'count stop ngram lda': 0.550357568533969, 'tfidf stop ngram nmf': 0.58745530393325385})\n"
     ]
    }
   ],
   "source": [
    "# run tests for logistic regression \n",
    "logreg_mini_results = test_features(logreg_model, non_negative_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec\n",
      "Tuned Classifier Parameters: {'C': 100, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.546\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1477  159  138  207  234]\n",
      " [ 583  279  379  503  235]\n",
      " [ 250  128  753 1362  435]\n",
      " [ 143   43  257 2775 2400]\n",
      " [ 130   12   72 1211 5971]]\n",
      "\n",
      "word to vec 2\n",
      "Tuned Classifier Parameters: {'C': 100, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.556\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1548  185  122  165  195]\n",
      " [ 550  393  395  456  185]\n",
      " [ 225  160  836 1307  400]\n",
      " [ 120   48  300 2918 2232]\n",
      " [ 106   21   61 1243 5965]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec': 0.54633492252681759, 'word to vec 2': 0.55572109654350421})\n"
     ]
    }
   ],
   "source": [
    "logreg_w2v_mini_results = test_features(logreg_model, w2v_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec 3\n",
      "Tuned Classifier Parameters: {'C': 100, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.560\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1585  168  120  168  174]\n",
      " [ 562  385  405  450  177]\n",
      " [ 219  158  859 1308  384]\n",
      " [ 105   45  313 2920 2235]\n",
      " [  96   22   54 1225 5999]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec 3': 0.55954509336511715})\n"
     ]
    }
   ],
   "source": [
    "logreg_w2v_mini_results_1000 = test_features(logreg_model, w2v_feature_set_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec google\n",
      "Tuned Classifier Parameters: {'C': 100, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.558\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1645  192  130  118  130]\n",
      " [ 553  473  419  366  168]\n",
      " [ 208  189  822 1301  408]\n",
      " [  72   50  279 2924 2293]\n",
      " [  67   16   80 1218 6015]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec google': 0.55790623758442592})\n"
     ]
    }
   ],
   "source": [
    "logreg_w2v_mini_results_google = test_features(logreg_model, w2v_feature_set_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.510\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2191   14    6    3    1]\n",
      " [  59 1865   43    7    5]\n",
      " [  17   49 2577  214   71]\n",
      " [  16   25  181 4328 1068]\n",
      " [  10   17   81  608 6680]]\n",
      "\n",
      "tfidf\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.563\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1885  170   78   48   34]\n",
      " [ 276 1227  289  132   55]\n",
      " [ 108  162 1767  661  230]\n",
      " [  41   63  266 3800 1448]\n",
      " [  28   24  109 1000 6235]]\n",
      "\n",
      "count stop\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.517\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2203    9    3    0    0]\n",
      " [  41 1903   31    2    2]\n",
      " [  11   41 2650  183   43]\n",
      " [  16   20  178 4401 1003]\n",
      " [  12   13   62  594 6715]]\n",
      "\n",
      "tfidf stop\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.578\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1910  169   66   52   18]\n",
      " [ 276 1279  268  118   38]\n",
      " [  96  164 1825  650  193]\n",
      " [  42   54  264 3860 1398]\n",
      " [  19   20  106  960 6291]]\n",
      "\n",
      "count stop ngram\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.519\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2215    0    0    0    0]\n",
      " [   0 1978    1    0    0]\n",
      " [   0    0 2915   11    2]\n",
      " [   2    3   21 5246  346]\n",
      " [   3    2   16  184 7191]]\n",
      "\n",
      "tfidf stop ngram\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.586\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1999  121   53   29   13]\n",
      " [ 193 1489  190   77   30]\n",
      " [  69  119 2060  528  152]\n",
      " [  22   29  179 4177 1211]\n",
      " [  14    9   64  759 6550]]\n",
      "\n",
      "count stop ngram lda\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.520\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2215    0    0    0    0]\n",
      " [   0 1979    0    0    0]\n",
      " [   0    0 2918    8    2]\n",
      " [   2    2   19 5257  338]\n",
      " [   3    2   17  181 7193]]\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.585\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2001  120   53   28   13]\n",
      " [ 194 1490  189   77   29]\n",
      " [  68  120 2063  523  154]\n",
      " [  22   30  181 4179 1206]\n",
      " [  14    9   62  770 6541]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.51008144616607076, 'tfidf': 0.56267381803734606, 'count stop': 0.51673619388160508, 'tfidf stop': 0.57762216924910603, 'count stop ngram': 0.51877234803337302, 'tfidf stop ngram': 0.58601509733810087, 'count stop ngram lda': 0.52026221692491059, 'tfidf stop ngram nmf': 0.58517083829956296})\n"
     ]
    }
   ],
   "source": [
    "# run tests for SVM\n",
    "svm_mini_results = test_features(svm_model, non_negative_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.552\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1462  300  150  140  163]\n",
      " [ 555  520  435  351  118]\n",
      " [ 241  297  924 1194  272]\n",
      " [ 160  110  433 3036 1879]\n",
      " [ 149   48  115 1560 5524]]\n",
      "\n",
      "word to vec 2\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.555\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1499  280  150  133  153]\n",
      " [ 564  532  437  335  111]\n",
      " [ 229  286  963 1194  256]\n",
      " [ 155  112  442 3048 1861]\n",
      " [ 144   42  118 1585 5507]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec': 0.55174811283273739, 'word to vec 2': 0.5553237981724275})\n"
     ]
    }
   ],
   "source": [
    "svm_w2v_mini_results = test_features(svm_model, w2v_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec 3\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.554\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1492  287  150  141  145]\n",
      " [ 562  520  439  346  112]\n",
      " [ 233  275  939 1215  266]\n",
      " [ 148  115  448 3066 1841]\n",
      " [ 135   47  107 1590 5517]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec 3': 0.5539332538736591})\n"
     ]
    }
   ],
   "source": [
    "svm_w2v_mini_results_1000 = test_features(svm_model, w2v_feature_set_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec google\n",
      "Tuned Classifier Parameters: {'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.547\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1586  231  134  172   92]\n",
      " [ 572  395  372  525  115]\n",
      " [ 234  174  565 1680  275]\n",
      " [ 111   53  175 3394 1885]\n",
      " [  81   26   46 1642 5601]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec google': 0.54673222089789431})\n"
     ]
    }
   ],
   "source": [
    "svm_w2v_mini_results_google = test_features(svm_model, w2v_feature_set_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 150}\n",
      "Tuned Classifier Accuracy: 0.510\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1650    8   58  172  327]\n",
      " [ 312  622  126  510  409]\n",
      " [  89    3 1201  871  764]\n",
      " [  20    0    6 3487 2105]\n",
      " [  11    1    9  174 7201]]\n",
      "\n",
      "tfidf\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 300}\n",
      "Tuned Classifier Accuracy: 0.519\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1787    6   43  131  248]\n",
      " [ 256  956   81  359  327]\n",
      " [  72    1 1671  572  612]\n",
      " [  14    0    4 4009 1591]\n",
      " [   7    1    4   77 7307]]\n",
      "\n",
      "count stop\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 150}\n",
      "Tuned Classifier Accuracy: 0.511\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1665   10   49  169  322]\n",
      " [ 270  698  105  506  400]\n",
      " [  63    0 1322  833  710]\n",
      " [  13    0    8 3572 2025]\n",
      " [   5    0    7  131 7253]]\n",
      "\n",
      "tfidf stop\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.522\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1846    6   34  116  213]\n",
      " [ 201 1062   73  346  297]\n",
      " [  57    1 1817  515  538]\n",
      " [   8    0    3 4160 1447]\n",
      " [   4    0    5   53 7334]]\n",
      "\n",
      "count stop ngram\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.514\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1654   12   41  166  342]\n",
      " [ 271  691  108  502  407]\n",
      " [  60    3 1239  890  736]\n",
      " [  10    0   10 3431 2167]\n",
      " [   5    0    4  138 7249]]\n",
      "\n",
      "tfidf stop ngram\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.524\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1797    9   36  130  243]\n",
      " [ 224  977   82  385  311]\n",
      " [  53    2 1668  629  576]\n",
      " [  10    0    5 3994 1609]\n",
      " [   2    0    3   80 7311]]\n",
      "\n",
      "count stop ngram lda\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 150}\n",
      "Tuned Classifier Accuracy: 0.473\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1739    0    8   75  393]\n",
      " [  76 1165   19  217  502]\n",
      " [  12    0 1899  236  781]\n",
      " [   1    0    1 4205 1411]\n",
      " [   1    0    0    5 7390]]\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 300}\n",
      "Tuned Classifier Accuracy: 0.522\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2009    0    3   56  147]\n",
      " [  88 1469    6  162  254]\n",
      " [  23    0 2276  216  413]\n",
      " [   2    0    0 4843  773]\n",
      " [   2    0    1    6 7387]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.51023043305522442, 'tfidf': 0.51862336114421936, 'count stop': 0.51147199046483904, 'tfidf stop': 0.52180174811283275, 'count stop ngram': 0.51445172824791419, 'tfidf stop ngram': 0.52438418752483118, 'count stop ngram lda': 0.47308303535955504, 'tfidf stop ngram nmf': 0.5224473579658323})\n"
     ]
    }
   ],
   "source": [
    "# run tests for random forest\n",
    "rf_mini_results = test_features(ranforest_model, non_negative_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 300}\n",
      "Tuned Classifier Accuracy: 0.505\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2059    0    7   56   93]\n",
      " [  28 1759    0   75  117]\n",
      " [  33    1 2697   31  166]\n",
      " [  24    1    0 5484  109]\n",
      " [  18    0    0    0 7378]]\n",
      "\n",
      "word to vec 2\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 300}\n",
      "Tuned Classifier Accuracy: 0.513\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2089    0    3   51   72]\n",
      " [  22 1790    0   69   98]\n",
      " [  33    0 2723   22  150]\n",
      " [  23    0    0 5493  102]\n",
      " [   9    0    1    0 7386]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec': 0.50481724274930473, 'word to vec 2': 0.51276321017083826})\n"
     ]
    }
   ],
   "source": [
    "rf_w2v_mini_results = test_features(ranforest_model, w2v_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec 3\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 300}\n",
      "Tuned Classifier Accuracy: 0.518\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2103    0    4   37   71]\n",
      " [  14 1841    0   53   71]\n",
      " [  32    0 2757   11  128]\n",
      " [  22    0    0 5509   87]\n",
      " [  10    0    0    0 7386]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec 3': 0.51822606277314265})\n"
     ]
    }
   ],
   "source": [
    "rf_w2v_mini_results_1000 = test_features(ranforest_model, w2v_feature_set_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec google\n",
      "Tuned Classifier Parameters: {'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "Tuned Classifier Accuracy: 0.495\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[2150    0    2    9   54]\n",
      " [   6 1877    1   22   73]\n",
      " [  10    0 2811    2  105]\n",
      " [   9    0    0 5586   23]\n",
      " [   1    0    0    0 7395]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec google': 0.49543106873261822})\n"
     ]
    }
   ],
   "source": [
    "rf_w2v_mini_results_google = test_features(ranforest_model, w2v_feature_set_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.5, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.510\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1929   63   56   75   92]\n",
      " [ 442  821  315  241  160]\n",
      " [ 205  104 1540  674  405]\n",
      " [  97   49  393 3275 1804]\n",
      " [  72   19  135  942 6228]]\n",
      "\n",
      "tfidf\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.555\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1959   49   74   55   78]\n",
      " [ 534  777  314  202  152]\n",
      " [ 168   94 1460  761  445]\n",
      " [  71   27  230 2935 2355]\n",
      " [  38   10   76  562 6710]]\n",
      "\n",
      "count stop\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.5, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.516\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1816  218   46   84   51]\n",
      " [ 268 1251  202  180   78]\n",
      " [ 144  375 1463  693  253]\n",
      " [  65  197  335 3540 1481]\n",
      " [  58   72   98 1182 5986]]\n",
      "\n",
      "tfidf stop\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.5, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.564\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1905   54   88   58  110]\n",
      " [ 746  333  437  270  193]\n",
      " [ 252  114  970 1052  540]\n",
      " [  95   39  172 2392 2920]\n",
      " [  42   11   47  465 6831]]\n",
      "\n",
      "count stop ngram\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.3, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.528\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1928  192   31   30   34]\n",
      " [ 174 1587  113   46   59]\n",
      " [ 139  441 1768  305  275]\n",
      " [  91  314  333 3137 1743]\n",
      " [  44   84   79  481 6708]]\n",
      "\n",
      "tfidf stop ngram\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.573\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1992   42   61   48   72]\n",
      " [ 547  885  273  148  126]\n",
      " [ 149   68 1668  669  374]\n",
      " [  47   13  163 3141 2254]\n",
      " [  25    2   45  432 6892]]\n",
      "\n",
      "count stop ngram lda\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.5, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.524\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1973  104   63   35   40]\n",
      " [ 219 1436  175   86   63]\n",
      " [ 114  193 1941  430  250]\n",
      " [  65   76  277 3707 1493]\n",
      " [  37   26   61  606 6666]]\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.3, 'penalty': 'elasticnet'}\n",
      "Tuned Classifier Accuracy: 0.573\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1963   50   57   46   99]\n",
      " [ 705  574  338  194  168]\n",
      " [ 221   98 1290  842  477]\n",
      " [  72   18  146 2705 2677]\n",
      " [  32    4   40  396 6924]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.51037941994437819, 'tfidf': 0.5553237981724275, 'count stop': 0.51648788239968213, 'tfidf stop': 0.56426301152165281, 'count stop ngram': 0.52771156138259834, 'tfidf stop ngram': 0.57305323798172425, 'count stop ngram lda': 0.52373857767183152, 'tfidf stop ngram nmf': 0.57255661501787847})\n"
     ]
    }
   ],
   "source": [
    "# run tests for sgd\n",
    "sgd_mini_results = test_features(sgd_model, non_negative_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.458\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1314  220  134    7  540]\n",
      " [ 570  364  449    8  588]\n",
      " [ 316  358  913   41 1300]\n",
      " [ 174  196  806   83 4359]\n",
      " [ 101   67  248   38 6942]]\n",
      "\n",
      "word to vec 2\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.482\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1257   97  207    1  653]\n",
      " [ 472  188  642    7  670]\n",
      " [ 227  124 1197   46 1334]\n",
      " [ 109   71  912  138 4388]\n",
      " [  67   22  253   41 7013]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec': 0.45783671036948748, 'word to vec 2': 0.48162495033770364})\n"
     ]
    }
   ],
   "source": [
    "sgd_w2v_mini_results = test_features(sgd_model, w2v_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec 3\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.494\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1536    1  131    8  539]\n",
      " [ 756    9  506   22  686]\n",
      " [ 390    2  933  156 1447]\n",
      " [ 185    1  511  276 4645]\n",
      " [ 100    0  109   88 7099]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec 3': 0.49359356376638858})\n"
     ]
    }
   ],
   "source": [
    "sgd_w2v_mini_results_1000 = test_features(sgd_model, w2v_feature_set_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec google\n",
      "Tuned Classifier Parameters: {'l1_ratio': 0.1, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.516\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1560    3  100    3  549]\n",
      " [ 684   14  398    5  878]\n",
      " [ 296    3  628   57 1944]\n",
      " [  96    2  315   99 5106]\n",
      " [  45    0   45   36 7270]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec google': 0.51589193484306717})\n"
     ]
    }
   ],
   "source": [
    "sgd_w2v_mini_results_google = test_features(sgd_model, w2v_feature_set_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.547\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1571  121  102  166  255]\n",
      " [ 300  849  234  338  258]\n",
      " [ 116  116 1279  904  513]\n",
      " [  52   19  164 3160 2223]\n",
      " [  32    9   46  935 6374]]\n",
      "\n",
      "tfidf\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.544\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1638  105   89  186  197]\n",
      " [ 290  983  190  322  194]\n",
      " [ 107   94 1384  872  471]\n",
      " [  57   23  113 3423 2002]\n",
      " [  31    9   56  915 6385]]\n",
      "\n",
      "count stop\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.549\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1597  123  103  168  224]\n",
      " [ 328  880  247  302  222]\n",
      " [ 111  113 1384  872  448]\n",
      " [  49   28  152 3240 2149]\n",
      " [  34   11   51  899 6401]]\n",
      "\n",
      "tfidf stop\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.553\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1699  111   86  143  176]\n",
      " [ 302 1044  191  268  174]\n",
      " [ 111  103 1485  827  402]\n",
      " [  56   18  118 3514 1912]\n",
      " [  35   13   51  907 6390]]\n",
      "\n",
      "count stop ngram\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.556\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1604  139   92  156  224]\n",
      " [ 305  917  252  282  223]\n",
      " [ 106  120 1346  891  465]\n",
      " [  38   23  138 3250 2169]\n",
      " [  29    6   33  928 6400]]\n",
      "\n",
      "tfidf stop ngram\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.556\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1716  108   82  145  164]\n",
      " [ 297 1027  205  282  168]\n",
      " [ 101  115 1498  820  394]\n",
      " [  47   17  111 3485 1958]\n",
      " [  35    4   38  930 6389]]\n",
      "\n",
      "count stop ngram lda\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.560\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1758  121   90  108  138]\n",
      " [ 297 1099  219  221  143]\n",
      " [ 133  131 1513  783  368]\n",
      " [  74   35  154 3475 1880]\n",
      " [  59   18   60  887 6372]]\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.571\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1897   73   52  107   86]\n",
      " [ 243 1258  164  217   97]\n",
      " [ 114   93 1688  725  308]\n",
      " [  57   28  118 3814 1601]\n",
      " [  33    9   40  753 6561]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.54688120778704807, 'tfidf': 0.54444775526420341, 'count stop': 0.54866905045689318, 'tfidf stop': 0.55343663090981332, 'count stop ngram': 0.55587008343265798, 'tfidf stop ngram': 0.55636670639650376, 'count stop ngram lda': 0.56048867699642435, 'tfidf stop ngram nmf': 0.57146404449741761})\n"
     ]
    }
   ],
   "source": [
    "# run tests for XG Boost\n",
    "xgb_mini_results = test_features(xgb_model, non_negative_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.526\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1823   99   78  107  108]\n",
      " [ 222 1164  178  279  136]\n",
      " [ 150  126 1590  760  302]\n",
      " [ 105   96  207 3824 1386]\n",
      " [  83   44   89  849 6331]]\n",
      "\n",
      "word to vec 2\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.532\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1867   75   81   98   94]\n",
      " [ 200 1251  175  226  127]\n",
      " [ 140  114 1674  721  279]\n",
      " [  91   75  207 3961 1284]\n",
      " [  60   43   96  833 6364]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec': 0.52552642034167663, 'word to vec 2': 0.53193285657528799})\n"
     ]
    }
   ],
   "source": [
    "xgb_w2v_mini_results = test_features(xgb_model, w2v_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec 3\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.536\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1957   61   56   79   62]\n",
      " [ 159 1382  139  203   96]\n",
      " [ 112  111 1893  590  222]\n",
      " [  79   80  182 4121 1156]\n",
      " [  55   37   99  702 6503]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec 3': 0.53570719110051646})\n"
     ]
    }
   ],
   "source": [
    "xgb_w2v_mini_results_1000 = test_features(xgb_model, w2v_feature_set_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec google\n",
      "Tuned Classifier Parameters: {'max_depth': 4, 'min_child_weight': 3}\n",
      "Tuned Classifier Accuracy: 0.534\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[1968   59   61   68   59]\n",
      " [ 140 1416  137  191   95]\n",
      " [  98  101 1791  679  259]\n",
      " [  58   61  147 4191 1161]\n",
      " [  40   32   84  645 6595]]\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec google': 0.53446563369090183})\n"
     ]
    }
   ],
   "source": [
    "xgb_w2v_mini_results_google = test_features(xgb_model, w2v_feature_set_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 26s 2ms/step - loss: 1.0842 - acc: 0.5271 - val_loss: 0.9568 - val_acc: 0.5872\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 26s 2ms/step - loss: 0.7454 - acc: 0.6868 - val_loss: 1.0529 - val_acc: 0.5549\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 24s 2ms/step - loss: 0.4013 - acc: 0.8440 - val_loss: 1.3199 - val_acc: 0.5531\n",
      "\n",
      "tfidf\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 25s 2ms/step - loss: 1.0748 - acc: 0.5164 - val_loss: 0.9524 - val_acc: 0.5817\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 22s 2ms/step - loss: 0.7629 - acc: 0.6756 - val_loss: 1.0302 - val_acc: 0.5780\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 24s 2ms/step - loss: 0.4147 - acc: 0.8419 - val_loss: 1.3780 - val_acc: 0.5431\n",
      "\n",
      "count stop\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 24s 2ms/step - loss: 1.0580 - acc: 0.5354 - val_loss: 0.9324 - val_acc: 0.5886\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 24s 2ms/step - loss: 0.7135 - acc: 0.6922 - val_loss: 1.0026 - val_acc: 0.5825\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 23s 2ms/step - loss: 0.3901 - acc: 0.8464 - val_loss: 1.3047 - val_acc: 0.5559\n",
      "\n",
      "tfidf stop\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 23s 2ms/step - loss: 1.0498 - acc: 0.5259 - val_loss: 0.9365 - val_acc: 0.5817\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 22s 2ms/step - loss: 0.7341 - acc: 0.6820 - val_loss: 0.9978 - val_acc: 0.5780\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 22s 2ms/step - loss: 0.4149 - acc: 0.8413 - val_loss: 1.3125 - val_acc: 0.5524\n",
      "\n",
      "count stop ngram\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 47s 3ms/step - loss: 1.0364 - acc: 0.5470 - val_loss: 0.9137 - val_acc: 0.6027\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 39s 3ms/step - loss: 0.6258 - acc: 0.7383 - val_loss: 1.0236 - val_acc: 0.5883\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 41s 3ms/step - loss: 0.2683 - acc: 0.9049 - val_loss: 1.5339 - val_acc: 0.5749\n",
      "\n",
      "tfidf stop ngram\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 57s 4ms/step - loss: 1.0287 - acc: 0.5392 - val_loss: 0.8986 - val_acc: 0.6050\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 53s 4ms/step - loss: 0.6405 - acc: 0.7324 - val_loss: 0.9956 - val_acc: 0.5779\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 57s 4ms/step - loss: 0.2762 - acc: 0.8971 - val_loss: 1.5197 - val_acc: 0.5537\n",
      "\n",
      "count stop ngram lda\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 61s 4ms/step - loss: 1.0291 - acc: 0.5511 - val_loss: 0.9255 - val_acc: 0.5867\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 51s 4ms/step - loss: 0.6316 - acc: 0.7338 - val_loss: 1.0713 - val_acc: 0.5799\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 53s 4ms/step - loss: 0.2720 - acc: 0.9000 - val_loss: 1.5795 - val_acc: 0.5504\n",
      "\n",
      "tfidf stop ngram nmf\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 55s 4ms/step - loss: 1.0274 - acc: 0.5342 - val_loss: 0.8931 - val_acc: 0.6002\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 54s 4ms/step - loss: 0.6554 - acc: 0.7248 - val_loss: 1.0061 - val_acc: 0.5829\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 51s 4ms/step - loss: 0.2923 - acc: 0.8937 - val_loss: 1.3842 - val_acc: 0.5635\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'count': 0.58715444463823951, 'tfidf': 0.5816917728408838, 'count stop': 0.58864426411573623, 'tfidf stop': 0.58169177298395069, 'count stop ngram': 0.60271478241947463, 'tfidf stop ngram': 0.60503227943380311, 'count stop ngram lda': 0.58665783811614047, 'tfidf stop ngram nmf': 0.60023174972017956})\n"
     ]
    }
   ],
   "source": [
    "# run tests for deep neural nets\n",
    "dnn_mini_results = dnn_test_results(non_negative_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 5s 347us/step - loss: 1.1395 - acc: 0.5022 - val_loss: 1.0685 - val_acc: 0.5304\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 4s 300us/step - loss: 1.0732 - acc: 0.5330 - val_loss: 1.0341 - val_acc: 0.5514\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 4s 303us/step - loss: 1.0510 - acc: 0.5415 - val_loss: 1.0300 - val_acc: 0.5545\n",
      "Epoch 4/30\n",
      "14095/14095 [==============================] - 4s 296us/step - loss: 1.0377 - acc: 0.5466 - val_loss: 1.0206 - val_acc: 0.5554\n",
      "Epoch 5/30\n",
      "14095/14095 [==============================] - 4s 297us/step - loss: 1.0248 - acc: 0.5516 - val_loss: 1.0177 - val_acc: 0.5572\n",
      "Epoch 6/30\n",
      "14095/14095 [==============================] - 4s 296us/step - loss: 1.0152 - acc: 0.5595 - val_loss: 1.0228 - val_acc: 0.5555\n",
      "Epoch 7/30\n",
      "14095/14095 [==============================] - 4s 298us/step - loss: 1.0026 - acc: 0.5662 - val_loss: 1.0234 - val_acc: 0.5564\n",
      "\n",
      "word to vec 2\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 5s 350us/step - loss: 1.1417 - acc: 0.5014 - val_loss: 1.0527 - val_acc: 0.5499\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 5s 333us/step - loss: 1.0727 - acc: 0.5326 - val_loss: 1.0464 - val_acc: 0.5511\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 5s 327us/step - loss: 1.0542 - acc: 0.5388 - val_loss: 1.0188 - val_acc: 0.5590\n",
      "Epoch 4/30\n",
      "14095/14095 [==============================] - 5s 321us/step - loss: 1.0355 - acc: 0.5491 - val_loss: 1.0449 - val_acc: 0.5504\n",
      "Epoch 5/30\n",
      "14095/14095 [==============================] - 5s 321us/step - loss: 1.0247 - acc: 0.5525 - val_loss: 1.0259 - val_acc: 0.5497\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec': 0.55719251775066692, 'word to vec 2': 0.55901340829716317})\n"
     ]
    }
   ],
   "source": [
    "dnn_w2v_mini_results = dnn_test_results(w2v_feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec 3\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 7s 489us/step - loss: 1.1452 - acc: 0.4959 - val_loss: 1.0608 - val_acc: 0.5408\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 7s 468us/step - loss: 1.0859 - acc: 0.5239 - val_loss: 1.0351 - val_acc: 0.5499\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 6s 459us/step - loss: 1.0603 - acc: 0.5319 - val_loss: 1.0461 - val_acc: 0.5340\n",
      "Epoch 4/30\n",
      "14095/14095 [==============================] - 6s 455us/step - loss: 1.0530 - acc: 0.5396 - val_loss: 1.0239 - val_acc: 0.5572\n",
      "Epoch 5/30\n",
      "14095/14095 [==============================] - 6s 454us/step - loss: 1.0425 - acc: 0.5472 - val_loss: 1.0306 - val_acc: 0.5562\n",
      "Epoch 6/30\n",
      "14095/14095 [==============================] - 6s 449us/step - loss: 1.0320 - acc: 0.5518 - val_loss: 1.0156 - val_acc: 0.5602\n",
      "Epoch 7/30\n",
      "14095/14095 [==============================] - 6s 451us/step - loss: 1.0206 - acc: 0.5536 - val_loss: 1.0281 - val_acc: 0.5529\n",
      "Epoch 8/30\n",
      "14095/14095 [==============================] - 6s 460us/step - loss: 1.0138 - acc: 0.5596 - val_loss: 1.0365 - val_acc: 0.5444\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec 3': 0.56017215688326094})\n"
     ]
    }
   ],
   "source": [
    "dnn_w2v_mini_results_1000 = dnn_test_results(w2v_feature_set_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to vec google\n",
      "Train on 14095 samples, validate on 6041 samples\n",
      "Epoch 1/30\n",
      "14095/14095 [==============================] - 8s 548us/step - loss: 1.1670 - acc: 0.4773 - val_loss: 1.0221 - val_acc: 0.5441\n",
      "Epoch 2/30\n",
      "14095/14095 [==============================] - 6s 435us/step - loss: 1.0394 - acc: 0.5395 - val_loss: 0.9898 - val_acc: 0.5668\n",
      "Epoch 3/30\n",
      "14095/14095 [==============================] - 6s 437us/step - loss: 1.0160 - acc: 0.5486 - val_loss: 1.0303 - val_acc: 0.5401\n",
      "Epoch 4/30\n",
      "14095/14095 [==============================] - 6s 425us/step - loss: 1.0047 - acc: 0.5590 - val_loss: 0.9900 - val_acc: 0.5698\n",
      "\n",
      "--------------------------\n",
      "defaultdict(<class 'float'>, {'word to vec google': 0.56977321631050803})\n"
     ]
    }
   ],
   "source": [
    "dnn_w2v_mini_results_google = dnn_test_results(w2v_feature_set_google)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
