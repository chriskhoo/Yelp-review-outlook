{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and determine working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import topic model \n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Get stop words \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import NLP vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import models \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get current directory\n",
    "dir = os.path.dirname(os.path.abspath('__file__'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2876509 entries, 0 to 2876508\n",
      "Data columns (total 2 columns):\n",
      "stars_review        int64\n",
      "processed_review    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load df from a csv - all text to lower case, tokenize into list of strings, remove punctuation and lemmatize\n",
    "file_path = os.path.join(dir, '02_processed_data','review_text_stars.csv')\n",
    "joint_df = pd.read_csv(file_path, index_col = False)\n",
    "joint_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To be deleted after code is tested \n",
    "joint_df = joint_df.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define dependent and independent variables\n",
    "X = joint_df.processed_review\n",
    "y = joint_df.stars_review\n",
    "\n",
    "# Create training and test sets using a fixed seed for reproducibility \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize data\n",
    "## Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scikit-learn's bag of words tool -  \"CountVectorizer\"\n",
    "# Source: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# Initial analysis using unigrams and remove all standard stopwords \n",
    "count_vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             stop_words = 'english',\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the training data (independent variables)\n",
    "count_train = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the test data (independent variables)\n",
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3841\n"
     ]
    }
   ],
   "source": [
    "# Print the length of features of the count_vectorizer\n",
    "print( len(count_vectorizer.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf vectorizer (weighted vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial analysis using unigrams and remove all standard stopwords \n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = 'english',\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the training data (independent variables)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the test data (independent variables)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900\n"
     ]
    }
   ],
   "source": [
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the problem will be modelled using classification algorithms. This decision will be revised when looking at methods to improve the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define cross_validation_tuning()\n",
    "def cross_validation_tuning(classifier, param_grid, X_trn, y_trn):\n",
    "    classifier_cv = GridSearchCV(classifier, param_grid, cv=5)\n",
    "    classifier_cv.fit(X_trn, y_trn)\n",
    "    # Print the optimal parameters and best score\n",
    "    print(\"Tuned Classifier Parameter: {}\".format(classifier_cv.best_params_))\n",
    "    print(\"Tuned Classifier Accuracy: {:.3f}\".format(classifier_cv.best_score_))\n",
    "    return classifier_cv\n",
    "\n",
    "# Define print_test_accuracy_and_cm()\n",
    "def print_test_accuracy_and_cm(classifier, X_tst, y_tst):\n",
    "    # Predict the labels\n",
    "    pred = classifier.predict(X_tst)\n",
    "    # Compute accuracy\n",
    "    score = metrics.accuracy_score(y_tst, pred)\n",
    "    # Calculate and print the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_tst, pred, labels=[1,2,3,4,5])\n",
    "    print('Test Set Score: {:.3f}'.format(score) )\n",
    "    print('For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.')\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive_bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nb_model()\n",
    "def nb_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'alpha': np.arange(0, 1, 0.1)}\n",
    "    # Iterate over the alphas and print the corresponding score\n",
    "    nb_classifier = MultinomialNB()\n",
    "    tuned_nb_classifier = cross_validation_tuning(nb_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_nb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define svm_model()\n",
    "def svm_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'kernel': ['poly', 'rbf', 'linear'], 'C': [1, 10, 100], 'gamma': [0.1, 0.01]}\n",
    "    svm_classifier = SVC()\n",
    "    tuned_svm_classifier = cross_validation_tuning(svm_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_svm_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define logreg_model()\n",
    "def logreg_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {'C': np.logspace(-5, 8, 15), 'penalty': ['l1', 'l2']}\n",
    "    logreg_classifier = LogisticRegression()\n",
    "    tuned_logreg_classifier = cross_validation_tuning(logreg_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_logreg_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define ranforest_model()\n",
    "def ranforest_model(X_trn, y_trn):\n",
    "    # Create parameters\n",
    "    param_grid = {\"n_estimators\": [2, 10, 100, 300, 1000],\n",
    "                  \"max_depth\": [2, 10, 100, 300],\n",
    "                  \"min_samples_split\": [2, 10, 100],\n",
    "                  \"min_samples_leaf\": [1, 10, 100]}\n",
    "    ranforest_classifier = RandomForestClassifier()\n",
    "    tuned_ranforest_classifier = cross_validation_tuning(ranforest_classifier, param_grid, X_trn, y_trn)\n",
    "    return tuned_ranforest_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'alpha': 0.40000000000000002}\n",
      "Tuned Classifier Accuracy: 0.586\n",
      "Test Set Score: 0.586\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 66497  19724   5027   1967   2305]\n",
      " [ 23496  29399  22557   5720   3117]\n",
      " [ 11201  16922  48895  35138  10560]\n",
      " [  6065   6549  27411 111408  89006]\n",
      " [  5884   2797   5903  55688 249717]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned nb_model for count vectorizer\n",
    "tuned_nb_classifier = nb_model(count_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_nb_classifier, count_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tuned svm_model for count vectorizer\n",
    "tuned_svm_classifier = svm_model(count_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_svm_classifier, count_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'C': 31.622776601683793, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.609\n",
      "Test Set Score: 0.610\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 73334   9045   3415   3723   6003]\n",
      " [ 25190  22998  17848  11241   7012]\n",
      " [  7322  10560  39826  47491  17517]\n",
      " [  1960   1604  12401 116037 108437]\n",
      " [  1176    423   1761  42120 274509]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned logreg_model for count vectorizer\n",
    "tuned_logreg_classifier = logreg_model(count_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_logreg_classifier, count_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest for count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tuned random forest_model for count vectorizer\n",
    "tuned_ranforest_classifier = ranforest_model(count_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_ranforest_classifier, count_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'alpha': 0.10000000000000001}\n",
      "Tuned Classifier Accuracy: 0.538\n",
      "Test Set Score: 0.550\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[113  42  23  27  33]\n",
      " [ 62  66  61  77  28]\n",
      " [ 12  27  90 193  72]\n",
      " [  9   4  46 393 392]\n",
      " [  4   2  19 218 987]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned nb_model for tfidf vectorizer\n",
    "tuned_nb_classifier = nb_model(tfidf_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_nb_classifier, tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.568\n",
      "Test Set Score: 0.582\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[123  68  17  20  10]\n",
      " [ 63 104  70  40  17]\n",
      " [ 12  55 141 143  43]\n",
      " [  5  17  67 433 322]\n",
      " [ 11   6  26 241 946]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned svm_model for tfidf vectorizer\n",
    "tuned_svm_classifier = svm_model(tfidf_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_svm_classifier, tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'C': 3.7275937203149381, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.557\n",
      "Test Set Score: 0.568\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[121  53  21  23  20]\n",
      " [ 56  97  64  54  23]\n",
      " [  9  39 116 170  60]\n",
      " [  7   9  66 401 361]\n",
      " [  9   5  13 233 970]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned logreg_model for tfidf vectorizer\n",
    "tuned_logreg_classifier = logreg_model(tfidf_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_logreg_classifier, tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest for tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'max_depth': 300, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Tuned Classifier Accuracy: 0.511\n",
      "Test Set Score: 0.520\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[  87   13   12   52   74]\n",
      " [  26   13   49  116   90]\n",
      " [   3    6   39  213  133]\n",
      " [   3    0   16  345  480]\n",
      " [   4    0    5  146 1075]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned random forest_model for tfidf vectorizer\n",
    "tuned_ranforest_classifier = ranforest_model(tfidf_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_ranforest_classifier, tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enchancing the feature engineering\n",
    "Since the best performing models appear to be emerging from the tfidf vectorizer, we will extend the model to include n-gram analysis, and a refined list of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chriskhoo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', \"should've\", 'been', 'as', 'myself', 'yourself', 'does', 'only', 'any', 'them', 'some', 'both', 'our', 'did', 'nor', 'own', 'into', 'do', 'him', 'these', \"hadn't\", 'because', 'theirs', 'further', 'm', 'service', 'she', \"you'll\", 'very', 'few', 'me', 'for', 're', 'in', 'while', 'o', 'so', \"haven't\", 'food', 'an', 'its', 'll', 'before', 'from', 'how', 'that', 'just', 'lunch', \"she's\", 'his', \"doesn't\", \"mightn't\", 'of', 'yourselves', 'having', 'i', 'my', 'a', 'are', 'at', 'it', 'hers', 've', 'ours', 'too', \"won't\", 'you', 'y', 'have', 'has', 'd', 'yours', \"mustn't\", 'dinner', 'himself', \"don't\", 'there', \"wouldn't\", 'here', 'is', 'whom', \"wasn't\", 'such', 'doing', 'the', 'when', 'each', 'had', 'but', 'why', \"aren't\", \"weren't\", 'or', 'through', 'herself', \"shouldn't\", 'where', 'now', 'bar', 'on', 'over', 'if', 'with', 'who', \"that'll\", 'after', \"needn't\", 'her', 'by', \"shan't\", 'once', 'their', 'can', 'place', 'will', \"didn't\", 'ordered', 'between', 'other', 'those', 'this', \"you'd\", 'about', 'itself', 'during', \"isn't\", 'ma', 'what', \"you're\", 'than', 'they', 'ourselves', 'same', \"hasn't\", 'should', 's', \"you've\", 'then', 'until', 'all', 'we', 'breakfast', 'be', 'he', 'restaurant', 'order', \"it's\", 'which', 'was', 'most', 'being', \"couldn't\", 'am', 'were', 'your', 'and', 'themselves']\n"
     ]
    }
   ],
   "source": [
    "# Add neutral words related to restaurants to list of stop words\n",
    "stopWords.update(['restaurant', 'place', 'bar', 'service', 'food', 'lunch', 'breakfast', 'dinner', 'price', 'order', 'ordered'])\n",
    "\n",
    "# Remove stopwords that might reflect sentiment\n",
    "stopWords = [word for word in stopWords if word not in ['above', 'not', 'below', 't', 'off', 'no', 'again', 'against', 'under', 'hadn', 'up', 'shan', 'more', 'hasn', 'won','couldn', 'wasn', 'mustn', 'out', 'don','down', 'haven', 'price', 'mightn', 'isn', 'wouldn', 'needn', 'shouldn', 'weren', 'aren', 'didn', 'ain', 'doesn']]\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text using unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial analysis using bigrams and unigrams and remove all standard stopwords \n",
    "tfidf_vectorizer_ngrams = TfidfVectorizer(analyzer = 'word',\n",
    "                             stop_words = stopWords,\n",
    "                             ngram_range = (1,3),\n",
    "                             max_df=0.95, \n",
    "                             min_df=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the training data (independent variables)\n",
    "tfidf_ngrams_train = tfidf_vectorizer_ngrams.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the test data (independent variables)\n",
    "tfidf_ngrams_test = tfidf_vectorizer_ngrams.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9994\n"
     ]
    }
   ],
   "source": [
    "# Print the length of features of the tfidf_vectorizer\n",
    "print( len(tfidf_vectorizer_ngrams.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling using Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF\n",
    "nmf = NMF(n_components=300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data (independent variables)\n",
    "nmf_train = nmf.fit_transform(tfidf_ngrams_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the test data (independent variables)\n",
    "nmf_test = nmf.transform(tfidf_ngrams_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "wa wa not wa delicious wa friendly wa amazing thought wa really wa nice wa wa staff wa\n",
      "Topic 1:\n",
      "taco taco wa taco shop shop fish taco best taco gourmet gourmet taco taco salsa taco good\n",
      "Topic 2:\n",
      "castle white castle white strip castle burger white castle burger white castle wa castle wa casino cheeseburger\n",
      "Topic 3:\n",
      "always always good always fresh always friendly always great staff always always get good always time always usually\n",
      "Topic 4:\n",
      "pizza pizza wa great pizza best pizza pizza pizza favorite pizza spinatos pepperoni style pizza pizza ever\n",
      "Topic 5:\n",
      "sandwich sandwich wa coleslaw fry sandwich primantis chicken sandwich pastrami turkey beef sandwich fry coleslaw\n",
      "Topic 6:\n",
      "beer tap craft craft beer beer tap list duckworths good beer beer selection beer wa\n",
      "Topic 7:\n",
      "sushi best sushi sushi bong bong good sushi sushi wa byob sushi ive great sushi best sushi ive\n",
      "Topic 8:\n",
      "dog hot dog retro retro dog dog wa coney chicago dog root beer root car\n",
      "Topic 9:\n",
      "ice cream ice cream scoop two scoop two chocolate cone ice cream shop cream shop\n",
      "Topic 10:\n",
      "good not good good good good great good price good not also good pretty good great good always good\n",
      "Topic 11:\n",
      "cool wa cool delish really cool cool atmosphere packed super cool decor pretty cool favs\n",
      "Topic 12:\n",
      "wa good good wa good wa wa good wa good wa good wa good not good not wa good nothing overall\n",
      "Topic 13:\n",
      "burger burger wa burger fry mini best burger castle burger white castle burger burger joint good burger great burger\n",
      "Topic 14:\n",
      "ive ive ever best ive ive never time ive best ive ever seen ive tried eaten ive seen\n",
      "Topic 15:\n",
      "great great salsa great little great taco really great great beer good great not great always great also great\n",
      "Topic 16:\n",
      "vegan vegan pizza vegan option brownie vegan brownie vegan bite non vegan treat dairy baked\n",
      "Topic 17:\n",
      "highly recommend highly recommend would highly would highly recommend delicious highly el chullo chullo highly recommend anyone delicious great\n",
      "Topic 18:\n",
      "first first time wa first wa first time time first time wa first time trying time trying first time ever trying\n",
      "Topic 19:\n",
      "wait worth wait wait staff long wait wait time wait wa minute wait wait long expect wait wait table\n",
      "Topic 20:\n",
      "vega la vega la vega strip vega wa que strip location vega la vega strip best pizza\n",
      "Topic 21:\n",
      "staff friendly staff staff wa staff friendly great staff wait staff staff great staff wa friendly staff always friendly staff great\n",
      "Topic 22:\n",
      "de le et pour un est pa en que la\n",
      "Topic 23:\n",
      "go go back definitely go not go would go dont go go again definitely go back go out time go\n",
      "Topic 24:\n",
      "salsa strawberry cilantro salsa wa strawberry salsa pecan great salsa pecan salsa peanut cilantro salsa\n",
      "Topic 25:\n",
      "love love love love love love love salsa great love love pizza love always also love absolutely love time love\n",
      "Topic 26:\n",
      "really really nice really like really really wa really liked really liked really great really enjoyed really enjoy\n",
      "Topic 27:\n",
      "not wa not not good good not not sure not great im not would not not much wa not good\n",
      "Topic 28:\n",
      "roll dynamite dynamite roll roll wa spider spider roll bong sushi roll roll dynamite roll huge\n",
      "Topic 29:\n",
      "fresh always fresh fresh delicious wa fresh ingredient fresh mint mint good fresh fresh ingredient not fresh\n",
      "Topic 30:\n",
      "amazing wa amazing amazing salsa amazing amazing amazing taco salsa amazing amazing wa absolutely amazing taco amazing amazing great\n",
      "Topic 31:\n",
      "price reasonable reasonable price great price good price price reasonable price wa price good high price great\n",
      "Topic 32:\n",
      "would would not would definitely would recommend would go thought would like would come thought would would never\n",
      "Topic 33:\n",
      "location location wa great location original location ha mooresville original location henderson good location strip\n",
      "Topic 34:\n",
      "chicken chicken wa fried chicken chicken ring chicken sandwich korean breast ring piece chicken pita\n",
      "Topic 35:\n",
      "poutine montreal gravy curd poutine wa cheese curd patati poutine montreal best poutine tiny\n",
      "Topic 36:\n",
      "tea cafe pot te te cafe cup study iced selection tea leaf\n",
      "Topic 37:\n",
      "coffee coffee cake cake cracked scramble cracked egg good coffee omelet skillet coffee wa\n",
      "Topic 38:\n",
      "ramen kenzo tonkotsu broth king ramen wa tonkotsu ramen kenzo ramen best ramen broth wa\n",
      "Topic 39:\n",
      "got also got finally got time got got wa husband got got burger good got looked friend got\n",
      "Topic 40:\n",
      "out world came out try out come out out world hang hang out out wa wa out\n",
      "Topic 41:\n",
      "delicious wa delicious delicious wa wa delicious wa fresh delicious delicious great delicious also absolutely delicious delicious staff delicious salsa\n",
      "Topic 42:\n",
      "best one best best ive best sushi wa best best part not best best salsa ha best best mexican\n",
      "Topic 43:\n",
      "chicago chicago dog dog chicago style italian beef vienna style windy city windy beef sandwich\n",
      "Topic 44:\n",
      "hand hand down hand down best down best down best taco down favorite hand down favorite pastry best\n",
      "Topic 45:\n",
      "garlic knot garlic knot garlic fry vegan garlic knot vegan garlic delivery meatball pizza company company\n",
      "Topic 46:\n",
      "one one best one wa wa one one favorite no one two one thing better one good one\n",
      "Topic 47:\n",
      "come come back definitely come come out definitely come back would come would come back come again wait come wait come back\n",
      "Topic 48:\n",
      "nice wa nice really nice decor touch nice friendly nice touch super nice nice people good nice\n",
      "Topic 49:\n",
      "try try out give try must try back try try again decided wanted try decided try salsa try\n",
      "Topic 50:\n",
      "pretty pretty good wa pretty wa pretty good pretty much pretty good wa pretty busy pretty good not good wa pretty decent\n",
      "Topic 51:\n",
      "sauce sauce wa hot sauce rib tomato williams hot sauce williams sauce williams tomato sauce red sauce\n",
      "Topic 52:\n",
      "happy happy hour wa happy im happy hour menu happy hour menu everyone great happy deal make happy\n",
      "Topic 53:\n",
      "peruvian peru el ceviche el chullo chullo best peruvian authentic peruvian mom run\n",
      "Topic 54:\n",
      "excellent wa excellent excellent wa excellent selection excellent great excellent beer excellent customer excellent staff taco excellent return\n",
      "Topic 55:\n",
      "night last last night friday saturday saturday night friday night date date night night wa\n",
      "Topic 56:\n",
      "hour happy hour cocktail 24 hour 24 open closed later markham station open 24\n",
      "Topic 57:\n",
      "awesome wa awesome awesome great salsa awesome awesome salsa rock forget small wait taco awesome staff awesome\n",
      "Topic 58:\n",
      "gluten free gluten free gluten free option free option treat gf cooky option vegan bite\n",
      "Topic 59:\n",
      "hot hot dog hot sauce wa hot good hot best hot dog best hot hot wa fresh hot mom\n",
      "Topic 60:\n",
      "gem hidden hidden gem little gem great taco great price delivery neighborhood lucky make feel\n",
      "Topic 61:\n",
      "eat could eat eat again never eat would eat not eat eat wa eating bite eat dont eat\n",
      "Topic 62:\n",
      "egg benedict cracked cracked egg egg benedict bacon pancake toast egg roll sausage\n",
      "Topic 63:\n",
      "menu menu wa interesting everything menu extensive menu great choice limited menu good menu not\n",
      "Topic 64:\n",
      "ha ha best ha great ha always ha become become salsa ha ha good great ha ha lot\n",
      "Topic 65:\n",
      "asada carne carne asada asada taco carne asada taco taco taco carne asada taco carne carnitas asada wa\n",
      "Topic 66:\n",
      "no no one wa no idea no idea no complaint complaint no flavor problem no menu\n",
      "Topic 67:\n",
      "up pick pick up fill fill up call grew up grew make up picked up\n",
      "Topic 68:\n",
      "fry cheese fry burger fry coleslaw french fry fry sandwich soggy fry wa fry good chili\n",
      "Topic 69:\n",
      "small wa small small great small not small portion piece little small small piece really small room\n",
      "Topic 70:\n",
      "well well worth wa well good well done well priced well done well back well wa prepared\n",
      "Topic 71:\n",
      "wrong go wrong cant go cant go wrong cant go anything menu nigiri greek price great\n",
      "Topic 72:\n",
      "mexican best mexican authentic mexican mexican ive great mexican best mexican ive mexican ive ever mexican phoenix favorite mexican good mexican\n",
      "Topic 73:\n",
      "fast fast friendly wa fast good fast great fast fast wa wa fast friendly out fast fast burger not fast\n",
      "Topic 74:\n",
      "salad salad wa dressing caesar caesar salad pizza salad side salad chicken salad grilled chicken grilled\n",
      "Topic 75:\n",
      "also wa also also good also great good also wa also good also got also love also tried delicious also\n",
      "Topic 76:\n",
      "server server wa bill ask water server wa friendly attentive rude great server took\n",
      "Topic 77:\n",
      "atmosphere great atmosphere good atmosphere atmosphere great atmosphere friendly atmosphere wa nice atmosphere cool atmosphere pleasant casual\n",
      "Topic 78:\n",
      "line long line line wa door out door long wa long line out line up line out door\n",
      "Topic 79:\n",
      "down sit sit down down street sat down thats let sat head let down\n",
      "Topic 80:\n",
      "definitely definitely back would definitely definitely recommend definitely go definitely worth definitely come definitely not definitely return definitely come back\n",
      "Topic 81:\n",
      "more wa more more like little more ask much more much often more often one more\n",
      "Topic 82:\n",
      "burrito burrito wa best burrito tortilla taco burrito chicken burrito asada burrito carne asada burrito wa best bean\n",
      "Topic 83:\n",
      "never ive never would never never go never again never bad never seen never tried time never seen\n",
      "Topic 84:\n",
      "even though even though even better not even though wa didnt even dont even even though wa great even\n",
      "Topic 85:\n",
      "drink cocktail drink wa great drink good drink refill drink good drink menu pub patio\n",
      "Topic 86:\n",
      "taste taste like taste good taste wa taste bud bud mcdonalds taste great taste better didnt taste\n",
      "Topic 87:\n",
      "again back again again again back again again go again try again come back again come again eat again coming back again\n",
      "Topic 88:\n",
      "everything everything wa everything menu everything wa good love everything everything else delicious everything everything fresh everything ive everything wa great\n",
      "Topic 89:\n",
      "flavor unique flavor wa great flavor no flavor unique flavor variety flavor much flavor more flavor lacked\n",
      "Topic 90:\n",
      "mole mole taco chicken mole black chicken mole taco black mole mole chicken oaxaca mole wa oaxacan\n",
      "Topic 91:\n",
      "favorite one favorite new favorite favorite spot favorite pizza favorite mexican favorite thing favorite wa not favorite favorite taco\n",
      "Topic 92:\n",
      "super super friendly wa super staff super super nice wa super friendly super good staff super friendly super fresh great super\n",
      "Topic 93:\n",
      "fried fried chicken rib fried rice deep fried deep korean chicken fried cleveland hsw\n",
      "Topic 94:\n",
      "rice bean rice bean rice wa fried rice bean rice green black bean white rice green bean\n",
      "Topic 95:\n",
      "special nothing nothing special good nothing wa nothing ok nothing good nothing special wa good nothing ok nothing special meh\n",
      "Topic 96:\n",
      "onion ring onion ring onion chip chicken ring grilled onion like onion grilled cheeseburger pickle\n",
      "Topic 97:\n",
      "bad not bad wa bad really bad bad not wasnt bad never bad bad experience bad thing isnt bad\n",
      "Topic 98:\n",
      "priced reasonably reasonably priced good reasonably priced good reasonably variety well priced polite taste good prompt\n",
      "Topic 99:\n",
      "every every time time every time go time go every time come time come go every every day every single\n",
      "Topic 100:\n",
      "selection beer selection great beer selection great beer selection beer great selection selection great craft beer craft selection wa\n",
      "Topic 101:\n",
      "friend friend wa friend family friend got group friend visiting came friend reminded said recommend friend\n",
      "Topic 102:\n",
      "pibil cochinita cochinita pibil mayan mayan cochinita mayan cochinita pibil pibil taco cochinita pibil taco taco condesa\n",
      "Topic 103:\n",
      "worth well worth definitely worth worth wait not worth wa worth worth try totally worth worth trip trip\n",
      "Topic 104:\n",
      "primanti bros primanti bros primanti bros sandwich bros sandwich coleslaw fried egg sandwich bros wa primanti bros wa\n",
      "Topic 105:\n",
      "must must try must go must visit must say must stop energy wa must solid pastrami\n",
      "Topic 106:\n",
      "coming coming back keep keep coming keep coming back back definitely coming definitely coming back coming back again mahi\n",
      "Topic 107:\n",
      "quality high high quality good quality ingredient quality ingredient great quality quality wa value quantity\n",
      "Topic 108:\n",
      "authentic authentic mexican great authentic authentic italian authentic peruvian chinese good authentic mexico authentic japanese look\n",
      "Topic 109:\n",
      "clean wa clean clean wa clean staff restroom nice clean exceptional super clean bathroom clean friendly\n",
      "Topic 110:\n",
      "spot hit hit spot great spot favorite spot little spot good spot go spot wa spot job\n",
      "Topic 111:\n",
      "steak steak wa cheese steak frites steak frites steak sandwich cooked steak taco steak egg fried steak\n",
      "Topic 112:\n",
      "ever ive ever worst best ive ever life ever eaten mexican ive ever worst ever ever seen burger ever\n",
      "Topic 113:\n",
      "new new favorite york new york try new out new new location something new new spot new one\n",
      "Topic 114:\n",
      "year year ago old year old ago last year last lived many year havent\n",
      "Topic 115:\n",
      "huge wa huge fan huge fan portion huge gyro huge portion not huge huge selection roll huge\n",
      "Topic 116:\n",
      "takeout bong sushi bong accept usually space dynamite limited doesnt roll huge\n",
      "Topic 117:\n",
      "harold harold kumar kumar movie watching harold kumar watching harold crap watching harold kumar go kumar go\n",
      "Topic 118:\n",
      "visit first visit visit wa recent recent visit must visit last visit wa first visit visit again last\n",
      "Topic 119:\n",
      "wife consistently wife got consistently good wife wa birthday complaint dump new spot enjoy\n",
      "Topic 120:\n",
      "loved loved salsa loved wa delicious loved loved taco decor absolutely loved classic boyfriend husband loved\n",
      "Topic 121:\n",
      "shrimp shrimp taco cabo cabo shrimp cabo shrimp taco cocktail shrimp cocktail shrimp wa tempura shrimp tempura\n",
      "Topic 122:\n",
      "slaw cole slaw cole fry cole slaw fry cole slaw wa slaw fry fry french fry cole cole slaw fry\n",
      "Topic 123:\n",
      "phoenix phoenix area sushi phoenix mexican phoenix visiting visiting phoenix taco phoenix number best sushi phoenix best sushi\n",
      "Topic 124:\n",
      "went time went went back went wa another went down renovated soon nicely went friday\n",
      "Topic 125:\n",
      "tasty wa tasty fresh tasty really tasty tasty not pretty tasty tasty well quite tasty tasty also tasty wa\n",
      "Topic 126:\n",
      "different different salsa something different something variety kind many different different kind different type different taco\n",
      "Topic 127:\n",
      "ok wa ok ok wa ok nothing ok not bland wa ok not wa ok nothing cashier run out\n",
      "Topic 128:\n",
      "option vegetarian vegetarian option vegan vegetarian vegan option vegetarian vegan option well lot option salsa option option menu\n",
      "Topic 129:\n",
      "family owned family owned family friendly business friend family whole family family run family love great family\n",
      "Topic 130:\n",
      "fish fish taco fish wa fresh fish fish chip fish sandwich fish burger fried fish freshest fish slider\n",
      "Topic 131:\n",
      "fantastic wa fantastic fantastic wa environment absolutely fantastic dessert mango salsa fantastic try one rock\n",
      "Topic 132:\n",
      "pittsburgh pastrami classic primantis pittsburgh classic tradition staple go pittsburgh true time pittsburgh\n",
      "Topic 133:\n",
      "customer manager customer wa poor poor customer rude employee great customer good customer excellent customer\n",
      "Topic 134:\n",
      "tried ive tried never tried also tried weve weve tried tried out havent tried wa phone\n",
      "Topic 135:\n",
      "meal meal wa part meal end meal part great meal enjoyed meal bad meal best part meal good meal\n",
      "Topic 136:\n",
      "horchata horchata wa fruit best horchata pecan strawberry nut cinnamon salsa horchata horchata ever\n",
      "Topic 137:\n",
      "going going back not going im going not going back wa going definitely going definitely going back recommend going going get\n",
      "Topic 138:\n",
      "lot parking parking lot variety lot variety ha lot lot option lot flavor wa lot lot people\n",
      "Topic 139:\n",
      "many many time many people cooking salsa many many year many many not many choose many option\n",
      "Topic 140:\n",
      "quick wa quick bite quick bite friendly quick good quick quick cheap quick friendly quick wa out quick\n",
      "Topic 141:\n",
      "decent wa decent decent price expect decent not pretty decent good decent decent wa overrated price\n",
      "Topic 142:\n",
      "cake chocolate cupcake cooky chocolate cake bakery cake wa cake pop frosting birthday\n",
      "Topic 143:\n",
      "give give try would give another give another give star shot decided give chance cook\n",
      "Topic 144:\n",
      "pork belly pork belly mac mac cheese belly mac pork belly mac pulled pulled pork cooper\n",
      "Topic 145:\n",
      "could could not could eat use could use tell wish could see could see could tell\n",
      "Topic 146:\n",
      "beef beef sandwich italian beef beef wa roast mongolian mongolian beef roast beef corned corned beef\n",
      "Topic 147:\n",
      "coast east east coast west coast west chain fulfill machine vega hungry\n",
      "Topic 148:\n",
      "want dont want want try not want go want want eat want good want go want more want get\n",
      "Topic 149:\n",
      "pasta brioni pasta brioni pasta wa spaghetti penne meatball vodka scottsdale calamari\n",
      "Topic 150:\n",
      "frozen frozen one store grocery store grocery buy exactly taste exactly like frozen one buy\n",
      "Topic 151:\n",
      "get always get good get pay get out usually get get table usually time get get better\n",
      "Topic 152:\n",
      "portion size portion size generous generous portion good portion large portion portion huge casual small portion\n",
      "Topic 153:\n",
      "wa great great wa great wa wa great wa salsa wa salsa wa great return complaint good wa great\n",
      "Topic 154:\n",
      "say say wa would say let must say let say least cant say wow say wow\n",
      "Topic 155:\n",
      "quesadilla quesadilla wa deep chicken quesadilla deep fried fried quesadilla margarita tortilla corn taco quesadilla\n",
      "Topic 156:\n",
      "thing thing wa good thing one thing great thing best thing heard thing menu first thing whole\n",
      "Topic 157:\n",
      "looking forward looking forward trying forward trying wa looking youre looking looking forward trying looking good looking something\n",
      "Topic 158:\n",
      "cold wa cold sub tasted beer wa rush ice cold disappointment microwave dosas\n",
      "Topic 159:\n",
      "made wa made feel made feel mistake made up made fresh made mistake home made made great\n",
      "Topic 160:\n",
      "real real deal deal real italian freshly wa real real mexican theyre cut not real\n",
      "Topic 161:\n",
      "spicy tuna spicy tuna tuna roll spicy tuna roll like spicy tuna wa squid roll spicy spicy salmon\n",
      "Topic 162:\n",
      "busy wa busy always busy get busy not busy busy time busy wa weekend pretty busy busy weekend\n",
      "Topic 163:\n",
      "noodle chinese bowl broth chow fried noodle noodle soup hk duck noodle dish\n",
      "Topic 164:\n",
      "bread bread wa piece thick cheesy white bread toasted garlic bread italian bread coleslaw\n",
      "Topic 165:\n",
      "town out town town great best town im town across town wa town pizza town taco town sushi town\n",
      "Topic 166:\n",
      "cheap quick cheap good cheap cheap price cheap sushi cheap delicious not cheap price cheap pretty cheap bong\n",
      "Topic 167:\n",
      "indian dosa south south indian sambar dosas chutney thali masala buffet\n",
      "Topic 168:\n",
      "hole wall hole wall tiny little hole little hole wall salsa choose choose mom pop wall wa\n",
      "Topic 169:\n",
      "youre youre looking youre not youre going getting mood youll hungry great youre youre getting\n",
      "Topic 170:\n",
      "japanese authentic japanese japanese sushi pricey japan tempura limited sushi toronto miso\n",
      "Topic 171:\n",
      "home write home write home fry back home nothing write nothing write home take home took close home\n",
      "Topic 172:\n",
      "right wa right two right away right amount price right amount get right right next right down\n",
      "Topic 173:\n",
      "sweet potato sweet potato sweet potato fry potato fry skillet potato hash wa sweet hash brisket\n",
      "Topic 174:\n",
      "side two west side west extra side wa side salad side dish side town served side\n",
      "Topic 175:\n",
      "truck mac sloppi chile jo jos sloppi jos mac cheese green sloppi jo\n",
      "Topic 176:\n",
      "minute 10 10 minute 30 minute 30 20 20 minute 15 waited took\n",
      "Topic 177:\n",
      "crust thin thin crust crust pizza spinatos thin crust pizza pizza thick crust crust wa pizza town\n",
      "Topic 178:\n",
      "drive thru drive thru window car long line employee worth drive jack jack box\n",
      "Topic 179:\n",
      "terrible wa terrible management high rude dough terrible experience lo gave burnt\n",
      "Topic 180:\n",
      "polish boy polish boy boy wa cleveland polish boy wa hot sauce williams polish girl wanted\n",
      "Topic 181:\n",
      "french french fry toast french toast french fry sandwich peanut pancake stuffed peanut butter always packed\n",
      "Topic 182:\n",
      "little wa little great little little bit little more little slow cute little gem little pricey little small\n",
      "Topic 183:\n",
      "italian italian beef old authentic italian old school school favorite italian best italian italian beef sandwich real italian\n",
      "Topic 184:\n",
      "tv game sport watch would expect expect watch game show great would plenty\n",
      "Topic 185:\n",
      "think dont think think wa bug think would much junk thought might twice\n",
      "Topic 186:\n",
      "way out way go out way go out wa way good way way better no way way get way much\n",
      "Topic 187:\n",
      "large group large group large portion large selection not large wa large pack party great late\n",
      "Topic 188:\n",
      "thai pad thai pad mint fresh mint vegetable curry tofu vegetarian peanut\n",
      "Topic 189:\n",
      "disappointed not disappointed wa disappointed disappointed wa wont disappointed wa disappointed wa wa not disappointed wa not never disappointed expensive\n",
      "Topic 190:\n",
      "box bento bento box bowl jack jack box teriyaki miso soup tempura miso\n",
      "Topic 191:\n",
      "wont probably probably wont wont back wont go probably best regret wont regret wont go back wont disappointed\n",
      "Topic 192:\n",
      "take take out great take great take out good take party get take out get take out value\n",
      "Topic 193:\n",
      "came came out quickly came back out quickly came out quickly later medium overall came wa\n",
      "Topic 194:\n",
      "next next time next time im time time im ill back next back next time next door time ill\n",
      "Topic 195:\n",
      "still wa still still great still good although used theyre im still theyre still still pretty\n",
      "Topic 196:\n",
      "music live enjoy jazz loud live music conversation terrific play band\n",
      "Topic 197:\n",
      "pepper red mushroom green chili sausage red pepper roasted banana onion\n",
      "Topic 198:\n",
      "off first off strip top off 10 wa off rip off rip off time off bill\n",
      "Topic 199:\n",
      "im im not im not sure fan sure not sure time im im going not fan im not fan\n",
      "Topic 200:\n",
      "owner wonderful owner wa great owner homemade healthy business owner super staff owner company\n",
      "Topic 201:\n",
      "wish wish wa closer wish could wa closer wish would lived more often often wish more\n",
      "Topic 202:\n",
      "fun kind great salsa lot fun fun atmosphere wa fun salsa fun bring really fun always good\n",
      "Topic 203:\n",
      "dont know dont know dont like let dont go dont let dont miss mind miss\n",
      "Topic 204:\n",
      "didnt didnt like didnt get didnt even didnt know know care didnt really wanted didnt care\n",
      "Topic 205:\n",
      "area around dining phoenix area dining area area wa area great around area seating area area not\n",
      "Topic 206:\n",
      "found yelp found yelp found out thanks finally found found little glad found found wa glad\n",
      "Topic 207:\n",
      "street street taco across across street parking down street rock pastor street pastor street taco 16th\n",
      "Topic 208:\n",
      "outside inside seating outside seating outdoor look sit outdoor seating patio inside wa\n",
      "Topic 209:\n",
      "enough not enough good enough get enough more enough wa enough say enough big enough enough time split\n",
      "Topic 210:\n",
      "star reason five five star get star give star star wa gave one star giving\n",
      "Topic 211:\n",
      "recommend recommend anyone would recommend anyone definitely recommend would definitely recommend would recommend anyone not recommend wouldnt recommend recommend going\n",
      "Topic 212:\n",
      "slider cheese slider slider fry slider wa strip fish slider chicken slider combo pickle casino\n",
      "Topic 213:\n",
      "starbucks penn william william penn hotel lobby omni william penn hotel penn hotel omni william\n",
      "Topic 214:\n",
      "away right away blown blown away seated seated right away seated right wa blown away wa blown away wa\n",
      "Topic 215:\n",
      "dish dish wa tofu atlantic unique dish not side dish dessert favorite dish rich\n",
      "Topic 216:\n",
      "veggie veggie taco veggie dog eater veggie option grilled vegetarian benny pita burro\n",
      "Topic 217:\n",
      "absolutely wa absolutely absolutely delicious absolutely amazing absolutely love fabulous wa absolutely delicious absolutely loved pleasant tapioca\n",
      "Topic 218:\n",
      "big party fan big deal not big big fan deal wa big pretty big big portion\n",
      "Topic 219:\n",
      "better much much better even better wa better better burger get better way better taste better better one\n",
      "Topic 220:\n",
      "enjoyed really enjoyed enjoyed meal thoroughly enjoyed wa thoroughly enjoyed evening certainly emphasis also enjoyed\n",
      "Topic 221:\n",
      "la santisima la santisima condesa la condesa los gourmet strong variety incredible\n",
      "Topic 222:\n",
      "bartender bartender wa sat attentive private polite window recommendation wa attentive male\n",
      "Topic 223:\n",
      "half burro coleslaw mahi eating ate half ate spinach spinatos half off\n",
      "Topic 224:\n",
      "husband time husband wonderful yum husband got husband went good husband husband wa cleveland sub\n",
      "Topic 225:\n",
      "however however wa good however poor wa good however hard wa poor available dry reason\n",
      "Topic 226:\n",
      "patty bun jerk ketchup jerk chicken patty wa hamburger soft steamed onion\n",
      "Topic 227:\n",
      "meat meat wa meat eater eater soy meat cheese dry little meat overall soggy\n",
      "Topic 228:\n",
      "make sure make sure feel make feel not sure feel like make up make feel like sure try\n",
      "Topic 229:\n",
      "shark shark taco taco pecan dogfish dogfish shark dogfish shark taco shark taco wa pecan salsa taco wa\n",
      "Topic 230:\n",
      "bit wa bit little bit bit pricey pricey quite bit bit slow bit bland although wait bit\n",
      "Topic 231:\n",
      "since since wa opened ever since since opened time since since moving moving year since ago\n",
      "Topic 232:\n",
      "time time wa last time last second time second several time time time last time wa several\n",
      "Topic 233:\n",
      "need need go need more need get need try no need wonderful go pizza visiting really need\n",
      "Topic 234:\n",
      "plate taco plate plate wa paper utensil yum plastic souvlaki small plate sized\n",
      "Topic 235:\n",
      "table table wa 17 table 17 waiting get table empty party sit one table\n",
      "Topic 236:\n",
      "local business great local local business support neighborhood support local feel support local business local beer\n",
      "Topic 237:\n",
      "find hard hard find find out find good find something hard find good something find seat out much\n",
      "Topic 238:\n",
      "cheese grilled grilled cheese cheese wa mac cheese fry mac cheese cheese curd curd bacon\n",
      "Topic 239:\n",
      "served wa served serving served hot brought done served side served wa served fresh brought out\n",
      "Topic 240:\n",
      "really good wa really good wa really really good really good wa good wa thought wa really really really good taco really\n",
      "Topic 241:\n",
      "bbq improper pulled pig improper pig bbq sauce pulled pork pork brisket rib\n",
      "Topic 242:\n",
      "kid kid friendly kid menu mom adult child daughter pepperoni son floor\n",
      "Topic 243:\n",
      "brunch cooper sunday morning saturday benedict sunday morning soccer tavern table 17\n",
      "Topic 244:\n",
      "hana byob uni sake sushi lori tempura rock sashimi chef\n",
      "Topic 245:\n",
      "anything else anything else somewhere else somewhere else wa everything else see something havent\n",
      "Topic 246:\n",
      "id id go id recommend id go back rather id say id rather expensive id come ridiculously\n",
      "Topic 247:\n",
      "stop must stop stop try stop wa decided stop couldnt stop really interesting stop eating neighborhood ill\n",
      "Topic 248:\n",
      "item menu item item menu item wa try item back try popular one item truffle various\n",
      "Topic 249:\n",
      "excited wa excited excited try extremely wa excited try wa extremely dry seemed another maybe\n",
      "Topic 250:\n",
      "outstanding wa outstanding outstanding great hotdog employee choice delish fine veganbites huevos\n",
      "Topic 251:\n",
      "wasabi blue blue wasabi sushi scottsdale martini fresh wasabi roll happy hour berry\n",
      "Topic 252:\n",
      "quite wa quite quite good quite tasty quite bit weekend not quite quite often wa quite good crowded\n",
      "Topic 253:\n",
      "wasnt impressed wasnt bad wasnt impressed either much wasnt even wasnt great wasnt good werent\n",
      "Topic 254:\n",
      "experience dining dining experience great experience experience wa overall good experience prompt overall experience experience overall\n",
      "Topic 255:\n",
      "review yelp read maybe reading update review wa great review update review read review\n",
      "Topic 256:\n",
      "soup soup wa miso miso soup pho onion soup french onion wonton french onion soup noodle soup\n",
      "Topic 257:\n",
      "al pastor al pastor taco al taco al pastor margarita pastor taco burro pastor street pastor street taco\n",
      "Topic 258:\n",
      "helpful friendly helpful staff friendly helpful staff friendly wa helpful wa friendly helpful helpful friendly friendly consistently consistently good\n",
      "Topic 259:\n",
      "back go back come back ill ill back definitely back back again back try back sure going back\n",
      "Topic 260:\n",
      "waiter waiter wa attentive kitchen great waiter another refill ask started dessert\n",
      "Topic 261:\n",
      "top notch top notch wa top wa top notch list fresh flavorful top off well back chicken pizza\n",
      "Topic 262:\n",
      "check check out out decided check decided check out decided go check go check out definitely check definitely check out\n",
      "Topic 263:\n",
      "like tasted tasted like feel feel like like wa taste like look looked felt like\n",
      "Topic 264:\n",
      "week ago week ago least twice last week least week couple ago wa couple week\n",
      "Topic 265:\n",
      "perfect wa perfect cooked perfectly perfect size size perfectly cooked absolutely perfect perfect portion like eating\n",
      "Topic 266:\n",
      "tour brewery german omb copper charlotte beer brew tour wa pretzel\n",
      "Topic 267:\n",
      "guacamole guacamole wa mango el miss el pastor cranberry guacamole salsa dont miss hit\n",
      "Topic 268:\n",
      "crawfish crab cajun juicy seafood hot juicy house use shrimp restroom\n",
      "Topic 269:\n",
      "friendly wa friendly friendly staff super friendly friendly wa always friendly fast friendly staff friendly friendly attentive great friendly\n",
      "Topic 270:\n",
      "square market market square square location market square location square wa network primantis around strip district\n",
      "Topic 271:\n",
      "chip chip salsa basket onion chip chocolate chip fish chip basket chip salsa chip chocolate chip wa\n",
      "Topic 272:\n",
      "work good work keep up keep up good up good work up good people work keep art art work\n",
      "Topic 273:\n",
      "asked said told waitress wanted wa told water manager said wa took\n",
      "Topic 274:\n",
      "kung kung pao pao kung pao chicken pao chicken soy tofu vegetarian soy chicken mint\n",
      "Topic 275:\n",
      "guy delivery guy working working guy fieri fieri delivery guy guy wa around joey\n",
      "Topic 276:\n",
      "great great great great great great great price havent success recipe great atmosphere new favorite great quality\n",
      "Topic 277:\n",
      "cant cant wait wait go cant wait go wait go back wait go back cant wait come wait come wait come back\n",
      "Topic 278:\n",
      "far far best far favorite duck cocktail far one wa far smokey incredible favourite\n",
      "Topic 279:\n",
      "people many people watching lot people people watching good people nice people people work great people working\n",
      "Topic 280:\n",
      "appetizer entree course dessert seafood main main course appetizer wa entree wa pretzel\n",
      "Topic 281:\n",
      "bagel deli tega cay tega cay york new york glad egg cheese bacon egg cheese\n",
      "Topic 282:\n",
      "min 30 min 30 20 min 10 min waited 10 20 waiting seated\n",
      "Topic 283:\n",
      "double style innout double double animal style animal double cheese protein got double shake\n",
      "Topic 284:\n",
      "wine bring bottle glass beer wine bottle wine wine beer bring beer soon glass wine\n",
      "Topic 285:\n",
      "day every day day wa next day bad day one day day later rest time day problem\n",
      "Topic 286:\n",
      "yummy wa yummy soooo cocktail mouth healthy birthday land fried pickle best beer\n",
      "Topic 287:\n",
      "long long time long wait ive long took ive long time took long wait long take long long line\n",
      "Topic 288:\n",
      "slow wa slow mediocre little slow typical bit slow chain extremely slow heart wa little slow\n",
      "Topic 289:\n",
      "city windy city windy iron city iron trying lake dip ha best jus\n",
      "Topic 290:\n",
      "ended ended up up ended up getting up getting getting ended up eating up eating fabulous diverse\n",
      "Topic 291:\n",
      "okay wa okay name okay nothing tasted update chinese okay not honest thought\n",
      "Topic 292:\n",
      "saltado lomo lomo saltado ceviche tacu pisco pisco sour sour ceviche mixto mixto\n",
      "Topic 293:\n",
      "late late night open night snack late night snack great late night snack open late early\n",
      "Topic 294:\n",
      "salmon sashimi piece salmon sashimi spicy salmon sashimi wa cut nigiri grilled salmon wa\n",
      "Topic 295:\n",
      "tomato settebello mozzarella basil crushed olive crushed tomato mozzarella basil olive oil oil\n",
      "Topic 296:\n",
      "pei pei wei wei asian pf chang pf chang enjoy mongolian bowl\n",
      "Topic 297:\n",
      "average wa average above average above below average below better average average best pretty average average not\n",
      "Topic 298:\n",
      "recommended highly recommended highly wa recommended fajita well priced affordable whatever consistently good fresh delicious\n",
      "Topic 299:\n",
      "wing philly chicken wing gold bold boneless nacho horrible duckworths ill\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "display_topics(nmf, tfidf_vectorizer_ngrams.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'alpha': 0.10000000000000001}\n",
      "Tuned Classifier Accuracy: 0.565\n",
      "Test Set Score: 0.573\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[127  52  21  18  20]\n",
      " [ 70  89  71  54  10]\n",
      " [ 11  43 117 180  43]\n",
      " [ 11  10  53 417 353]\n",
      " [  5   8  29 220 968]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned nb_model for ngrams\n",
    "tuned_nb_classifier = nb_model(tfidf_ngrams_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_nb_classifier, tfidf_ngrams_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.583\n",
      "Test Set Score: 0.596\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[126  69  19  11  13]\n",
      " [ 57 127  70  27  13]\n",
      " [  7  50 151 149  37]\n",
      " [  6  14  72 416 336]\n",
      " [  8   4  19 230 969]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned svm_model for ngrams\n",
    "tuned_svm_classifier = svm_model(tfidf_ngrams_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_svm_classifier, tfidf_ngrams_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'C': 3.7275937203149381, 'penalty': 'l2'}\n",
      "Tuned Classifier Accuracy: 0.581\n",
      "Test Set Score: 0.589\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 130   49   14   18   27]\n",
      " [  60  105   69   42   18]\n",
      " [   7   47  130  162   48]\n",
      " [   5    7   61  402  369]\n",
      " [   5    4   14  206 1001]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned logreg_model for ngrams\n",
    "tuned_logreg_classifier = logreg_model(tfidf_ngrams_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_logreg_classifier, tfidf_ngrams_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'max_depth': 300, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "Tuned Classifier Accuracy: 0.509\n",
      "Test Set Score: 0.515\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[  82   13   14   51   78]\n",
      " [  24   13   47  121   89]\n",
      " [   4    5   35  220  130]\n",
      " [   3    0   14  341  486]\n",
      " [   3    0    3  149 1075]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned random forest_model for tfidf vectorizer\n",
    "tuned_ranforest_classifier = ranforest_model(tfidf_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_ranforest_classifier, tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'alpha': 0.0}\n",
      "Tuned Classifier Accuracy: 0.397\n",
      "Test Set Score: 0.410\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[   0    0    0    0  238]\n",
      " [   0    0    0    0  294]\n",
      " [   0    0    0    0  394]\n",
      " [   0    0    0    0  844]\n",
      " [   0    0    0    0 1230]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "# Build tuned nb_model for nmf\n",
    "tuned_nb_classifier = nb_model(nmf_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_nb_classifier, nmf_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'C': 100, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Tuned Classifier Accuracy: 0.532\n",
      "Test Set Score: 0.551\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[120  46  20  23  29]\n",
      " [ 60  89  54  58  33]\n",
      " [ 23  47 108 160  56]\n",
      " [ 14  23  54 359 394]\n",
      " [ 18   8  21 206 977]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned svm_model for nmf\n",
    "tuned_svm_classifier = svm_model(nmf_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_svm_classifier, nmf_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'C': 11787686.347935867, 'penalty': 'l1'}\n",
      "Tuned Classifier Accuracy: 0.545\n",
      "Test Set Score: 0.567\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[123  48  16  21  30]\n",
      " [ 67  96  64  41  26]\n",
      " [ 16  64 110 151  53]\n",
      " [ 14  17  69 377 367]\n",
      " [ 20  12  22 182 994]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned logreg_model for nmf\n",
    "tuned_logreg_classifier = logreg_model(nmf_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_logreg_classifier, nmf_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classifier Parameter: {'max_depth': 300, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Tuned Classifier Accuracy: 0.530\n",
      "Test Set Score: 0.539\n",
      "For the confusion matrix, rows correspond to actual ratings and the columns correspond to predicted ratings.\n",
      "[[ 111   16   14   45   52]\n",
      " [  41   43   55   97   58]\n",
      " [  13   11   58  223   89]\n",
      " [   5    2   19  366  452]\n",
      " [   8    0    6  176 1040]]\n"
     ]
    }
   ],
   "source": [
    "# Build tuned random forest_model for nmf\n",
    "tuned_ranforest_classifier = ranforest_model(nmf_train, y_train)\n",
    "\n",
    "# Print test score and CM for tuned model\n",
    "print_test_accuracy_and_cm(tuned_ranforest_classifier, nmf_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to delete\n",
    "# .\n",
    "# . \n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# . \n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# . \n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# . \n",
    "# .\n",
    "# .\n",
    "# ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize text and convert the result to an Numpy array\n",
    "vectorized_text = vectorizer.transform(\" \".join(line) for line in joint_df['processed_review'])\n",
    "vectorized_text_array = vectorized_text.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore word vector properties and most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (2876509, 3000)\n",
      "number of non-zeros: 119826809\n",
      "sparsity: 1.39%\n",
      "food            2293171\n",
      "good            1892858\n",
      "place           1889152\n",
      "great           1344620\n",
      "service         1180824\n",
      "time            1178872\n",
      "like            1131077\n",
      "one             1004655\n",
      "get              938087\n",
      "back             863566\n",
      "go               857214\n",
      "really           827370\n",
      "restaurant       826086\n",
      "would            821958\n",
      "ordered          700710\n",
      "order            698338\n",
      "also             658247\n",
      "chicken          643743\n",
      "menu             595335\n",
      "got              580615\n",
      "nice             557659\n",
      "best             555557\n",
      "come             543735\n",
      "well             541939\n",
      "came             533220\n",
      "try              531427\n",
      "delicious        526938\n",
      "table            519562\n",
      "even             514624\n",
      "love             504504\n",
      "                 ...   \n",
      "deliciously        5203\n",
      "sriracha           5203\n",
      "macaroon           5199\n",
      "politely           5199\n",
      "remove             5197\n",
      "americanized       5195\n",
      "eager              5195\n",
      "carolina           5191\n",
      "traveling          5189\n",
      "surrounding        5188\n",
      "fixe               5187\n",
      "reviewed           5180\n",
      "garage             5177\n",
      "timing             5174\n",
      "tipped             5173\n",
      "ignore             5173\n",
      "opposed            5167\n",
      "chilled            5167\n",
      "powder             5164\n",
      "smart              5161\n",
      "eve                5157\n",
      "matcha             5150\n",
      "holding            5149\n",
      "success            5143\n",
      "towel              5142\n",
      "pricier            5140\n",
      "july               5136\n",
      "push               5135\n",
      "fritter            5132\n",
      "boat               5126\n",
      "Length: 3000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get list of words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(vectorized_text_array, axis=0)\n",
    "\n",
    "# Explore vector metrics\n",
    "print ('sparse matrix shape:', vectorized_text.shape)\n",
    "print ('number of non-zeros:', vectorized_text.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * vectorized_text.nnz / (vectorized_text.shape[0] * vectorized_text.shape[1])))\n",
    "\n",
    "# Print most used words in reverse order\n",
    "print( pd.Series( dict(list(zip(vocab, dist))) ).sort_values( ascending = False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Explore the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23 36 56 ..., 64 33 28]\n"
     ]
    }
   ],
   "source": [
    "# Sum up the word counts of each review\n",
    "review_words = np.sum(vectorized_text_array, axis=1)\n",
    "print(review_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 50.1913590397\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b07ab278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print( 'Mean :', np.mean(review_words) )\n",
    "review_word_count = pd.Series(review_words)\n",
    "review_word_count.plot(kind='hist', bins=25, normed=True, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save vectorized text array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save vectorized text into a npy\n",
    "# https://stackoverflow.com/questions/28439701/how-to-save-and-load-numpy-array-data-properly\n",
    "filename_out = os.path.join(dir, '02_processed_data','vectorized_reviews.npy')\n",
    "np.save(filename_out, vectorized_text_array)    # .npy extension is added if not given\n",
    "\n",
    "# note to load use vectorized_text_array = np.load(filename_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
